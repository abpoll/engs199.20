[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Adam Pollack\n adam.b.pollack@dartmouth.edu\n IR 150\n By appointment\n\n\n\n\n\n MWF\n 2:00-:2:55pm\n No Extra Hour\n Zoom Room",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "",
    "text": "Adam Pollack\n adam.b.pollack@dartmouth.edu\n IR 150\n By appointment\n\n\n\n\n\n MWF\n 2:00-:2:55pm\n No Extra Hour\n Zoom Room",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThe 21st century’s biggest challenges resist traditional problem-solving approaches. These wicked problems (Rittel & Webber, 1973), such as addressing climate risks, can’t be solved with standard engineering methods because they involve multiple stakeholders with competing goals, uncertainties that evolve over time, and complex system behaviors we’re still trying to understand. Decision-makers need new tools and frameworks that can handle this complexity while still pointing the way toward practical action. In this course, students will learn about state-of-the-art technical methods that offer promise for meeting decision-making needs to respond to climate risks. Topics range from complex systems analysis to reinforcement learning, and multi-objective robust decision-making.\n\nRittel, H. W. J., & Webber, M. M. (1973). Dilemmas in a general theory of planning. Policy Sci., 4(2), 155–169. https://doi.org/10.1007/BF01405730",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-goals",
    "href": "syllabus.html#course-goals",
    "title": "Syllabus",
    "section": "Course Goals",
    "text": "Course Goals\nThis course prepares engineers to tackle wicked climate problems. Students master in-demand quantitative techniques in multi-objective robust decision-making, which build on a number of courses in the Thayer catalog. Students also develop essential professional skills in software implementation, data visualization, and science communication. Through carefully designed projects addressing the real-world challenges that students care about, participants will build a solid portfolio that demonstrates mastery of both technical and communication skills vital to modern engineering practice. The course creates natural pathways to honors theses, graduate research, and professional opportunities.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nUpon completing this course, students will be able to:\n\nFrame complex societal challenges in ways amenable to structured decision analysis while preserving essential complexities.\nApply multi-objective robust decision-making frameworks to real-world problems using open source software tools.\nEvaluate trade-offs between competing objectives using appropriate quantitative techniques.\nIdentify actionable insights for addressing problems characterized by deep uncertainties through sensitivity analysis and exploratory modeling.\nCommunicate complex technical analyses clearly and effectively through data visualization and presentation of trade-offs.\nDevelop professional-quality deliverables including software repositories, technical reports, and oral presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nENGS 93 or comparable course in probability and statistics. Students should be proficient in a programming language such as Julia, Python, R, or MATLAB. ENGS 172 recommended. The prerequisites can be replaced by permission from the instructor. If you are unsure if your background is sufficient, please contact the instructor.\n\n\n\n\n\n\nWhat If My Skills Are Rusty?\n\n\n\nIf your programming or statistics skills are a little rusty, don’t worry! We will review concepts and build skills as needed. While we will use the Python programming language in class, if you are familiar with Julia or MATLAB, the fundamentals are similar.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#teaching-approach",
    "href": "syllabus.html#teaching-approach",
    "title": "Syllabus",
    "section": "Teaching Approach",
    "text": "Teaching Approach\n\nCourse Structure\nThe course is divided into several modules and each week is generally structured as follows:\n\nMondays: Lecture\nTuesdays: Occasional X-hour for programming tutorials\nWednesdays: Student-led presentations or serious game followed by a group discussion\nFridays: Computational labs\n\n\n\nExpectations of Students\nThis course will require your sustained attention. Students are expected to:\n\ncome prepared to class (e.g., by carefully reading and synthesizing the reading assignments before class and being ready to present their synthesis in class);\nactively contribute to the group discussions and activities;\nsubmit the assignments on time; and\nattend office hours as needed.\n\nStudents should expect to spend roughly three times the in-course hours outside the classroom for readings and assignments.\n\nCommunication\n\n\n\n\n\n\nPlease, Be Excellent To Teach Other\n\n\n\nWe all make mistakes in our communications with one another, both when speaking and listening. Be mindful of how spoken or written language might be misunderstood, and be aware that, for a variety of reasons, how others perceive your words and actions may not be exactly how you intended them. At the same time, it is also essential that we be respectful and interpret each other’s comments and actions in good faith.\n\n\n\n\n\n\n\n\nTroubleshooting Tips\n\n\n\n\nDo not take screenshots of code. I will not respond. Screenshots can be difficult to read and limit accessibility. Put your code on GitHub, share the link, and point to specific line numbers if relevant, or provide a simple, self-contained example of the problem you’re running into.\nIf you wait until the day an assignment is due (or even late the previous night) to ask a question on Canvas, there is a strong chance that I will not see your post prior to the deadline.\nIf you see unanswered questions and you have some insight, please answer! This class will work best when we all work together as a community.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\n\nActive Participation: 10%\nCollaboration and collegiality are important skills for interdisciplinary decision analysts. We will practice these skills in the following ways:\n\nAsking questions during course meetings and/or office hours;\nParticipating in serious games and group discussions;\nProviding constructive feedback on student-led presentations;\nCollaborating on computational labs and managing a shared GitHub repository.\n\nGraduate students will additionally practice these skills by leading a journal club session on one of the assigned readings.\nNote that passively attending class will not yield full participation points. Participation points are not free. I will record lectures, student-led presentations and group discussions, and lab tutorials and post them to Canvas. It is possible to attend these sessions remotely on Zoom, but this can affect your ability to actively participate. It is not possible to attend some of the Wednesday active learning sessions remotely and these will not be recorded.\n\n\nComputational Labs: 30%\nWe will use class time for hands-on programming exercises to give you guided practice applying the concepts and methods from class. These labs will be done in groups; if you cannot bring a laptop to class, you will be able to (and are encouraged to) work with other students, though you must turn in your own lab report for grading. All labs will cover either a set of programming tools or a case study, such as reproducing analyses in peer-reviewed studies (e.g., https://github.com/abpoll/j40_gc/tree/v1.0.0-pub and https://github.com/jdossgollin/2022-elevation-robustness).\nLab reports are due before the following lab.\n\n\nCourse Project: 60%\nThe goal of the course project is to develop and report on an actionable plan for implementing a quantitative decision analysis for the student’s topic of choice. This project can be highly synergistic with thesis projects and students are encouraged to choose a decision problem related to their thesis if they are working on one.\nStudents will work on this project throughout the entire term, with ongoing evaluations and three major evaluation milestones as follows:\n\nProject Progress Reports: At the end of each course module, students will have one week to submit a progress report that details a technically appropriate plan to integrate methods covered in the module in a quantitative decision analysis. A list of guiding questions will be available at the beginning of each module, so students will have more than a week to work on these reports. These reports test your understanding and synthesis of material in each module, and your ability to extend the underlying concepts and tools in an application area of your choice.\nProject Presentation: Students will present their projects to the class at the end of the term. Presentations should be no more than 15 minutes long and should be aimed at a general science audience. Each presentation should demonstrate comprehension of technical aspects, clearly communicate limitations and opportunities for future work, and include signposts for peer feedback. Part of your grade on this component consists of the quality of your feedback to your peers on their presentations (more on this in the relevant assignment).\nWritten Project Report: Students will synthesize their term-long progress reports and feedback from their project presentation into a detailed proposal for how to implement the planned decision analysis.\n\nThese components are each worth 20% of your overall grade. The course project is broken into complimentary components to help keep you on track over the term and to lower the stakes of the overall project.\nAssignments must be submitted via Canvas by the deadline to receive a full grade. Late submissions will receive a downgrading by 25% of the full credit for each day they are late. Submissions after the end of the examination period do not receive any points. Each partial day will be rounded up. You may want to submit a few hours before the deadline to avoid last minute technical problems. Extension requests must be made via email 24 hours before the submission deadline with a submission of what has been achieved, thus far.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#class-resources",
    "href": "syllabus.html#class-resources",
    "title": "Syllabus",
    "section": "Class Resources",
    "text": "Class Resources\nThis course covers material in a relatively new and fast evolving field. As such, there is no single resource that comprehensively synthesizes the state-of-the-art in decision analysis for wicked climate problems. Students may find it helpful to consult the open access textbook published several years ago with contributions from prominent scientists in the field titled Decision Making under Deep Uncertainty : From Theory to Practice. (Marchau et al., 2019)\n\nMarchau, V. A. W., Walker, W. E., Bloemen, P. J. T., & Popper, S. W. (Eds.). (2019). Decision making under deep uncertainty: From theory to practice (2019th ed.). Cham, Switzerland: Springer Nature. https://doi.org/10.1007/978-3-030-05252-2\nOther course materials consist, for example, of meeting notes, peer-reviewed publications, and reports. Students will be able to access all materials freely through Canvas or through the Dartmouth library system.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#class-policies",
    "href": "syllabus.html#class-policies",
    "title": "Syllabus",
    "section": "Class Policies",
    "text": "Class Policies\n\nCanvas\nCourse communication, assignments, submissions of written materials will be handled through the course Canvas site. Recordings will be made for most sessions (i.e., not several active learning sessions on Wednesdays) and posted on Canvas.\n\n\nAcademic Honor Principle\nStudents are expected to follow Dartmouth’s Academic Honor Principle and Thayer’s Academic Honor Policy.\n\n\nReligious Observances\nDartmouth has a deep commitment to support students’ religious observances and diverse faith practices. Some students may wish to take part in religious observances that occur during this academic term. If you have a religious observance that conflicts with your participation in the course, please meet with me as soon as possible — before the end of the second week of the term at the latest — to discuss appropriate course adjustments.\n\n\nStudent Accessibility and Accommodations\nStudents requesting disability-related accommodations and services for this course are required to register with Student Accessibility Services (SAS; Apply for Services webpage; student.accessibility.services@dartmouth.edu; 1-603-646-9900) and to request that an accommodation email be sent to me in advance of the need for an accommodation. Then, students should schedule a follow-up meeting with me to determine relevant details such as what role SAS or its Testing Center may play in accommodation implementation. This process works best for everyone when completed as early in the quarter as possible. If students have questions about whether they are eligible for accommodations or have concerns about the implementation of their accommodations, they should contact the SAS office. All inquiries and discussions will remain confidential.\n\n\nClass Climate and Inclusivity\nWe will discuss topics that for some are associated with strong emotions and opinions. We will read and discuss sources from a wide range of authors. These sources are selected to sample a wide range of diverse perspectives. However, it seems likely that important perspectives are missing.\nThis class strives to provide a respectful, inclusive, and civil space for all. Please inform all members of this class if your names and pronouns you prefer differ from your official record. Please reach out to the instructor if your performance and ability to learn is impacted by factors outside of the classroom (including financial challenges). The instructor is here to help as much as possible. Feedback can also be anonymous (including a note under the office door of the instructor). We all are continuously learning about how to ensure an inclusive environment and how to discuss contentious and emotional subjects. Please provide feedback if something makes you feel uncomfortable.\n\n\nUse of GenAI\nWork submitted for a grade in this course must reflect your own understanding. The use and consultation of AI/ML tools, such as ChatGPT or similar, for any purpose whatsoever, must be pre-approved and clearly referenced. Please see Dartmouth’s Guidelines on using GenAI.\nMy general view is that Large language models (LLMs) are powerful tools that you will encounter and use when you leave this classroom, so it’s important to learn how to use them responsibly and effectively. However, these are difficult skills to teach and are not the focus of this course. I will likely not accept requests to use GenAI to generate text for writing assignments. However, I may accept requests to use LLMs to support coding tasks. This can help accelerate your workflow, especially when you are less familiar with a new language. However, LLMs can end up being a less productive use of your time than talking to colleagues, checking out documentation and APIs, looking at StackOverflow, and experimenting with trial and failure. You are responsible for understanding and debugging your code, and for ensuring that it does what you intend it to do.\nIf approved, you must:\n\nreference the URL of the service you are using, including the specific date you accessed it;\nprovide the exact query or queries used to interact with the tool; and\nreport the exact response received.\n\n\n\nStudent’s Consent\nBy enrolling in this course:\n\nI affirm my understanding that the instructor may record meetings of this course and any associated meetings open to multiple students and the instructor, including but not limited to scheduled and ad hoc office hours and other consultations, within any digital platform, including those used to offer remote instruction for this course.\nI further affirm that the instructor owns the copyright to their instructional materials, of which these recordings constitute a part, and my distribution of any of these recordings in whole or in part to any person or entity other than other members of the class without prior written consent of the instructor may be subject to discipline by Dartmouth up to and including separation from Dartmouth.\nRequirement of consent to one-on-one recordings By enrolling in this course, I hereby affirm that I will not make a recording in any medium of any one-on-one meeting with the instructor or another member of the class or group of members of the class without obtaining the prior written consent of all those participating, and I understand that if I violate this prohibition, I will be subject to discipline by Dartmouth up to and including separation from Dartmouth, as well as any other civil or criminal penalties under applicable law. I understand that an exception to this consent applies to accommodations approved by SAS for a student’s disability, and that one or more students in a class may record class lectures, discussions, lab sessions, and review sessions and take pictures of essential information, and/or be provided class notes for personal study use only.\n\nIf you have questions, please contact the Office of the Dean of the Faculty of Arts and Sciences.\n\n\nMental Health and Wellbeing\nThe academic environment is challenging, our terms are intensive, and classes are not the only demanding part of your life. There are a number of resources available to you on campus to support your wellness, including: the Counseling Center, which allows you to book triage appointments online, the Student Wellness Center which offers wellness check-ins, and your undergraduate dean. The student-led Dartmouth Student Mental Health Union and their peer support program may be helpful if you would like to speak to a trained fellow student support listener. If you need immediate assistance, please contact the counselor on-call at (603) 646-9442 at any time. Please make me aware of anything that will hinder your success in this course.\n\n\nTitle IX\nAt Dartmouth, we value integrity, responsibility, and respect for the rights and interests of others, all central to our Principles of Community. We are dedicated to establishing and maintaining a safe and inclusive campus where all community members have equal access to Dartmouth’s educational and employment opportunities. We strive to promote an environment of sexual respect, safety, and well-being. Through the Sexual and Gender-Based Misconduct Policy (SMP), Dartmouth demonstrates that sex and gender-based discrimination, sex and gender-based harassment, sexual assault, dating violence, domestic violence, stalking, etc., are not tolerated in our community. For more information regarding Title IX and to access helpful resources, visit Title IX’s website. As a faculty member, I am required to share disclosures of sexual or gender-based misconduct with the Title IX office. If you have any questions or want to explore support and assistance, please contact the Title IX office at 603-646-0922 or TitleIX@dartmouth.edu. Speaking to Title IX does not automatically initiate a college resolution. Instead, much of their work is around providing supportive measures to ensure you can continue to engage in Dartmouth’s programs and activities.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "readings/readings06.html",
    "href": "readings/readings06.html",
    "title": "Week 6 Readings",
    "section": "",
    "text": "Herman et al. (2020)\n\nVery helpful overview for thinking about policy search in water resources (and presented in a way that makes it easier to think about applications in other areas).\n\nQuinn et al. (2017)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one."
  },
  {
    "objectID": "readings/readings06.html#required-reading",
    "href": "readings/readings06.html#required-reading",
    "title": "Week 6 Readings",
    "section": "",
    "text": "Herman et al. (2020)\n\nVery helpful overview for thinking about policy search in water resources (and presented in a way that makes it easier to think about applications in other areas).\n\nQuinn et al. (2017)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one."
  },
  {
    "objectID": "readings/readings06.html#optional-reading",
    "href": "readings/readings06.html#optional-reading",
    "title": "Week 6 Readings",
    "section": "Optional Reading",
    "text": "Optional Reading\nBelow are a combination of example applications of policy search, along with some more theory-based papers that offer helpful classifications for thinking about different approaches to optimization and understanding where policy search sits in general.\n\nCohen & Herman (2021)\n\nThis is a unique study because the authors consider a very wide variety of indicators, thresholds, and actions to find the optimal policy structure and form instead of prespecifying the policy.\n\nHerman & Giuliani (2018)\n\nThis is a unique study because the authors explicitly aim to find interpretable policies using a policy tree structure.\n\nGiuliani et al. (2021)\n\nThis is a helpful study to review because it situates policy search in a broader set of approaches for optimization in water resources.\n\n\n\nReferences\n\n\nCohen, J. S., & Herman, J. D. (2021). Dynamic adaptation of water resources systems under uncertainty by learning policy structure and indicators. Water Resour. Res., 57(11), e2021WR030433. https://doi.org/10.1029/2021wr030433\n\n\nGiuliani, M., Lamontagne, J. R., Reed, P. M., & Castelletti, A. (2021). A state‐of‐the‐art review of optimal reservoir control for managing conflicting demands in a changing world. Water Resour. Res., 57(12), e2021WR029927. https://doi.org/10.1029/2021wr029927\n\n\nHerman, J. D., & Giuliani, M. (2018). Policy tree optimization for threshold-based water resources management over multiple timescales. Environ. Model. Softw., 99, 39–51. https://doi.org/10.1016/j.envsoft.2017.09.016\n\n\nHerman, J. D., Quinn, J. D., Steinschneider, S., Giuliani, M., & Fletcher, S. (2020). Climate adaptation as a control problem: Review and perspectives on dynamic water resources planning under uncertainty. Water Resour. Res., 56(2). https://doi.org/10.1029/2019wr025502\n\n\nQuinn, J. D., Reed, P. M., & Keller, K. (2017). Direct policy search for robust multi-objective management of deeply uncertain socio-ecological tipping points. Environmental Modelling & Software, 92, 125–141. https://doi.org/10.1016/j.envsoft.2017.02.017"
  },
  {
    "objectID": "readings/readings05.html",
    "href": "readings/readings05.html",
    "title": "Week 5 Readings",
    "section": "",
    "text": "Kwakkel & Haasnoot (2019)\n\nThis is a relatively short read and is good for thinking about the overall picture.\n\nBartholomew & Kwakkel (2020)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one."
  },
  {
    "objectID": "readings/readings05.html#required-reading",
    "href": "readings/readings05.html#required-reading",
    "title": "Week 5 Readings",
    "section": "",
    "text": "Kwakkel & Haasnoot (2019)\n\nThis is a relatively short read and is good for thinking about the overall picture.\n\nBartholomew & Kwakkel (2020)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one."
  },
  {
    "objectID": "readings/readings05.html#optional-reading",
    "href": "readings/readings05.html#optional-reading",
    "title": "Week 5 Readings",
    "section": "Optional Reading",
    "text": "Optional Reading\nThere are a few approaches since the DMDU taxonomy chapter that I think are worth knowing about and considering as potentially relevant for your study.\n\nTrindade et al. (2019)\nShavazipour et al. (2021)\nDoss-Gollin & Keller (2023)\nCiullo et al. (2023)\n\n\nReferences\n\n\nBartholomew, E., & Kwakkel, J. H. (2020). On considering robustness in the search phase of robust decision making: A comparison of many-objective robust decision making, multi-scenario many-objective robust decision making, and many objective robust optimization. Environmental Modelling & Software, 127, 104699. https://doi.org/10.1016/j.envsoft.2020.104699\n\n\nCiullo, A., Domeneghetti, A., Kwakkel, J. H., De Bruijn, K. M., Klijn, F., & Castellarin, A. (2023). Belief-informed robust decision making (BIRDM): Assessing changes in decision robustness due to changing distributions of deep uncertainties. Environmental Modelling & Software, 159, 105560. https://doi.org/10.1016/j.envsoft.2022.105560\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective bayesian framework for synthesizing deep uncertainties in climate risk management. Earths Future, 11(1). https://doi.org/10.1029/2022ef003044\n\n\nKwakkel, J. H., & Haasnoot, M. (2019). Supporting DMDU: A taxonomy of approaches and tools. In Decision making under deep uncertainty (pp. 355–374). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05252-2\\_15\n\n\nShavazipour, B., Kwakkel, J. H., & Miettinen, K. (2021). Multi-scenario multi-objective robust optimization under deep uncertainty: A posteriori approach. Environ. Model. Softw., 144(105134), 105134. https://doi.org/10.1016/j.envsoft.2021.105134\n\n\nTrindade, B. C., Reed, P. M., & Characklis, G. W. (2019). Deeply uncertain pathways: Integrated multi-city regional water supply infrastructure investment and portfolio management. Adv. Water Resour., 134(103442), 103442. https://doi.org/10.1016/j.advwatres.2019.103442"
  },
  {
    "objectID": "readings/readings02.html",
    "href": "readings/readings02.html",
    "title": "Week 2 Readings",
    "section": "",
    "text": "Oddo et al. (2020)\n\nThis article builds on a classic decision analysis framework by incorporating key uncertainties and multiple objectives, demonstrating how these considerations affect the representation of “optimal” strategies. It clearly distinguishes between “deep” and “shallow” uncertainties, and includes a global sensitivity analysis on how uncertainty in parameters influence objective evaluation. Table 1 is great."
  },
  {
    "objectID": "readings/readings02.html#required-reading",
    "href": "readings/readings02.html#required-reading",
    "title": "Week 2 Readings",
    "section": "",
    "text": "Oddo et al. (2020)\n\nThis article builds on a classic decision analysis framework by incorporating key uncertainties and multiple objectives, demonstrating how these considerations affect the representation of “optimal” strategies. It clearly distinguishes between “deep” and “shallow” uncertainties, and includes a global sensitivity analysis on how uncertainty in parameters influence objective evaluation. Table 1 is great."
  },
  {
    "objectID": "readings/readings02.html#optional-reading",
    "href": "readings/readings02.html#optional-reading",
    "title": "Week 2 Readings",
    "section": "Optional Reading",
    "text": "Optional Reading\nI want to provide you with more references for identifying, characterizing, and quantifying uncertainties in general and in decision analysis papers.\nMore on uncertainty analysis in general: Srikrishnan et al. (2022) and related e-book on uncertainty characterization and quantification in MSD analyses\nMore on deep uncertainty: Lempert et al. (2024)\nPapers with Methods sections (or Supplementary Info) worth reviewing to help with your module progress report (look at the presentation of the analysis workflow, how the authors represent and account for various types of uncertainty, etc.,):\n\nOddo et al. (2020)\nZarekarizi et al. (2020)\nPollack et al. (2025)\n\n\nReferences\n\n\nLempert, R. J., Lawrence, J., Kopp, R. E., Haasnoot, M., Reisinger, A., Grubb, M., & Pasqualino, R. (2024). The use of decision making under deep uncertainty in the IPCC. Frontiers in Climate, 6. https://doi.org/10.3389/fclim.2024.1380054\n\n\nOddo, P. C., Lee, B. S., Garner, G. G., Srikrishnan, V., Reed, P. M., Forest, C. E., & Keller, K. (2020). Deep uncertainties in sea-level rise and storm surge projections: Implications for coastal flood risk management. Risk Anal., 40(1), 153–168. https://doi.org/10.1111/risa.12888\n\n\nPollack, A. B., Santamaria-Aguilar, S., Maduwantha, P., Helgeson, C., Wahl, T., & Keller, K. (2025). Funding rules that promote equity in climate adaptation outcomes. Proceedings of the National Academy of Sciences, 122(2), e2418711121. https://doi.org/10.1073/pnas.2418711121\n\n\nSrikrishnan, V., Lafferty, D. C., Wong, T. E., Lamontagne, J. R., Quinn, J. D., Sharma, S., et al. (2022). Uncertainty analysis in multi‐sector systems: Considerations for risk analysis, projection, and planning for complex systems. Earths Future, 10(8). https://doi.org/10.1029/2021ef002644\n\n\nZarekarizi, M., Srikrishnan, V., & Keller, K. (2020). Neglecting uncertainties biases house-elevation decisions to manage riverine flood risks. Nat. Commun., 11(1), 5361. https://doi.org/10.1038/s41467-020-19188-9"
  },
  {
    "objectID": "project/report.html",
    "href": "project/report.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "Your project report is a draft of several sections of your paper:\n\nYour introduction\nSeveral parts of your methods section\nThe hypothesized results from your decision analysis\n\nGoing into more detail on each section:\n\n\nWith all the attention we’ve paid to framing, you are ready to write a first (or more refined) draft of your paper’s introduction. You can follow other structures, but one way to do an effective structure mirrors your presentation by answering:\n\nWhat is your decision problem?\nWhat is the state of the art in addressing this problem?\nWhat is wrong with the status quo?\nWhat knowledge gaps are there and which do you address?\nHow will you address these knowledge gaps (not too technical)?\nWhat are your hypotheses about your planned decision analysis?\nWhat are the implications?\n\nFor some journals, especially those with space constraints, the last two bullets are not necessary (or possible) to include. For your report, I would like you to include these for completeness. You can always use that text in other sections.\n\n\n\nOne of the main things we learned in this course is that our analytical choices stem from our framing choices and that we should have strong justifications for why our methods appropriately operationalize these choices. Here are the types of analytical choices you can write a draft about in your report:\n\nOverview of the overall decision analysis workflow\n\nTake this opportunity to really highlight what your paper’s goals are and how the workflow accommodates answering your specific research questions\nTalk about any framing activities (e.g., workshops or meetings with stakeholders, policy document review, etc.,)\n\nHighlight the key uncertainties relevant to your decision analysis (specifically your paper’s goals), explain the role uncertainty plays in your analysis (e.g., robustness considerations, scenario discovery, factor fixing, etc.,) and how you will address that\nThe relevant objectives (and trade-offs/synergies) for your study, how you identified them, and how you will measure them\nIf relevant, how you define and measure robustness and the justification for that\nYour exploratory modeling approach\n\nYou do not have to describe your model entirely here (though you might for your paper). For your report, you should think of this as being at the level of deatil for the “main” manuscript, but not for the supplementary info (e.g., extensive details on validation or calibration, data sources, etc.,)\n\nYour policy search approach (prespecified or generating solutions?) - if generating solutions, how will you handle deep uncertainties and do you consider robustness in your search?\n\nYou should comment on the justification for your policy search approach, including how you represent the policy structure. Is it reasonable in your setting for a decision-maker to implement policies in this form? How stylized is this representation?\n\n\n\n\n\nIt is important for you to form hypotheses before you start your analysis. Given the exploratory nature of our modeling, and the many uncertainties involved with framing and data and projections, hypotheses are a good way to safeguard against errors in implementation. More importantly, they require you to think hard and encourage you to treat your modeling exercise as a complement to your reasoning instead of replacement. Importantly, one reason that we use exploratory modeling is that it can be a priori infeasible to anticipate the consequences of our complex system of study and the decision space we consider. You can form hypotheses about the types of mechanisms that could explain results in different directions. You can also explain the types of analyses you will do to get to the bottom of reuslts you find.\nIn our trade-offs module report, we proposed figure sketches about how the strategies under consideration might navigate trade-offs in our problem. Now, we will apply the figure sketch technique more broadly to the results-based narrative of our study. What are the key results you will generate in response to your research questions? Sketch these out, include figure captions, and write text summarizing your hypotheses. Compared to the other sections, you do not have to write these summaries in the same narrative style but they should be in complete sentences and reflect your reasoning about the complex system and decision space.",
    "crumbs": [
      "Course Project",
      "Final Report"
    ]
  },
  {
    "objectID": "project/report.html#overview",
    "href": "project/report.html#overview",
    "title": "Project Report Instructions",
    "section": "",
    "text": "Your project report is a draft of several sections of your paper:\n\nYour introduction\nSeveral parts of your methods section\nThe hypothesized results from your decision analysis\n\nGoing into more detail on each section:\n\n\nWith all the attention we’ve paid to framing, you are ready to write a first (or more refined) draft of your paper’s introduction. You can follow other structures, but one way to do an effective structure mirrors your presentation by answering:\n\nWhat is your decision problem?\nWhat is the state of the art in addressing this problem?\nWhat is wrong with the status quo?\nWhat knowledge gaps are there and which do you address?\nHow will you address these knowledge gaps (not too technical)?\nWhat are your hypotheses about your planned decision analysis?\nWhat are the implications?\n\nFor some journals, especially those with space constraints, the last two bullets are not necessary (or possible) to include. For your report, I would like you to include these for completeness. You can always use that text in other sections.\n\n\n\nOne of the main things we learned in this course is that our analytical choices stem from our framing choices and that we should have strong justifications for why our methods appropriately operationalize these choices. Here are the types of analytical choices you can write a draft about in your report:\n\nOverview of the overall decision analysis workflow\n\nTake this opportunity to really highlight what your paper’s goals are and how the workflow accommodates answering your specific research questions\nTalk about any framing activities (e.g., workshops or meetings with stakeholders, policy document review, etc.,)\n\nHighlight the key uncertainties relevant to your decision analysis (specifically your paper’s goals), explain the role uncertainty plays in your analysis (e.g., robustness considerations, scenario discovery, factor fixing, etc.,) and how you will address that\nThe relevant objectives (and trade-offs/synergies) for your study, how you identified them, and how you will measure them\nIf relevant, how you define and measure robustness and the justification for that\nYour exploratory modeling approach\n\nYou do not have to describe your model entirely here (though you might for your paper). For your report, you should think of this as being at the level of deatil for the “main” manuscript, but not for the supplementary info (e.g., extensive details on validation or calibration, data sources, etc.,)\n\nYour policy search approach (prespecified or generating solutions?) - if generating solutions, how will you handle deep uncertainties and do you consider robustness in your search?\n\nYou should comment on the justification for your policy search approach, including how you represent the policy structure. Is it reasonable in your setting for a decision-maker to implement policies in this form? How stylized is this representation?\n\n\n\n\n\nIt is important for you to form hypotheses before you start your analysis. Given the exploratory nature of our modeling, and the many uncertainties involved with framing and data and projections, hypotheses are a good way to safeguard against errors in implementation. More importantly, they require you to think hard and encourage you to treat your modeling exercise as a complement to your reasoning instead of replacement. Importantly, one reason that we use exploratory modeling is that it can be a priori infeasible to anticipate the consequences of our complex system of study and the decision space we consider. You can form hypotheses about the types of mechanisms that could explain results in different directions. You can also explain the types of analyses you will do to get to the bottom of reuslts you find.\nIn our trade-offs module report, we proposed figure sketches about how the strategies under consideration might navigate trade-offs in our problem. Now, we will apply the figure sketch technique more broadly to the results-based narrative of our study. What are the key results you will generate in response to your research questions? Sketch these out, include figure captions, and write text summarizing your hypotheses. Compared to the other sections, you do not have to write these summaries in the same narrative style but they should be in complete sentences and reflect your reasoning about the complex system and decision space.",
    "crumbs": [
      "Course Project",
      "Final Report"
    ]
  },
  {
    "objectID": "project/report.html#rubric",
    "href": "project/report.html#rubric",
    "title": "Project Report Instructions",
    "section": "Rubric",
    "text": "Rubric\nFor the purposes of the training, there is no grade assigned. For credit-based versions of this course, it is up to instructor discretion to design a grading scheme.\nThat said, there will be a peer evaluation aspect to the final report in the training, and I propose this for credit-based versions of the course as well.",
    "crumbs": [
      "Course Project",
      "Final Report"
    ]
  },
  {
    "objectID": "project/presentation.html",
    "href": "project/presentation.html",
    "title": "Project Presentation Instructions",
    "section": "",
    "text": "Your project presentation will provide a narrative overview of your project. You are allowed 15 minutes for your presentation, which should cover the following:\n\nWhat is your decision problem?\nWhat is the state of the art in addressing this problem?\nWhat is wrong with the status quo?\nWhat knowledge gaps are there and which do you address?\nHow will you address these knowledge gaps (not too technical)?\nWhat are your hypotheses about your planned decision analysis?\nWhat are the implications?\n\nAs an audience member, you will be responsible for one short summary and round of moderation of group discussion for a colleague’s project. Your summary should focus on the story of the project and should clarify any parts of the story that were not clear to you. Your round of moderation should facilitate specific attention from the other audience members. For example, you can prompt other students to comment on potential limitations in proposed methods or you can prompt the presenter to clarify a specific narrative point.",
    "crumbs": [
      "Course Project",
      "Presentation"
    ]
  },
  {
    "objectID": "project/presentation.html#overview",
    "href": "project/presentation.html#overview",
    "title": "Project Presentation Instructions",
    "section": "",
    "text": "Your project presentation will provide a narrative overview of your project. You are allowed 15 minutes for your presentation, which should cover the following:\n\nWhat is your decision problem?\nWhat is the state of the art in addressing this problem?\nWhat is wrong with the status quo?\nWhat knowledge gaps are there and which do you address?\nHow will you address these knowledge gaps (not too technical)?\nWhat are your hypotheses about your planned decision analysis?\nWhat are the implications?\n\nAs an audience member, you will be responsible for one short summary and round of moderation of group discussion for a colleague’s project. Your summary should focus on the story of the project and should clarify any parts of the story that were not clear to you. Your round of moderation should facilitate specific attention from the other audience members. For example, you can prompt other students to comment on potential limitations in proposed methods or you can prompt the presenter to clarify a specific narrative point.",
    "crumbs": [
      "Course Project",
      "Presentation"
    ]
  },
  {
    "objectID": "project/presentation.html#rubric",
    "href": "project/presentation.html#rubric",
    "title": "Project Presentation Instructions",
    "section": "Rubric",
    "text": "Rubric\nThe below is a proposed grading scheme for future credit-based versions of the course. For the purposes of the training, there is no grade assigned. We will do the peer evaluation, though.\nYou can receive a maximum of 20 points, with credit opportunities as follows:\n\nUp to 10 points based on instructor’s rating\nUp to 8 points based on average of peer rating (low quality dropped)\nUp to 2 points for summary and moderation\n1 point for strong and constructive peer evaluation\nUp to 2 points for submitting a revised and improved recorded presentation on Canvas within the following week\n\nYou can lose points as follows:\n\n-5 points for going over 15 minutes on either presentation opportunity\n-2 points for low quality peer evaluation\n\nNote: Low quality peer evaluations are ones that suggest you were not paying attention to your colleague’s presentation. Even if you struggle to understand some material, your colleague deserves your respect. In fact, conveying clearly that you struggled to understand some material can be very helpful to your colleague in synthesizing feedback and working on their final report.",
    "crumbs": [
      "Course Project",
      "Presentation"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website contains course materials for the Fall 2025 edition of Decision Analysis for Wicked Climate Problems, taught by Adam Pollack at Dartmouth College."
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe layout for this site draws heavily from Environmental Systems Analysis by Dr. Vivek Srikrishnan at Cornell University, Data Science for Climate Risk Assessment by Dr. James Doss-Gollin at Rice University, and course materials by Dr. Andrew Heiss at Georgia State University."
  },
  {
    "objectID": "about.html#tools-and-generation-workflow",
    "href": "about.html#tools-and-generation-workflow",
    "title": "About",
    "section": "Tools and Generation Workflow",
    "text": "Tools and Generation Workflow\nThis course website was built with Quarto. All materials can be found at the GitHub Repository."
  },
  {
    "objectID": "slides/lecture07-dps.html#basic-idea",
    "href": "slides/lecture07-dps.html#basic-idea",
    "title": "Direct policy search",
    "section": "Basic idea",
    "text": "Basic idea\nRecall: RDM aims to identify strategies that perform well over a wide range of future conditions\n\n\nWhat if we identify strategies that respond to new information over time?\nGoal: Optimize the sequence, timing, and/or threshold values of observed variables to initiate actions"
  },
  {
    "objectID": "slides/lecture07-dps.html#a-few-technical-considerations",
    "href": "slides/lecture07-dps.html#a-few-technical-considerations",
    "title": "Direct policy search",
    "section": "A few technical considerations",
    "text": "A few technical considerations\n\nOptimizing dynamic strategies is highly sensitive to uncertainty characterization\n\nUC determines how extreme stressors are and the sequence of events\n\nThere are many methods to implement this optimization and we will spend most of our time on one approach - direct policy search"
  },
  {
    "objectID": "slides/lecture07-dps.html#problem-overview",
    "href": "slides/lecture07-dps.html#problem-overview",
    "title": "Direct policy search",
    "section": "Problem overview",
    "text": "Problem overview\n\n\n\nHerman et al., (2020)"
  },
  {
    "objectID": "slides/lecture07-dps.html#in-math",
    "href": "slides/lecture07-dps.html#in-math",
    "title": "Direct policy search",
    "section": "In math",
    "text": "In math\n\\[\\begin{aligned}\n& \\min_{\\pi} \\mathbb{E}_{e_{1}, \\ldots, e_{H+1}} \\sum_{t=0}^{H-1} J_{t}(x_{t}, a_{t}, e_{t}) + J_{H+1}(x_{H+1}) \\\\\n\\text{subject to:} \\quad\n& x_{t+1} = f_{t}(x_{t}, a_{t}, e_{t+1}), \\quad t=0,\\ldots,H-1 \\\\\n& a_{t} = \\pi(I_{t}), \\quad t=0,\\ldots,H-1\n\\end{aligned}\\]\nwhere \\(x_{t}\\) are system states, \\(a_{t}\\) are discrete actions, \\(e_{t}\\) are external forcings, \\(I_{t}\\) is the information, and \\(J\\) are objectives. The system follows the state transition equation \\(x_{t+1} = f_{t}(x_{t}, a_{t}, e_{t+1})\\) in a single realization of the external disturbance with a single action."
  },
  {
    "objectID": "slides/lecture07-dps.html#open-loop",
    "href": "slides/lecture07-dps.html#open-loop",
    "title": "Direct policy search",
    "section": "Open loop",
    "text": "Open loop\n\\[\\begin{aligned}\n& \\min_{\\pi} \\mathbb{E}_{e_{1}, \\ldots, e_{H+1}} \\sum_{t=0}^{H-1} J_{t}(x_{t}, a_{t}, e_{t}) + J_{H+1}(x_{H+1}) \\\\\n\\text{subject to:} \\quad\n& x_{t+1} = f_{t}(x_{t}, a_{t}, e_{t+1}), \\quad t=0,\\ldots,H-1 \\\\\n& a_{t} = \\pi(I_{t}), \\quad t=0,\\ldots,H-1\n\\end{aligned}\\]\nOpen loop directly optimizes directly optimizes the sequence of actions over the time horizon, \\(a_{t} = \\pi(t)\\). In theory, you can use any optimization method (e.g., linear programming, nonlinear programming, heuristics, enumeration)."
  },
  {
    "objectID": "slides/lecture07-dps.html#closed-loop",
    "href": "slides/lecture07-dps.html#closed-loop",
    "title": "Direct policy search",
    "section": "Closed loop",
    "text": "Closed loop\nIn contrast to open loop, closed loop approaches direct actions based on observed conditions. At a high level, there are three main approaches:\n\nStochastic dynamic programming\nFind the optimal sequence of decisions based on approximating the optimal value function\nApproximate the optimal sequence of decisions based on optimizing a particular policy structure\n\nMost MORDM studies employ one class of methods in that last approach."
  },
  {
    "objectID": "slides/lecture07-dps.html#intuition",
    "href": "slides/lecture07-dps.html#intuition",
    "title": "Direct policy search",
    "section": "Intuition",
    "text": "Intuition\nSpecify a policy with a functional form and parameters such that \\(a_{t} = \\pi(I_{t}, \\theta)\\).\nAfter reformulating the optimization problem this way, what we identify is a mapping of observations to actions. The “optimal” policy is limited by the functional form of \\(\\pi\\) and the numerical convergence of the optimization."
  },
  {
    "objectID": "slides/lecture07-dps.html#why-optimal-in-quotes",
    "href": "slides/lecture07-dps.html#why-optimal-in-quotes",
    "title": "Direct policy search",
    "section": "Why “Optimal” in quotes?",
    "text": "Why “Optimal” in quotes?\nWe usually can’t guarantee that we can find an optimum (or even quantify an optimality gap) because:\n\nSimulation-optimization is applied in very general settings;\nMay not have much a priori information about the response surface;\nThe response surface can be highly nonlinear.\n\n\nCredit to Vivek Srikrishnan, Environmental Systems Analysis"
  },
  {
    "objectID": "slides/lecture07-dps.html#why-optimal-in-quotes-1",
    "href": "slides/lecture07-dps.html#why-optimal-in-quotes-1",
    "title": "Direct policy search",
    "section": "Why “Optimal” in quotes?",
    "text": "Why “Optimal” in quotes?\n\nThe optimal solution of a model is not an optimal solution of a problem unless the model is a perfect representation of the problem, which it never is.\n\n— Ackoff, R. L. (1979). “The Future of Operational Research is Past.” The Journal of the Operational Research Society, 30(2), 93–104. https://doi.org/10.1057/jors.1979.22\n\n\n\nCredit to Vivek Srikrishnan, Environmental Systems Analysis"
  },
  {
    "objectID": "slides/lecture07-dps.html#some-more-technical-details",
    "href": "slides/lecture07-dps.html#some-more-technical-details",
    "title": "Direct policy search",
    "section": "Some more technical details",
    "text": "Some more technical details\nStudies consider many functional forms for \\(\\pi\\) including linear decision rules, neural networks, radial basis functions, and binary trees.\nEvolutionary algorithms are widely used to support policy search because the relationship between policy parameters and objective functions may be multimodal or discontinuous (which makes gradient-based techniques complicated or infeasible)."
  },
  {
    "objectID": "slides/lecture07-dps.html#policy-search-schematic",
    "href": "slides/lecture07-dps.html#policy-search-schematic",
    "title": "Direct policy search",
    "section": "Policy search schematic",
    "text": "Policy search schematic\n\nHerman et al., (2020)"
  },
  {
    "objectID": "slides/lecture07-dps.html#dps-example---radial-basis-function",
    "href": "slides/lecture07-dps.html#dps-example---radial-basis-function",
    "title": "Direct policy search",
    "section": "DPS example - radial basis function",
    "text": "DPS example - radial basis function\n\nQuinn et al., (2017)"
  },
  {
    "objectID": "slides/lecture07-dps.html#dps-example---radial-basis-function-1",
    "href": "slides/lecture07-dps.html#dps-example---radial-basis-function-1",
    "title": "Direct policy search",
    "section": "DPS example - radial basis function",
    "text": "DPS example - radial basis function\n\nMarangoni et al., (2021)"
  },
  {
    "objectID": "slides/lecture07-dps.html#dps-example---policy-tree",
    "href": "slides/lecture07-dps.html#dps-example---policy-tree",
    "title": "Direct policy search",
    "section": "DPS example - policy tree",
    "text": "DPS example - policy tree\n\nHerman and Giuliani (2018)"
  },
  {
    "objectID": "slides/lecture07-dps.html#dps-example---infrastructure-pathways",
    "href": "slides/lecture07-dps.html#dps-example---infrastructure-pathways",
    "title": "Direct policy search",
    "section": "DPS example - infrastructure pathways",
    "text": "DPS example - infrastructure pathways\n\n\n\nTrindade et al. (2019)"
  },
  {
    "objectID": "slides/lecture07-dps.html#who-will-implement-these-policies",
    "href": "slides/lecture07-dps.html#who-will-implement-these-policies",
    "title": "Direct policy search",
    "section": "Who will implement these policies?",
    "text": "Who will implement these policies?\n\nStudies document limited uptake of optimization-based methods by practitioners (e.g., Pianosi et al., (2020))\nDecision-makers may prefer simpler and more understandable tools"
  },
  {
    "objectID": "slides/lecture07-dps.html#even-with-promised-uptake-remember-these-challenges",
    "href": "slides/lecture07-dps.html#even-with-promised-uptake-remember-these-challenges",
    "title": "Direct policy search",
    "section": "Even with promised uptake, remember these challenges",
    "text": "Even with promised uptake, remember these challenges\n\nPay attention to convergence\nConsider your computational budget\nYou may not be able to gurantee more than a local “optimum”\nMOEAs are less “transparent” than conventional OR-based approaches and may have important but hidden assumptions"
  },
  {
    "objectID": "slides/lecture07-dps.html#this-week",
    "href": "slides/lecture07-dps.html#this-week",
    "title": "Direct policy search",
    "section": "This week",
    "text": "This week\n\nCase study Wednesday\nTutorial on DPS for lake problem Friday"
  },
  {
    "objectID": "slides/case01-rdm.html#define-key-concepts",
    "href": "slides/case01-rdm.html#define-key-concepts",
    "title": "Robust decision-making",
    "section": "Define key concepts",
    "text": "Define key concepts\nSpend a few minutes writing down your definitions of:\n\nWell-characterized uncertainty\nOptimal expected utility\nDeep uncertainty\nPrecautionary principle\nRobust decision\nRegret\nSatisficing\nKeeping options open"
  },
  {
    "objectID": "slides/case01-rdm.html#discuss-results",
    "href": "slides/case01-rdm.html#discuss-results",
    "title": "Robust decision-making",
    "section": "Discuss results",
    "text": "Discuss results\n\n\nWhat visual languages accompany different types of robustness metrics?\nWhat strategy should the decision-makers choose?"
  },
  {
    "objectID": "slides/case01-rdm.html#scrutinize-key-concepts",
    "href": "slides/case01-rdm.html#scrutinize-key-concepts",
    "title": "Robust decision-making",
    "section": "Scrutinize key concepts",
    "text": "Scrutinize key concepts\nLet’s discuss:\n\n\nWell-characterized vs. deep uncertainty\nOptimal expected utility vs. robustness\nPrecautionary principle vs. robustness\nRegret vs. satisficing\nConsidering too little vs. too much"
  },
  {
    "objectID": "slides/case01-rdm.html#example---home-elevation-problem",
    "href": "slides/case01-rdm.html#example---home-elevation-problem",
    "title": "Robust decision-making",
    "section": "Example - home elevation problem",
    "text": "Example - home elevation problem\n\n\nWhat can I do to know what robustness metrics are relevant?\nDo beliefs about uncertain drivers or problem scope uncertainties matter more?"
  },
  {
    "objectID": "slides/case01-rdm.html#your-examples",
    "href": "slides/case01-rdm.html#your-examples",
    "title": "Robust decision-making",
    "section": "Your examples",
    "text": "Your examples\nLet’s take turns applying some new concepts to our problem framing."
  },
  {
    "objectID": "slides/case01-rdm.html#this-week",
    "href": "slides/case01-rdm.html#this-week",
    "title": "Robust decision-making",
    "section": "This week",
    "text": "This week\n\nWe’ll go over the RDM tutorial on Friday together"
  },
  {
    "objectID": "slides/case01-rdm.html#next-week",
    "href": "slides/case01-rdm.html#next-week",
    "title": "Robust decision-making",
    "section": "Next week",
    "text": "Next week\n\nMonday - high-level overview of considering and navigating tradeoffs in our decision problems\nWednesday - case study (who can lead?)\nFriday - tutorial on visualizing trade-offs"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#what-is-uncertainty",
    "href": "slides/lecture03-uncertainty.html#what-is-uncertainty",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "What is uncertainty?",
    "text": "What is uncertainty?\n\nA lack of certainty; an inability to exactly describe current or future states and values\n\nParphrased from the Wikipedia entry on Uncertainty"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#two-overarching-types-of-uncertainty",
    "href": "slides/lecture03-uncertainty.html#two-overarching-types-of-uncertainty",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Two overarching types of uncertainty",
    "text": "Two overarching types of uncertainty\n\n\nAleatoric uncertainty results from randomness\n\nEpistemic uncertainty results from lack of knowledge"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#fundamental-limits-to-earth-systems-modeling",
    "href": "slides/lecture03-uncertainty.html#fundamental-limits-to-earth-systems-modeling",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Fundamental limits to Earth systems modeling",
    "text": "Fundamental limits to Earth systems modeling\nEarth systems are open, posing fundamental limitations on our ability to reduce all relevant areas of epistemic uncertainty.\n\n\n\nVerification and validation of numerical models of natural systems is impossible… because natural systems are never closed and because model results are always nonunique…\n\n\nThus, the primary value of models is heuristic: Models are representations, useful for guiding further study but not susceptible to proof.\n\n\n\n— Oreskes, N., Shrader-Frechette, K., & Belitz, K. (1994). Verification, validation, and confirmation of numerical models in the earth sciences. Science, 263(5147), 641-646."
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#example-climate-scenario-uncertainty-is-often-called-deep",
    "href": "slides/lecture03-uncertainty.html#example-climate-scenario-uncertainty-is-often-called-deep",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Example: Climate scenario uncertainty is often called “deep”",
    "text": "Example: Climate scenario uncertainty is often called “deep”\n\n\n\nHausfather and Peters (2020). RCP8.5 is a problematic scenario for near-term emissions"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#example-scenario-and-model-choices-reflect-deep-uncertainty-in-sea-level-rise-projections",
    "href": "slides/lecture03-uncertainty.html#example-scenario-and-model-choices-reflect-deep-uncertainty-in-sea-level-rise-projections",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Example: Scenario and model choices reflect “deep” uncertainty in sea-level rise projections",
    "text": "Example: Scenario and model choices reflect “deep” uncertainty in sea-level rise projections\n\nDoss-Gollin and Keller (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#example-triggering-managed-retreat",
    "href": "slides/lecture03-uncertainty.html#example-triggering-managed-retreat",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Example: Triggering managed retreat",
    "text": "Example: Triggering managed retreat\n\nHegde et al., (2025). Timing managed retreat for robust coastal adaptation strategies. Preprint."
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#myopic-strategies",
    "href": "slides/lecture03-uncertainty.html#myopic-strategies",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Myopic strategies",
    "text": "Myopic strategies\n\nHegde et al., (2025). Timing managed retreat for robust coastal adaptation strategies.. Preprint"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#schematic-representation",
    "href": "slides/lecture03-uncertainty.html#schematic-representation",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Schematic Representation",
    "text": "Schematic Representation\n\nReed et al., (2025). Addressing Uncertainty in multisector dynamics research"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#many-purposes---a-few-highlights-for-decision-analyses",
    "href": "slides/lecture03-uncertainty.html#many-purposes---a-few-highlights-for-decision-analyses",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Many purposes - a few highlights for decision analyses",
    "text": "Many purposes - a few highlights for decision analyses\n\n\nHow can we simplify our model to run it faster and sample more scenarios?\nWhat are worst-case scenarios?\nWhich actions perform well in a wide range of scenarios?\nWhat causes actions to fail?\nWhat types of observations should we collect to improve our model and better evaluate actions?\n\n\n\n\n\n\n\n\n\nNote\n\n\nNote the iterative nature of using uncertainty characterization and sensitivity analysis to inform and assess strategies"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#types-of-analyses",
    "href": "slides/lecture03-uncertainty.html#types-of-analyses",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Types of analyses",
    "text": "Types of analyses\n\nReed et al., (2025). Addressing Uncertainty in multisector dynamics research"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#analysis-methods",
    "href": "slides/lecture03-uncertainty.html#analysis-methods",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Analysis methods",
    "text": "Analysis methods\n\n\n\nReed et al., (2025). Addressing Uncertainty in multisector dynamics research"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#revisiting-the-managed-retreat-example",
    "href": "slides/lecture03-uncertainty.html#revisiting-the-managed-retreat-example",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Revisiting the managed retreat example",
    "text": "Revisiting the managed retreat example\n\n\n\n\n\nHegde et al., (2025). Timing managed retreat for robust coastal adaptation strategies.. Preprint\n\n\n\n\n\n\n\nHegde et al., (2025). Timing managed retreat for robust coastal adaptation strategies.. Preprint"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#example-from-my-research",
    "href": "slides/lecture03-uncertainty.html#example-from-my-research",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Example from my research",
    "text": "Example from my research\n\n\nBecause of the stated goals of the Justice40 Initiative, our main analytical goal was to evaluate different funding rules on equity and economic objectives and identify which rules perform well. To make claims about policy performance, our decision analysis was built on the following modeling chain…\n\n\n\nPollack et al., (2025). Funding rules that promote equity in climate adaptation outcomes\n\n\nOne reviewer complimented us for our extensive accounting of uncertainty. However, our (narrowly scoped!) research and policy questions did not call for factor mapping or prioritization (though we call for this in our discussion)."
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#some-personal-reflections",
    "href": "slides/lecture03-uncertainty.html#some-personal-reflections",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Some personal reflections",
    "text": "Some personal reflections\n\n\nAvoid inconsistent uncertainty framing and analysis (e.g., calculating expected values when framing uncertainties as “deep”)\nI feel like there’s not good guidance out there on saying for decision analysis purposes, how to represent and analyze various types of uncertainty\nDon’t lose sight of why uncertainty matters for your decision analysis (i.e., don’t ignore uncertainty but don’t overcomplicate how you handle it)\nRemember that peer-reviewed studies are units of overall decision analysis projects (i.e., don’t overstuff these studies with too many analyses and points)"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#this-week",
    "href": "slides/lecture03-uncertainty.html#this-week",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "This week",
    "text": "This week\n\nWe will only meet today this week - prioritize your project proposal and schedule office hours as needed\nLet me know if you are testing out the lab and need any help"
  },
  {
    "objectID": "slides/lecture03-uncertainty.html#next-week",
    "href": "slides/lecture03-uncertainty.html#next-week",
    "title": "Dealing with uncertainty in analyzing decisions",
    "section": "Next week",
    "text": "Next week\n\nStarting RDM module\nJournal club next Wednesday (stay on track with 9/29 readings)"
  },
  {
    "objectID": "slides/case02-tradeoffs.html#apply-concepts-on-trade-offs",
    "href": "slides/case02-tradeoffs.html#apply-concepts-on-trade-offs",
    "title": "Representing trade-offs",
    "section": "Apply concepts on trade-offs",
    "text": "Apply concepts on trade-offs\nSee if you can come up with examples of trade-offs that can occur across:\n\n\nObjectives\nTime scales\nSpatial scales\nPerspectives\nGroups\nSecular values\nSacred values\nSacred vs. secular values"
  },
  {
    "objectID": "slides/case02-tradeoffs.html#discuss-the-studys-approach",
    "href": "slides/case02-tradeoffs.html#discuss-the-studys-approach",
    "title": "Representing trade-offs",
    "section": "Discuss the study’s approach",
    "text": "Discuss the study’s approach\nWhat role did the following methods play?\n\n\nGroup discussions\nModel calibration\nParticipatory conceptual modeling\nToy model\nWorkshop observation and qualitative interviews"
  },
  {
    "objectID": "slides/case02-tradeoffs.html#discuss-results",
    "href": "slides/case02-tradeoffs.html#discuss-results",
    "title": "Representing trade-offs",
    "section": "Discuss results",
    "text": "Discuss results\n\n\nWhat did the conventional profit/conservation analysis suggest?\nHow did considering different resource users produce new insights?\nHow did considering secular and sacred values produce new insights?\nHow did interaction with stakeholders affect their understanding of the management problem?\n\n\n\nA stakeholder takeaway that resonated with me: “We sometimes have issues with poor stakeholders because they normally oppose whatever you propose. During the workshop, I actually came to understand why they normally oppose, and when I get a negative answer from them, I now understand why.”"
  },
  {
    "objectID": "slides/case02-tradeoffs.html#lets-discuss-the-following-claims",
    "href": "slides/case02-tradeoffs.html#lets-discuss-the-following-claims",
    "title": "Representing trade-offs",
    "section": "Let’s discuss the following claims",
    "text": "Let’s discuss the following claims\n\n\n“We propose that assertions of unconditional win-wins in ecosystem management should be scrutinized for losses of marginalized, undervalued, and less obvious stakeholders.”\n“To reveal [overlooked] trade-offs, analysis needs to reflect complexities in the biophysical system producing the trade-offs… assessments also need to map out the processes that determine who benefits from ecosystem services”\n“…well-being trade-offs may be overlooked if losers, through poverty or political marginalization, have no voice in decision making”\n“…conflicts can arise if primary stakeholders hold different sacred values than managers and government, or if sacred values are not reflected in the framing of decision making”\n“Explicit consideration of the diversity of values and possible taboos help explain the deeply felt conflicts that can result from ecosystem service trade-offs, and ultimately support improved decision making processes.”"
  },
  {
    "objectID": "slides/case02-tradeoffs.html#scrutinizing-claims-continued",
    "href": "slides/case02-tradeoffs.html#scrutinizing-claims-continued",
    "title": "Representing trade-offs",
    "section": "Scrutinizing claims, continued",
    "text": "Scrutinizing claims, continued\n\n\n“Values can be secularized to frame trade-offs as routine, as with the quantitative, monetary valuation and benefit-cost analyses that dominate ecosystem services assessment… however, this approach hides the incommensurable value conflicts, common in environmental decision making, that need to be deliberated and negotiated”\n“The combination of participatory modeling and scenarios can enhance transparency (through participatory modeling), accountability (through explicitly mapping winners and losers), and relevance (through a focus on people’s well-being)… has a greater chance of achieving socially equitable and sustainable decision making.”"
  },
  {
    "objectID": "slides/case02-tradeoffs.html#your-examples",
    "href": "slides/case02-tradeoffs.html#your-examples",
    "title": "Representing trade-offs",
    "section": "Your examples",
    "text": "Your examples\nLet’s take turns applying some new concepts to our problem framings."
  },
  {
    "objectID": "slides/case02-tradeoffs.html#friday",
    "href": "slides/case02-tradeoffs.html#friday",
    "title": "Representing trade-offs",
    "section": "Friday",
    "text": "Friday\n\nWe’ll rate and discuss some trade-off visualizations\nHW3 due (Sunday ok)"
  },
  {
    "objectID": "slides/case02-tradeoffs.html#next-week",
    "href": "slides/case02-tradeoffs.html#next-week",
    "title": "Representing trade-offs",
    "section": "Next week",
    "text": "Next week\n\nMonday: bringing things together with MORDM (guest lecture)\nWednesday: case study\nFriday: case study and HW 4 due (Sunday ok)"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#who-has-evaluated-a-decision-like-this",
    "href": "slides/lecture05-tradeoffs.html#who-has-evaluated-a-decision-like-this",
    "title": "Quantifying trade-offs",
    "section": "Who has evaluated a decision like this?",
    "text": "Who has evaluated a decision like this?\n\nApple 2025 MacBook Pro Comparison"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#robustness-vs.-cost",
    "href": "slides/lecture05-tradeoffs.html#robustness-vs.-cost",
    "title": "Quantifying trade-offs",
    "section": "Robustness vs. cost",
    "text": "Robustness vs. cost\nOur example from last week:\n\nGroves et al., 2019. Robust Decision Making (RDM): Application to Water Planning and Climate Policy."
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#robustness-vs.-optimality",
    "href": "slides/lecture05-tradeoffs.html#robustness-vs.-optimality",
    "title": "Quantifying trade-offs",
    "section": "Robustness vs. “optimality”",
    "text": "Robustness vs. “optimality”\nThink back to our lab from Friday. Anyone want to explain why this trade-off exists?\n\n\nOptimizing for a particular state of the world often produces strategies that are vulnerable to different-looking states of the world\nSimilarly, optimizing for robustness will sacrifice optimality within any single state of the world\n\n\n\nWe’ll spend next week talking about analyzing trade-offs and robustness together next week"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#economic-vs.-ecological",
    "href": "slides/lecture05-tradeoffs.html#economic-vs.-ecological",
    "title": "Quantifying trade-offs",
    "section": "Economic vs. ecological",
    "text": "Economic vs. ecological\nWho can offer examples from their project?"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#efficiency-vs.-equity",
    "href": "slides/lecture05-tradeoffs.html#efficiency-vs.-equity",
    "title": "Quantifying trade-offs",
    "section": "Efficiency vs. equity",
    "text": "Efficiency vs. equity\n\n\nNote that this represents a theorized trade-off between economic efficiency and one conception of equity.\n\n\n\n\nPollack et al. (2025)"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#equity-vs.-equity",
    "href": "slides/lecture05-tradeoffs.html#equity-vs.-equity",
    "title": "Quantifying trade-offs",
    "section": "Equity vs. equity",
    "text": "Equity vs. equity\n\nJafino et al. (2022)"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#other-considerations-to-keep-in-mind",
    "href": "slides/lecture05-tradeoffs.html#other-considerations-to-keep-in-mind",
    "title": "Quantifying trade-offs",
    "section": "Other considerations to keep in mind",
    "text": "Other considerations to keep in mind\n\nHow outcomes take shape over time\nThe spatial scale of the outcomes\nThe distribution of the outcomes\nSecular vs. sacred values\nWe may also see win-wins!\n\n\n\n\n\n\n\n\nWhat trade-offs and other considerations are relevant for your project?\n\n\nTake turns naming one"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#typically-focus-on-the-pareto-front",
    "href": "slides/lecture05-tradeoffs.html#typically-focus-on-the-pareto-front",
    "title": "Quantifying trade-offs",
    "section": "Typically, focus on the Pareto front",
    "text": "Typically, focus on the Pareto front\n\nNazari et al. (2023)"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#n-d-objective-scatter-plot",
    "href": "slides/lecture05-tradeoffs.html#n-d-objective-scatter-plot",
    "title": "Quantifying trade-offs",
    "section": "N-d objective scatter plot",
    "text": "N-d objective scatter plot\n\nHadka et al. (2015)"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#parallel-axis-plot",
    "href": "slides/lecture05-tradeoffs.html#parallel-axis-plot",
    "title": "Quantifying trade-offs",
    "section": "Parallel axis plot",
    "text": "Parallel axis plot\n\nHadka et al. (2015)"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#small-multiples",
    "href": "slides/lecture05-tradeoffs.html#small-multiples",
    "title": "Quantifying trade-offs",
    "section": "Small multiples",
    "text": "Small multiples\n\n\n\nDaw et al. (2015)"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#practical-considerations",
    "href": "slides/lecture05-tradeoffs.html#practical-considerations",
    "title": "Quantifying trade-offs",
    "section": "Practical considerations",
    "text": "Practical considerations\n\nFraming analyses inform which objectives to include (e.g., ViMMs) and trade-offs to present\n\nAnalysts map transparent trade-offs; aggregation is the normative decision-maker role\nUse multiple visualizations and experiment with new ones"
  },
  {
    "objectID": "slides/lecture05-tradeoffs.html#remainder-of-week",
    "href": "slides/lecture05-tradeoffs.html#remainder-of-week",
    "title": "Quantifying trade-offs",
    "section": "Remainder of week",
    "text": "Remainder of week\n\nCase study on Wednesday\nCritiquing ways to visualize tradeoffs on Friday\nHW 3 due Friday (Sunday ok)"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#why-is-scenario-based-planning-so-common",
    "href": "slides/lecture08-scendisc.html#why-is-scenario-based-planning-so-common",
    "title": "Scenario discovery",
    "section": "Why is scenario-based planning so common?",
    "text": "Why is scenario-based planning so common?\nAs Bryant and Lempert (2010) write, “Scenarios provide a commonly used and intuitively appealing means to communicate and characterize uncertainty in many decision support applications.” They often:\n\n\n“Build a narrative description that captures decision makers’ imaginations”\n“Aim to reduce overconfidence”\n“Encourage… groups to reflect on a broader range of futures”\n“Make it easier for decision makers to consider inconvenient or contentious futures”"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#limitations-to-traditional-scenarios",
    "href": "slides/lecture08-scendisc.html#limitations-to-traditional-scenarios",
    "title": "Scenario discovery",
    "section": "Limitations to traditional scenarios",
    "text": "Limitations to traditional scenarios\n\n\nOften seen as arbitrary or biased\nIt is difficult to a priori choose a small number of policy-relevant scenarios to summarize the breadth of uncertainty about the future\nCan work well with small groups of clients who have strong relationships with scenario developers, but can fail to work for wicked problems (e.g., diverse views, many large uncertainties, etc.,)"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#how-scenario-discovery-addresses-traditional-scenario-limitations",
    "href": "slides/lecture08-scendisc.html#how-scenario-discovery-addresses-traditional-scenario-limitations",
    "title": "Scenario discovery",
    "section": "How scenario discovery addresses traditional scenario limitations",
    "text": "How scenario discovery addresses traditional scenario limitations\n\n\nDefines scenarios as a set of future states of the world that represent vulnerabilities of proposed policies\nTypically, but not necessarily, uses statistical or data-mining algorithms in an effort to find easy-to-interpret, policy-relevant regions in the space of uncertain input parameters to computer simulation models\n\n\n\nIn more accessible terms: scenario discovery is a method to find, rather than pre-specify, decision-relevant scenarios to plan for. Thish is particularly relevant for a robust decision-making analysis."
  },
  {
    "objectID": "slides/lecture08-scendisc.html#the-steps",
    "href": "slides/lecture08-scendisc.html#the-steps",
    "title": "Scenario discovery",
    "section": "The steps",
    "text": "The steps\n\nGenerate data\nFind scenarios\nAssess scenarios\n\nGenerate more data?\nFind more scenarios?\n\nSelect scenarios to plan for"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#generate-data-from-a-simulation-model",
    "href": "slides/lecture08-scendisc.html#generate-data-from-a-simulation-model",
    "title": "Scenario discovery",
    "section": "Generate data from a simulation model",
    "text": "Generate data from a simulation model\n\nDefine an experimental design over the uncertain inputs while holding constant a candidate strategy\n\nBryant and Lempert (2010) argue for Latin Hypercube sampling, which is common.\n\nDefine a policy-relevant criteria to identify “cases of interest” that meet this threshold or fail"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#find-scenarios",
    "href": "slides/lecture08-scendisc.html#find-scenarios",
    "title": "Scenario discovery",
    "section": "Find scenarios",
    "text": "Find scenarios\nOften, studies use supervised classification machine learning to identify regions or rules that best differentiate successes and failures. The goals are:\n\nHigh coverage (analagous to recall)\nHigh density (analagous to precision)\nHigh interpretability (highly subjective!)"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#trade-offs-in-achieving-scenario-quality",
    "href": "slides/lecture08-scendisc.html#trade-offs-in-achieving-scenario-quality",
    "title": "Scenario discovery",
    "section": "Trade-offs in achieving scenario “quality”",
    "text": "Trade-offs in achieving scenario “quality”\nOften, coverage, density, and interpretability compete with one another"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#scenario-discovery-algorithms",
    "href": "slides/lecture08-scendisc.html#scenario-discovery-algorithms",
    "title": "Scenario discovery",
    "section": "Scenario discovery algorithms",
    "text": "Scenario discovery algorithms\n\n\nYou often see classification & regression trees (CART) or patient rule induction method (PRIM) - examples to follow\nBut, keep your goal in mind a use your imagination!\nNo matter what you implement, calculate diagnostics!"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#patient-rule-induction-method",
    "href": "slides/lecture08-scendisc.html#patient-rule-induction-method",
    "title": "Scenario discovery",
    "section": "Patient Rule Induction Method",
    "text": "Patient Rule Induction Method\n\n\n\n\n\n\n\n\n\nBryant and Lempert (2010)"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#classification-and-regression-trees",
    "href": "slides/lecture08-scendisc.html#classification-and-regression-trees",
    "title": "Scenario discovery",
    "section": "Classification and Regression Trees",
    "text": "Classification and Regression Trees\n\nHadka et al., (2025)"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#multi-trait-scenario-storyline-discovery",
    "href": "slides/lecture08-scendisc.html#multi-trait-scenario-storyline-discovery",
    "title": "Scenario discovery",
    "section": "Multi-trait Scenario Storyline Discovery",
    "text": "Multi-trait Scenario Storyline Discovery\n\nHadjimichael et al., (2024)"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#scenario-discovery-was-conceived-as-part-of-a-deliberative-iterative-process",
    "href": "slides/lecture08-scendisc.html#scenario-discovery-was-conceived-as-part-of-a-deliberative-iterative-process",
    "title": "Scenario discovery",
    "section": "Scenario discovery was conceived as part of a deliberative, iterative process",
    "text": "Scenario discovery was conceived as part of a deliberative, iterative process\n\n\nThis is one of the best opportunities for learning in the co-production process\nIt’s also a great opportunity for producing science insights (even outside of co-production settings!)\n\nSeveral optional readings demonstrate this by considering a wide range of ensembles beyond the limited SSP-RCP ones"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#dont-let-exploratory-modeling-taking-your-judgement-out-of-the-equation",
    "href": "slides/lecture08-scendisc.html#dont-let-exploratory-modeling-taking-your-judgement-out-of-the-equation",
    "title": "Scenario discovery",
    "section": "Don’t let exploratory modeling taking your judgement out of the equation",
    "text": "Don’t let exploratory modeling taking your judgement out of the equation\n\n\n\n\nScenario discovery is a process\nSimple visual analysis can inform algorithmic choices, but can also be effective in-and-of itself\n\n\n\n\n\n\nQuinn et al., (2017)"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#this-week",
    "href": "slides/lecture08-scendisc.html#this-week",
    "title": "Scenario discovery",
    "section": "This week",
    "text": "This week\n\nOptimization tutorial on Wed. by Prabhat\nCase study on Friday"
  },
  {
    "objectID": "slides/lecture08-scendisc.html#next-week",
    "href": "slides/lecture08-scendisc.html#next-week",
    "title": "Scenario discovery",
    "section": "Next week",
    "text": "Next week\n\nDiscussion about revisiting decision analysis framing\nPractice presentations Wed. & Fri."
  },
  {
    "objectID": "updates/dmdu.html",
    "href": "updates/dmdu.html",
    "title": "Your presentation and report outlines",
    "section": "",
    "text": "This is your opportunity to get feedback on how you’re structuring your ideas for your final presentation and report. Your presentation and report should tell the same story, but they should be appropriate for the context and audience. Your presentation is 15 minutes, so you need to deliver a tight and clear high-level story about your decision analysis. Your report is a draft of several sections of your next peer-reviewed publication about your decision analysis. You can, and should, extrapolate on context and details in this format but should not depart much from the narrative arc of your presentation. The two are great to work on simultaneously.\nIn this module, the main thing we are learning how to do is tie together the concepts from previous modules into a coherent and appropriate end-to-end analytical framework that address our decision problem. As such, this is the time for you in both presentation and report form to lay out:\n\nWhat is the decision problem?\nWhy is it important?\nHow are decisions made right now?\nWhat is the state-of-the-art in understanding this problem and status quo?\nWhat room is there for progress?\nWhat gap do you address?\n\nHow can you simply characterize your main refinement and contribution?\nOne way to do this convincingly is to frame it as a “status quo + 1” type of story (or something analogous)\nIt can help to focus on one key part of the logic of the overarching DMDU approach (e.g., this study focuses on trade-offs, this study focuses on how accounting for deep uncertainties reveals that currently preferred strategies may have high regret)\n\nHow will you address this in your study?\n\nWhat is “real” and “stylized” in your approach?\nWhat is out of scope?\n\nWhat do you hypothesize you will find?\nWhat are your plans to connect your decision analysis insights to decision-makers?\n\nFor both your presentation and report, imagine a general science audience. This means that you don’t have to completely simplify your language like you might for interviews with journalists, but you should define concepts that are specific to your field and the kind of decision analysis we learned in this course. For example, if you refer to parameters in your model without clarification (e.g., with a schematic representation of your model and avoiding abbreviations when possible), your colleagues in the course and I will likely not be able to follow your talk (or report). Similarly, if you do not define robustness, a more general audience will not follow your talk. And, if you do not define how you operationalize robustness in your setting, and why, your colleagues in the course and I will have a hard time evaluating your approach.\n\n\nConsider the presentation instructions and rubric. Your presentation outline should document each slide and a clear description of the main point and associated content. For example:\n\nTitle slide\n1: The Justice40 Initiative is one of the main policies that the US government employs to address equity concerns.\n\nThis is a short presentation so jump right into the current policy approach to deal with the decision problem\nIntroduce the key elements of the Justice40 Initiative (very quickly) and highlight parts of it that the study focuses on\nSpecifically need to introduce the spatial mismatch problem of intended goals and what they measure\n\n2: Can prioritizing funding at coarse spatial scales achieve equity across households?\n\nShow figure that captures this main idea\nIntroduce short bullets with animation on the figure that really emphasizes this potential spatial mismatch problem\n\n3: Adaptation outcomes are shaped by multi-level policy contexts, calling for a local case study.\n\nJustify case study approach\nExplain what is in and out of scope\nFor the Justice40 context, it’s relevant to highlight flooding issues in the case study, how the city has not obtained funds before from the programs “covered” by Justice40, etc.,\n\n\nI recommend targeting an average of 1.5 minutes per slide, which translates to 10 slides. Note that some slides will take more or less time, so 10 is guidance but not an expectation. You don’t want to rush through any detail-heavy slides or crucial points.\n\n\n\nConsider the report instructions and rubric. Your report outline should consist of the following:\n\nA full outline of your introduction\n\nUse strong signal sentences for each paragraph to trace the high-level narrative\nInclude sub-bullets under each signal sentence bullet to indicate key references, important evidence, any figures you plan to cite\n\nA partial outline of your methods (not nec. in the following order)\n\nOverview of the overall decision analysis workflow\n\nTake this opportunity to really highlight what your paper’s goals are and how the workflow accommodates answering your specific research questions\nTalk about any framing activities (e.g., workshops or meetings with stakeholders, policy document review, etc.,)\n\nHighlight the key uncertainties relevant to your decision analyslis (specifically your paper’s goals), explain the role uncertainty plays in your analysis (e.g., robustness considerations, scenario discovery, factor fixing, etc.,) and how you will address that\nThe relevant objectives (and trade-offs/synergies) for your study, how you identified them, and how you will measure them\nIf relevant, how you define and measure robustness and the justification for that\nYour exploratory modeling approach\n\nYou do not have to describe your model entirely here (though you might for your paper). For yoru report, you should think of this as being at the level of deatil for the “main” manuscript, but not for the supplementary info (e.g., extensive details on validation or calibration, data sources, etc.,)\n\nYour policy search approach (prespecified or generating solutions?) - if generating solutions, how will you handle deep uncertainties and do you consider robustness in your search?\n\nYou should comment on the justification for your policy search approach, including how you represent the policy structure. Is it reasonable in your setting for a decision-maker to implement policies in this form? How stylized is this representation?\n\n\nFigure sketches and bullet points of what question these anticipated results address and what hypotheses you represent in the figures"
  },
  {
    "objectID": "updates/dmdu.html#instructions",
    "href": "updates/dmdu.html#instructions",
    "title": "Your presentation and report outlines",
    "section": "",
    "text": "This is your opportunity to get feedback on how you’re structuring your ideas for your final presentation and report. Your presentation and report should tell the same story, but they should be appropriate for the context and audience. Your presentation is 15 minutes, so you need to deliver a tight and clear high-level story about your decision analysis. Your report is a draft of several sections of your next peer-reviewed publication about your decision analysis. You can, and should, extrapolate on context and details in this format but should not depart much from the narrative arc of your presentation. The two are great to work on simultaneously.\nIn this module, the main thing we are learning how to do is tie together the concepts from previous modules into a coherent and appropriate end-to-end analytical framework that address our decision problem. As such, this is the time for you in both presentation and report form to lay out:\n\nWhat is the decision problem?\nWhy is it important?\nHow are decisions made right now?\nWhat is the state-of-the-art in understanding this problem and status quo?\nWhat room is there for progress?\nWhat gap do you address?\n\nHow can you simply characterize your main refinement and contribution?\nOne way to do this convincingly is to frame it as a “status quo + 1” type of story (or something analogous)\nIt can help to focus on one key part of the logic of the overarching DMDU approach (e.g., this study focuses on trade-offs, this study focuses on how accounting for deep uncertainties reveals that currently preferred strategies may have high regret)\n\nHow will you address this in your study?\n\nWhat is “real” and “stylized” in your approach?\nWhat is out of scope?\n\nWhat do you hypothesize you will find?\nWhat are your plans to connect your decision analysis insights to decision-makers?\n\nFor both your presentation and report, imagine a general science audience. This means that you don’t have to completely simplify your language like you might for interviews with journalists, but you should define concepts that are specific to your field and the kind of decision analysis we learned in this course. For example, if you refer to parameters in your model without clarification (e.g., with a schematic representation of your model and avoiding abbreviations when possible), your colleagues in the course and I will likely not be able to follow your talk (or report). Similarly, if you do not define robustness, a more general audience will not follow your talk. And, if you do not define how you operationalize robustness in your setting, and why, your colleagues in the course and I will have a hard time evaluating your approach.\n\n\nConsider the presentation instructions and rubric. Your presentation outline should document each slide and a clear description of the main point and associated content. For example:\n\nTitle slide\n1: The Justice40 Initiative is one of the main policies that the US government employs to address equity concerns.\n\nThis is a short presentation so jump right into the current policy approach to deal with the decision problem\nIntroduce the key elements of the Justice40 Initiative (very quickly) and highlight parts of it that the study focuses on\nSpecifically need to introduce the spatial mismatch problem of intended goals and what they measure\n\n2: Can prioritizing funding at coarse spatial scales achieve equity across households?\n\nShow figure that captures this main idea\nIntroduce short bullets with animation on the figure that really emphasizes this potential spatial mismatch problem\n\n3: Adaptation outcomes are shaped by multi-level policy contexts, calling for a local case study.\n\nJustify case study approach\nExplain what is in and out of scope\nFor the Justice40 context, it’s relevant to highlight flooding issues in the case study, how the city has not obtained funds before from the programs “covered” by Justice40, etc.,\n\n\nI recommend targeting an average of 1.5 minutes per slide, which translates to 10 slides. Note that some slides will take more or less time, so 10 is guidance but not an expectation. You don’t want to rush through any detail-heavy slides or crucial points.\n\n\n\nConsider the report instructions and rubric. Your report outline should consist of the following:\n\nA full outline of your introduction\n\nUse strong signal sentences for each paragraph to trace the high-level narrative\nInclude sub-bullets under each signal sentence bullet to indicate key references, important evidence, any figures you plan to cite\n\nA partial outline of your methods (not nec. in the following order)\n\nOverview of the overall decision analysis workflow\n\nTake this opportunity to really highlight what your paper’s goals are and how the workflow accommodates answering your specific research questions\nTalk about any framing activities (e.g., workshops or meetings with stakeholders, policy document review, etc.,)\n\nHighlight the key uncertainties relevant to your decision analyslis (specifically your paper’s goals), explain the role uncertainty plays in your analysis (e.g., robustness considerations, scenario discovery, factor fixing, etc.,) and how you will address that\nThe relevant objectives (and trade-offs/synergies) for your study, how you identified them, and how you will measure them\nIf relevant, how you define and measure robustness and the justification for that\nYour exploratory modeling approach\n\nYou do not have to describe your model entirely here (though you might for your paper). For yoru report, you should think of this as being at the level of deatil for the “main” manuscript, but not for the supplementary info (e.g., extensive details on validation or calibration, data sources, etc.,)\n\nYour policy search approach (prespecified or generating solutions?) - if generating solutions, how will you handle deep uncertainties and do you consider robustness in your search?\n\nYou should comment on the justification for your policy search approach, including how you represent the policy structure. Is it reasonable in your setting for a decision-maker to implement policies in this form? How stylized is this representation?\n\n\nFigure sketches and bullet points of what question these anticipated results address and what hypotheses you represent in the figures"
  },
  {
    "objectID": "updates/dmdu.html#submitting-the-report",
    "href": "updates/dmdu.html#submitting-the-report",
    "title": "Your presentation and report outlines",
    "section": "Submitting the Report",
    "text": "Submitting the Report\nYou can submit one report, but please clearly separate the presentation outline and report outline.\nExport your writeup as a PDF and submit it to me over Slack. You can upload it to our course channel or direct message me, whichever you prefer. I will return my feedback to you before your practice presentation with ample time for you to incorporate it into your practice talk."
  },
  {
    "objectID": "updates/uncertainty.html",
    "href": "updates/uncertainty.html",
    "title": "Accounting for Key Uncertainties",
    "section": "",
    "text": "Your guiding question for this report is:\n“What are the key uncertainties in my decision analysis and how will I address them?”\nIt may be helpful to think of this as a rough draft of (a sizable component of) the Methods section of your paper. The more you include in your write-up, the more progress you make on your paper and the more feedback you get on your current analysis plan. Please consult this week’s readings to see papers with strong Methods text describing the key uncertainties in a decision analysis and how the authors address them.\nWhile you have flexibility for structuring your response to this guiding question, I would like you to produce a XLRM diagram for this assignment. Each key uncertainty you identify will end up as an “X” in your XLRM. You may take any classification approach you’d like for identifying the components in “X” and expanding on how you identified them and plan to address them, but I recommend a few representations from Srikrishnan et al. (2022). I think the whole article includes useful information for documenting your uncertainty considerations and analysis, but in particular you may benefit from considering the taxonomy of uncertainty types in Table 1, the model coupling diagrams in Figure 4, and the final section on best practices.\n\nSrikrishnan, V., Lafferty, D. C., Wong, T. E., Lamontagne, J. R., Quinn, J. D., Sharma, S., et al. (2022). Uncertainty analysis in multi‐sector systems: Considerations for risk analysis, projection, and planning for complex systems. Earths Future, 10(8). https://doi.org/10.1029/2021ef002644\nYou may feel trepidation about over-committing to a particular uncertain factor in your “X” and/or an analysis plan for addressing some uncertainties. I encourage you to embrace that feeling and take an inquiring approach to addressing it. For example, perhaps you feel that you lack the data to calibrate a parameter and you’re unsure about how to respresent it: you could use wide uniform priors or you could use different discrete values seen in the literature as scenarios. For this assignment, you do not have to choose an option - identifying your options and thinking through various pros/cons of different approaches is valuable. When you write your Methods text for your paper, regardless of what you choose, you should be able to defend your choice and justify it against defensible alternatives (or test that results are not sensitive to using a defensible alternative).\nAs with the last assignment, your report does not have to be in an essay form. You should write all ideas as complete sentences, even if you incorporate bullet and sub-bullets into your report structure. In addition to a XLRM, I think that a table with priors on parameters, a table documenting data sources, figure sketches, etc., could be helpful for drafting a comprehensive response.\n\n\n\n\n\n\nHints\n\n\n\nThe clearer your decision analyslis framing, the easier it is to identify key uncertainties and how to address them for your decision analysis. I recommend you think of the modules as cumulatively building on each other, not independent. That means that as you think about uncertainties in more detail, you may want to refine your analysis’s framing."
  },
  {
    "objectID": "updates/uncertainty.html#instructions",
    "href": "updates/uncertainty.html#instructions",
    "title": "Accounting for Key Uncertainties",
    "section": "",
    "text": "Your guiding question for this report is:\n“What are the key uncertainties in my decision analysis and how will I address them?”\nIt may be helpful to think of this as a rough draft of (a sizable component of) the Methods section of your paper. The more you include in your write-up, the more progress you make on your paper and the more feedback you get on your current analysis plan. Please consult this week’s readings to see papers with strong Methods text describing the key uncertainties in a decision analysis and how the authors address them.\nWhile you have flexibility for structuring your response to this guiding question, I would like you to produce a XLRM diagram for this assignment. Each key uncertainty you identify will end up as an “X” in your XLRM. You may take any classification approach you’d like for identifying the components in “X” and expanding on how you identified them and plan to address them, but I recommend a few representations from Srikrishnan et al. (2022). I think the whole article includes useful information for documenting your uncertainty considerations and analysis, but in particular you may benefit from considering the taxonomy of uncertainty types in Table 1, the model coupling diagrams in Figure 4, and the final section on best practices.\n\nSrikrishnan, V., Lafferty, D. C., Wong, T. E., Lamontagne, J. R., Quinn, J. D., Sharma, S., et al. (2022). Uncertainty analysis in multi‐sector systems: Considerations for risk analysis, projection, and planning for complex systems. Earths Future, 10(8). https://doi.org/10.1029/2021ef002644\nYou may feel trepidation about over-committing to a particular uncertain factor in your “X” and/or an analysis plan for addressing some uncertainties. I encourage you to embrace that feeling and take an inquiring approach to addressing it. For example, perhaps you feel that you lack the data to calibrate a parameter and you’re unsure about how to respresent it: you could use wide uniform priors or you could use different discrete values seen in the literature as scenarios. For this assignment, you do not have to choose an option - identifying your options and thinking through various pros/cons of different approaches is valuable. When you write your Methods text for your paper, regardless of what you choose, you should be able to defend your choice and justify it against defensible alternatives (or test that results are not sensitive to using a defensible alternative).\nAs with the last assignment, your report does not have to be in an essay form. You should write all ideas as complete sentences, even if you incorporate bullet and sub-bullets into your report structure. In addition to a XLRM, I think that a table with priors on parameters, a table documenting data sources, figure sketches, etc., could be helpful for drafting a comprehensive response.\n\n\n\n\n\n\nHints\n\n\n\nThe clearer your decision analyslis framing, the easier it is to identify key uncertainties and how to address them for your decision analysis. I recommend you think of the modules as cumulatively building on each other, not independent. That means that as you think about uncertainties in more detail, you may want to refine your analysis’s framing."
  },
  {
    "objectID": "updates/uncertainty.html#submitting-the-report",
    "href": "updates/uncertainty.html#submitting-the-report",
    "title": "Accounting for Key Uncertainties",
    "section": "Submitting the Report",
    "text": "Submitting the Report\nExport your writeup as a PDF and submit it to me over Slack. You can upload it to our course channel or direct message me, whichever you prefer."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENGS 199.20: Decision Analysis for Wicked Climate Problems",
    "section": "",
    "text": "This is the course website for the Fall 2025 edition of ENGS 199.20, Decision Analysis for Wicked Climate Problems, taught at Dartmouth College by Adam Pollack. This iteration of the website is a not-for-credit training/development trial of the course.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "ENGS 199.20: Decision Analysis for Wicked Climate Problems",
    "section": "Course Information",
    "text": "Course Information\n\nDetails on the class and course policies are provided in the syllabus.\nTopics and weekly materials can be found in the schedule.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#software-tools",
    "href": "index.html#software-tools",
    "title": "ENGS 199.20: Decision Analysis for Wicked Climate Problems",
    "section": "Software Tools",
    "text": "Software Tools\n\nThis course will use the Python programming language. Python is free, open source, and popular (and your instructor can help you with it more than other languages). No prior knowledge of Python is required. My recommendation is to use Visual Studio Code for coding. If you use VS code, install the official Python extension and Jupyter extension. Please see this page for using Python effectively in VS Code.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "labs/lab03-ucuq.html",
    "href": "labs/lab03-ucuq.html",
    "title": "Lab 3: Addressing Uncertainty in Multisector Dynamics Tutorials",
    "section": "",
    "text": "Today we’re going to get comfortable running code for uncertainty analysis and apply relevant methods to our decision analysis problem.\nToday’s objectives:\n\nReproduce the MSD UC/UQ book Python environment locally\nChoose a tutorial that resonates with you and run its code\nDemonstrate your conceptual understanding of the analysis you ran and its interpretation\n(Optional) Apply the workflow from the tutorial to your project data"
  },
  {
    "objectID": "labs/lab03-ucuq.html#overview",
    "href": "labs/lab03-ucuq.html#overview",
    "title": "Lab 3: Addressing Uncertainty in Multisector Dynamics Tutorials",
    "section": "",
    "text": "Today we’re going to get comfortable running code for uncertainty analysis and apply relevant methods to our decision analysis problem.\nToday’s objectives:\n\nReproduce the MSD UC/UQ book Python environment locally\nChoose a tutorial that resonates with you and run its code\nDemonstrate your conceptual understanding of the analysis you ran and its interpretation\n(Optional) Apply the workflow from the tutorial to your project data"
  },
  {
    "objectID": "labs/lab03-ucuq.html#lab-workflow",
    "href": "labs/lab03-ucuq.html#lab-workflow",
    "title": "Lab 3: Addressing Uncertainty in Multisector Dynamics Tutorials",
    "section": "Lab Workflow",
    "text": "Lab Workflow\n\nGitHub Repository Setup\nWe are interested in running the MSD e-book tutorials locally, so we will adapt the contribution instructions for this purpose.\n\nFork the MSD Uncertainty e-book then clone it locally\nCreate a conda or mamba environment with a compatible Python version for the e-book. You can check the compatible Python versions here. I created an environment ucuq_book with python version 3.12 and was able to run the notebooks locally.\nMake sure your environment is activated and cd to your local repository. Run pip install . to install all the packages you need to run the tutorials. I also installed ipykernel to set up the environment as a kernel and run the code in VSCode.\n\n\n\nChoose the tutorial to run and adapt for your project data\nTake a look at the online tutorials and identify the uncertainty analyses that you can envision running with your project data. Pick the one that you feel is the most relevant for your decision analysis goals and run this notebook locally. If you feel that none of these tutorials map well to your goals, let’s chat ;)\n\n\nRun the uncertainty analysis you chose on your project data\nThis is optional, but recommended. It is likely easier for you to run the uncertainty analysis in the directory where you work on your decision analysis. Figure out which packages to install into your project environment to run the uncertainty analysis you tried out in the previous step. Apply this uncertainty analysis with your decision analysis model. You do not have to successfully execute the uncertainty analysis to receive the full grade on this lab, but I recommend you try because it is more synergistic with your project than running code for a generic tutorial.\n\n\nExplain the uncertainty analysis you ran and its interpretation\nFor your lab report, please submit a concise write-up on the uncertainty analysis you ran and its interpretation for the underlying data. I recommend calibrating your writing content and tone for manuscript style (e.g., see the “Sensitivity Analysis” section of the Methods in this paper and also see how the authors interpret the results of this procedure in the Results subsection “Uncertainties that drive the variance of projected damages”). I expect the following qualities in your text:\n\na clear English explanation of the analysis plan\n\nthis includes emphasizing what question(s) the analysis addresses and why it is appropriate for the problem at hand\n\njustification of the chosen sampling plan and sample size\nspecific reference to the analysis results\n\ne.g., include a figure (or more than one) in your report\n\nthoughtful interpretation\n\nthis includes highlighting key assumptions\n\naccurate references\n\nyou might only cite the MSD book and references within it, but you might also find it helpful to look beyond those\n\n\nIf you ran the analysis on your project data, you only have to explain the analysis and interpretation for that example.\n\n\nSubmit your lab report\nFor the online training, you can send me the pdf of your lab report. I do not need to see any code, but you are welcome to share your repositories with me if you would like me to take a look. I recommend wrapping up your notebook and lab report into a GitHub repository so that you can bolster your online portfolio, but I understand that this is time-intensive so it is not required."
  },
  {
    "objectID": "labs/lab02-intro.html",
    "href": "labs/lab02-intro.html",
    "title": "Lab 2: Python and GitHub Bootcamp",
    "section": "",
    "text": "Today we’re going to practice our lab GitHub workflow and learn/refresh Python code for the types of analyses we’ll see in future labs. There’s a lot of content in this lab, so I will host an extra hour session on Tuesday so that I’m available for questions. Today’s lab is not graded but I will provide feedback on your submission to provide guidance on expectations for future submissions.\nToday’s objectives:\n\nBecome more comfortable using numpy for efficient array-based operations\nBecome more comfortable using pandas for structured data analysis\nCreate a reproducible analysis with GitHub\nGet some practice using Python to address a toy decision analysis problem\n\nIt’s pretty easy to find tutorials online for #1-#3 above. I thought it would be fun to have a more applied example. I will link to reference material and point out tips for where certain coding decisions come from to help you engage with that type of material.\nLet’s pretend you just got hired as a flood risk analyst for an emergency management department. The city council has a budget to spend on flood mitigation strategies before the next storm season. Your job is to analyze two potential flood scenarios and provide actionable recommendations.\n\n\n\n\n\n\nThis tutorial is for learning coding skills. We will violate a lot of decision analysis principles, which I will teach later in the term, in service of meeting our primary objectives for today’s lab."
  },
  {
    "objectID": "labs/lab02-intro.html#overview",
    "href": "labs/lab02-intro.html#overview",
    "title": "Lab 2: Python and GitHub Bootcamp",
    "section": "",
    "text": "Today we’re going to practice our lab GitHub workflow and learn/refresh Python code for the types of analyses we’ll see in future labs. There’s a lot of content in this lab, so I will host an extra hour session on Tuesday so that I’m available for questions. Today’s lab is not graded but I will provide feedback on your submission to provide guidance on expectations for future submissions.\nToday’s objectives:\n\nBecome more comfortable using numpy for efficient array-based operations\nBecome more comfortable using pandas for structured data analysis\nCreate a reproducible analysis with GitHub\nGet some practice using Python to address a toy decision analysis problem\n\nIt’s pretty easy to find tutorials online for #1-#3 above. I thought it would be fun to have a more applied example. I will link to reference material and point out tips for where certain coding decisions come from to help you engage with that type of material.\nLet’s pretend you just got hired as a flood risk analyst for an emergency management department. The city council has a budget to spend on flood mitigation strategies before the next storm season. Your job is to analyze two potential flood scenarios and provide actionable recommendations.\n\n\n\n\n\n\nThis tutorial is for learning coding skills. We will violate a lot of decision analysis principles, which I will teach later in the term, in service of meeting our primary objectives for today’s lab."
  },
  {
    "objectID": "labs/lab02-intro.html#lab-workflow",
    "href": "labs/lab02-intro.html#lab-workflow",
    "title": "Lab 2: Python and GitHub Bootcamp",
    "section": "Lab Workflow",
    "text": "Lab Workflow\n\nGitHub Repository Setup\n\nGo to GitHub and create a new repository called “toy-flood-analysis”\nClone the repo locally (I recommend using ssh as discussed last lab)\nCreate the folder structure below\n\ntoy-flood-analysis/\n   ├── data/\n   ├── notebooks/\n   ├── figures/\n   └── README.md\n   └── .gitignore\nYour README.md should include the following structure:\n# Toy Flood Analysis\n\n## Project Description\n[explain project]\n\n## How to Run\n[reproducibility instructions]\nHere’s an example of a GitHub repository with reproducibility instructions: https://github.com/IMMM-SFA/burleyson-etal_2024_applied_energy. You can check out the raw markdown here.\nRemember to regularly add, commit, and push changes to your remote branch.\n\n\nSetting up our computing environment\nCreate a file called environment.yml in your project directory. Add the following contents and save:\nname: lab2\nchannels:\n  - conda-forge\ndependencies:\n  - python\n  - pandas # great for tabular data\n  - numpy # great for efficient operations on large arrays\n  - matplotlib # standard plotting library for python\n  - nbconvert # for converting our notebook to pdf for submission\n  - ipykernel # for setting up the kernel for using this env in jupyter\n  - seaborn # good for exploratory data analysis and some final figures\n  - jupyter \nRun mamba env create -f envvironment.yml. Run mamba activate lab2. Set up the kernel for the environment with python -m ipykernel install --user --name lab2.\n\n\nSetting up our analysis\nCreate a new .ipynb file in the notebooks/ subdirectory. In the first cell, let’s import our tools for the analysis.\n\n\nCode\n# Core numerical computing\nimport numpy as np\n\n# Data manipulation and analysis\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nBuilding our toy problem\nOur city is fake, so we have to build it. We will create a fake digital elevation grid, a fake built environment, and fake flood scenarios.\nBelow, we define a function to create an elevation grid for the city. We use numpy because it’s a great tool for setting up our 2d array. We will use a meshgrid to set up our grid.\nPlease read the comments and test out any lines of code you want to understand better. I encourage you to reference the numpy documentation to read about any functions you’re seeing for the first time.\n\n\nCode\ndef create_terrain(width=100, height=80):\n    \"\"\"\n    Create an elevation grid using numpy meshgrid\n\n    Key features:\n        - Rivers are the lowest elevations (like real topography)\n        - Land slopes toward rivers creating natural drainage\n        - Floodplains are flat areas adjacent to rivers\n        - Higher elevations are away from water\n\n    Keyword arguments:\n    width -- the number of columns in our elevation grid\n    height -- the number of rows in our elevation grid\n    \"\"\"\n    # Set seed for reproducible terrain\n    rng = np.random.default_rng(42)\n\n    # Create coordinate grids\n    x = np.linspace(0, width/10, width)\n    y = np.linspace(0, height/10, height)\n    X, Y = np.meshgrid(x, y)\n\n    # Define river path (straight east-west through middle)\n    river_center_y = height // 2\n    \n    # Create a slightly meandering river path\n    river_path = river_center_y + 6 * np.sin(np.linspace(0, 3*np.pi, width))\n    \n    # Calculate distance from each point to the river\n    # Elevation increases with distance from water\n    distance_to_river = np.zeros((height, width))\n    \n    for i in range(height):\n        for j in range(width):\n            river_y_at_x = river_path[j]\n            distance_to_river[i, j] = abs(i - river_y_at_x)\n    \n    # Create elevation based on distance to water\n    # Start with river at elevation 0, then build up from there\n    \n    # Base elevation: increases as you move away from river\n    elevation = distance_to_river * 1.2  # 1.2m rise per grid cell from river\n    \n    # Add gentle regional slope (slightly higher in the east)\n    elevation += X * 1.5\n    \n    # Add some rolling hills in areas far from the river\n    upland_mask = distance_to_river &gt; 10\n    hill_effect = np.where(upland_mask, \n                          8 * np.sin(X * 0.6) * np.cos(Y * 0.5), \n                          0)\n    elevation += hill_effect\n    \n    # Create realistic floodplain (very flat near river)\n    floodplain_mask = distance_to_river &lt; 3\n    elevation[floodplain_mask] = distance_to_river[floodplain_mask] * 0.3 + X[floodplain_mask] * 0.5\n    \n    # Ensure river channel is the absolute lowest point\n    river_mask = distance_to_river &lt; 1\n    elevation[river_mask] = X[river_mask] * 0.2  # Gentle downstream slope\n    \n    # Add small-scale variation\n    elevation += np.random.normal(0, 0.5, elevation.shape)\n    \n    # Ensure no negative elevations\n    elevation = np.maximum(elevation, 0)\n    \n    return elevation, distance_to_river\n\n# Generate terrain\nterrain, river_distance = create_terrain()\n\n# Create a much better visualization\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Elevation map with proper terrain colormap\nim1 = ax.imshow(terrain, cmap='terrain', origin='lower')\nax.axis('off')\nplt.colorbar(im1, ax=ax, label='Elevation (m)')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow we’ll use both numpy and pandas to set up our building inventory. There are a lot of things that go into where people build & occupy buildings with respect to flood hazard, but we’re going to ignore that in setting up this problem. (You have no idea how uncommon it is for your instructor to let explanations on those things slide!)\n\n\nCode\ndef generate_building_inventory(terrain, n_buildings=500, min_elevation=10):\n    \"\"\"\n    Creates building inventory, excluding buildings in the river.\n    \n    Generate buildings randomly, then filter out those \n    below minimum elevation.\n\n    Keyword arguments:\n    terrain -- digital elevation grid\n    n_buildings -- the max number of buildings in the city\n    min_elevation -- the lowest elevation a building can have\n    \"\"\"\n    # Set seed for reproducible inventory\n    rng = np.random.default_rng(42)\n\n    # Keep track of grid dimensions\n    height, width = terrain.shape\n    \n    # Vectorized location sampling\n    x_coords = rng.integers(0, width, n_buildings)\n    y_coords = rng.integers(0, height, n_buildings)\n    \n    # Get elevations using numpy indexing\n    elevations = terrain[y_coords, x_coords]\n    \n    # Keep only buildings above minimum elevation\n    # This automatically excludes buildings in the river, and some near\n    valid_mask = elevations &gt; min_elevation\n    \n    # Apply filter to all arrays\n    x_coords = x_coords[valid_mask]\n    y_coords = y_coords[valid_mask]\n    elevations = elevations[valid_mask]\n    \n    # Check how many buildingds we're left with\n    n_actual = len(x_coords)\n    \n    # Generate building characteristics (vectorized)\n    building_types = rng.choice(\n        ['residential', 'commercial', 'industrial'],\n        size=n_actual,\n        p=[0.75, 0.2, 0.05]\n    )\n    \n    # Structure values by type (vectorized with masks)\n    values = np.zeros(n_actual)\n    residential_mask = (building_types == 'residential')\n    commercial_mask = (building_types == 'commercial')\n    industrial_mask = (building_types == 'industrial')\n    \n    values[residential_mask] = rng.lognormal(12, 0.5, residential_mask.sum())\n    values[commercial_mask] = rng.lognormal(13, 0.7, commercial_mask.sum())\n    values[industrial_mask] = rng.lognormal(14, 0.8, industrial_mask.sum())\n\n    # Create DataFrame\n    buildings = pd.DataFrame({\n        'bld_id': [f'{i:04d}' for i in range(n_actual)],\n        'x': x_coords,\n        'y': y_coords,\n        'elevation': elevations,\n        'type': building_types,\n        'value': values\n    })\n    \n    return buildings\n\n# Generate building inventory\nbuildings = generate_building_inventory(terrain, n_buildings=500, min_elevation=10)\n\n\n# Visualize building locations on terrain\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Show terrain\nimg = ax.imshow(terrain, cmap='terrain', origin='lower', alpha=0.8)\ncbar = plt.colorbar(img)\ncbar.set_label('Elevation (m)', size=14, rotation=90)\ncbar.ax.tick_params(labelsize=12)\n\n# Add building locations colored by type\ncolors = {'residential': 'red', 'commercial': 'blue', 'industrial': 'purple'}\nfor building_type, color in colors.items():\n    type_buildings = buildings[buildings['type'] == building_type]\n    ax.scatter(type_buildings['x'], type_buildings['y'], \n               c=color, s=20, alpha=0.7, label=building_type)\n\nax.legend(bbox_to_anchor=(.5, -.1),\n          loc='center',\n          ncols=3,\n          fontsize=14,\n          title_fontsize=14,\n          title='Occupancy')\nax.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow we’ll generate two flood scenarios. We’ll organize our parameterizations for each scenario using dictionaries. Then we’ll simulate the floods in a highly stylized fashion to get depth grids.\n\n\nCode\nscenarios = {\n        'moderate': {\n            'name': 'Moderate Flood',\n            'rainfall_mm': 100,\n            'river_discharge_multiplier': 3.0,\n            'storm_duration_hours': 12\n        },\n        'extreme': {\n            'name': 'Extreme Flood', \n            'rainfall_mm': 200,\n            'river_discharge_multiplier': 6.0,\n            'storm_duration_hours': 24\n        }\n    }\n\ndef simulate_flood(terrain, river_distance, scenario_params):\n    \"\"\"\n    Simple linear flood model based on terrain characteristics.\n    \n    Flood depth depends on:\n    - Base scenario intensity\n    - Distance to river (closer = more flooding)\n    - Elevation (lower = more flooding)\n    - Random noise (varies by distance to river)\n\n    Keyword arguments:\n    terrain -- digital elevation grid\n    river_distance -- grid of distances from river\n    scenario_params -- dict of flood scenario details\n    \"\"\"\n    # Random seed for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Base flood intensity from scenario\n    base_intensity = scenario_params['rainfall_mm'] / 1000  # Convert to meters\n    river_intensity = base_intensity * scenario_params['river_discharge_multiplier']\n    \n    # Linear model for flood depth\n    # Flood depth = base + river_effect + elevation_effect + noise\n    \n    # River effect: flooding decreases with distance from river\n    river_effect = np.maximum(0, river_intensity * (10 - river_distance) / 10)\n    \n    # Elevation effect: flooding decreases with elevation above minimum\n    min_elevation = np.min(terrain)\n    elevation_effect = np.maximum(0, base_intensity * (35 - (terrain - min_elevation)) / 35)\n    \n    # Spatially varying noise\n    # More variability near river (complex hydrology)\n    # Less variability on uplands (simple runoff)\n    noise_scale = 0.1 * base_intensity * (1 + 0.5 / (1 + river_distance))\n    noise = rng.normal(0, noise_scale, terrain.shape)\n    \n    # Combine effects\n    flood_depth = river_effect + elevation_effect + noise\n    \n    # Ensure realistic constraints\n    # No negative depths\n    flood_depth = np.maximum(flood_depth, 0)\n    \n    # Cap maximum depth (physical limit)\n    flood_depth = np.minimum(flood_depth, 5.0)\n    \n    # Areas above certain elevation rarely flood\n    high_elevation_mask = terrain &gt; (min_elevation + 40)\n    flood_depth[high_elevation_mask] *= 0.1  # Reduce flooding on high ground\n    \n    # Very small depths set to zero (not meaningful flooding)\n    flood_depth[flood_depth &lt; 0.02] = 0\n    \n    return flood_depth\n\n# Run the simplified simulations\nflood_results = {}\n\nfor scenario_id, params in scenarios.items():\n    flood_depth = simulate_flood(terrain, river_distance, params)\n    flood_results[scenario_id] = {\n        'depth': flood_depth,\n        'params': params,\n        'flooded_cells': np.sum(flood_depth &gt; 0.01),  # 1cm threshold\n        'max_depth': np.max(flood_depth),\n        'mean_depth': np.mean(flood_depth[flood_depth &gt; 0.01]) if np.any(flood_depth &gt; 0.01) else 0\n    }\n\n# Enhanced visualization with proper handling of zero flood depths\nfig, axes = plt.subplots(2, 2, figsize=(16, 12), sharey='col')\n\nfor i, (scenario_id, results) in enumerate(flood_results.items()):\n    row = i\n    \n    # Only show flood depths, gray for no flooding\n    flood_depth_viz = results['depth'].copy()\n    \n    # Create a masked array where zeros are masked\n    flood_depth_masked = np.ma.masked_where(flood_depth_viz &lt;= 0.01, flood_depth_viz)\n    \n    # Show only flood depths (zeros will be transparent, showing gray background)\n    im = axes[row, 0].imshow(flood_depth_masked, cmap='Blues', vmin=0, vmax=1)\n    \n    axes[row, 0].set_title(f\"{results['params']['name']}\")\n    \n    # Add colorbar with proper label\n    cbar = plt.colorbar(im, ax=axes[row, 0], extend='max', label='Flood Depth (m)')\n    \n    # Show flooded and non-flooded areas\n    # Get all terrain elevations\n    all_elevations = terrain.flatten()\n    all_flood_depths = results['depth'].flatten()\n    \n    # Separate flooded and non-flooded areas\n    flooded_mask = all_flood_depths &gt; 0.01\n    non_flooded_mask = all_flood_depths &lt;= 0.01\n    \n    # Plot non-flooded areas (gray triangles)\n    if np.any(non_flooded_mask):\n        axes[row, 1].scatter(all_elevations[non_flooded_mask], \n                           all_flood_depths[non_flooded_mask],\n                           c='lightgray', marker='^', s=8, alpha=1, \n                           label='No flooding')\n    \n    # Plot flooded areas (blue circles)\n    if np.any(flooded_mask):\n        scatter = axes[row, 1].scatter(all_elevations[flooded_mask], \n                                     all_flood_depths[flooded_mask],\n                                     c=all_flood_depths[flooded_mask], \n                                     cmap='Blues', s=12, alpha=1,\n                                     vmin=0, vmax=1,\n                                     label='Flooded')\n        \n        # Add colorbar for scatter plot\n        plt.colorbar(scatter, ax=axes[row, 1], extend='max', label='Flood Depth (m)')\n    \n    axes[row, 1].set_xlabel('Terrain Elevation (m)')\n    axes[row, 1].set_ylabel('Flood Depth (m)')\n    axes[row, 1].set_title(f\"{results['params']['name']}\\nElevation vs Flood Depth\")\n    axes[row, 1].grid(True, alpha=0.3)\n    axes[row, 1].legend()\n    \n    # Add some summary statistics as text\n    flooded_cells = np.sum(flooded_mask)\n    total_cells = len(all_elevations)\n    flood_percentage = (flooded_cells / total_cells) * 100\n       \n    # Add text box with statistics\n    stats_text = f'Flooded: {flooded_cells:,} cells ({flood_percentage:.1f}%)\\n'\n    stats_text += f'Max depth: {results[\"max_depth\"]:.2f}m'\n    axes[row, 1].text(0.42, 0.98, stats_text, transform=axes[row, 1].transAxes, \n                     verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing risk\nNow it’s time to calculate flood damages. We will identify which buildings face flooding in each scenario, then will apply (made up) depth-damage functions to translate exposure into damages. These functions return damage as a fraction of building value, so we will scale these outputs by each building’s value.\n\n\nCode\n# Get building depths for each scenario\nfor scenario_id, results in flood_results.items():\n    depths = results['depth'][buildings['y'], buildings['x']]\n    buildings['depth_{}'.format(scenario_id)] = pd.Series(depths)\n\n# Function for estimating damages\ndef calc_flood_damages(buildings, depth_col):\n    \"\"\"\n    Estimate flood damages for the merged flood scenarios\n\n    Keyword arguments:\n    buildings -- our building inventory\n    depth_cols -- name of the flood scenario to calculate damages\n\n    Note: we must have preprocessed depth columns in the buildings dataframe\n    to avoid throwing an Error (it's much better to explicitly handle this if you can!)\n    \"\"\"\n\n    # Set seed for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Helpful to have a function for the depth-damage curves\n    def damage_curve_deterministic(depth, building_type):\n        \"\"\"\n        Base damage curves without noise - cleaner for teaching the core concept.\n        \"\"\"\n        if depth &lt;= 0.01: \n            return 0\n        elif depth &lt;= 0.25:  \n            if building_type == 'residential':\n                return 0.1 + 0.3 * (depth - 0.1) / 0.4\n            else:\n                return 0.05 + 0.2 * (depth - 0.1) / 0.4\n        elif depth &lt;= .75:  \n            if building_type == 'residential':\n                return 0.4 + 0.3 * (depth - 0.5) / 0.5\n            else:\n                return 0.25 + 0.4 * (depth - 0.5) / 0.5\n        else:  \n            if building_type == 'residential':\n                return min(0.7 + 0.2 * (depth - 1.0), 0.95)\n            else:\n                return min(0.65 + 0.25 * (depth - 1.0), 0.90)\n    \n    # Calculate deterministic damage ratios using vectorized pandas apply\n    damage_ratios_base = buildings.apply(\n        lambda row: damage_curve_deterministic(row[depth_col], row['type']), \n        axis=1\n    )\n\n    # Add noise\n    # Uncertainty increases with predicted damage level\n    # Low damage = low uncertainty, High damage = high uncertainty\n    # Create noise scale that increases with predicted damage\n    # No noise for zero damage, maximum noise for high damage\n    noise_scale = 0.15 * damage_ratios_base  # 15% relative noise\n        \n    # Generate heteroskedastic noise\n    damage_noise = rng.normal(0, noise_scale)\n    \n    # Apply noise to damage ratios\n    damage_ratios_noisy = damage_ratios_base + damage_noise\n    \n    # Ensure realistic bounds (damage ratios must be between 0 and 1)\n    damage_ratios_final = np.clip(damage_ratios_noisy, 0, 1)\n        \n    # Return damage amounts\n    return buildings['value'] * damage_ratios_final\n\n# Calculate damages\n# Identify depth columns and loop through them\n# Store the scenario name to create the damage column\n\n# This is called list comprehension - very handy!\ndepth_cols = [x for x in buildings.columns if 'depth' in x]\n# Our names of interest come after a '_' so we can split our strings\nfor d_col in depth_cols:\n    # This returns a list of strings after splitting on '_'\n    # and we want the last indexed string in that list\n    scenario = d_col.split('_')[-1]\n    dam_col = 'damage_{}'.format(scenario)\n    damage = calc_flood_damages(buildings, d_col)\n    buildings[dam_col] = damage\n\n\n## Diagnostic plot\nfig, (ax1, ax2) = plt.subplots(2, 1,\n                               sharex=True,\n                               figsize=(14, 10))\n    \n# Define colors and scenario info\nscenarios_plot = {\n    'moderate_flood': {\n        'color': 'blue',\n        'label': 'Moderate Flood',\n        'depth_col': 'depth_moderate',\n        'damage_col': 'damage_moderate'\n    },\n    'extreme_flood': {\n        'color': 'red', \n        'label': 'Extreme Flood',\n        'depth_col': 'depth_extreme',\n        'damage_col': 'damage_extreme'\n    }\n}\n\nmin_dam_depth = .01\n\n# Cumulative damage curves\nfor scenario_id, info in scenarios_plot.items():\n    # Filter to buildings with flooding in this scenario\n    flooded_buildings = buildings[\n        buildings[info['depth_col']] &gt; min_dam_depth\n    ].copy()\n    \n    if len(flooded_buildings) &gt; 0:\n        # Sort by flood depth\n        flooded_buildings = flooded_buildings.sort_values(info['depth_col'])\n        \n        # Calculate cumulative damage\n        cumulative_damage = flooded_buildings[info['damage_col']].cumsum()\n        flood_depths = flooded_buildings[info['depth_col']]\n        \n        # Create step plot\n        ax1.step(flood_depths, cumulative_damage / 1e6, \n                where='post', linewidth=2.5, \n                color=info['color'], label=info['label'])\n        \nax1.set_ylabel('Cumulative Damage ($ millions)', size=14)\nax1.set_title('Cumulative Damage by Flood Depth\\n(Step plot shows damage accumulation)', fontsize=14)\nax1.grid(True, alpha=0.3)\nax1.legend(fontsize=12)\n\n# Overlapping histograms of flood depths\nfor scenario_id, info in scenarios_plot.items():\n    flood_depths = buildings[info['depth_col']]\n    flooded_depths = flood_depths[flood_depths &gt; min_dam_depth]\n    \n    if len(flooded_depths) &gt; 0:\n        ax2.hist(flooded_depths, bins=30, alpha=0.6, \n                color=info['color'], label=info['label'],\n                edgecolor='black', linewidth=0.5, density=True)\n\nax2.set_xlabel('Flood Depth (m)', size=14)\nax2.set_ylabel('Density', size=14)\nax2.grid(True, alpha=0.3)\n\n# Add summary statistics as text annotations\nsummary_text = \"SCENARIO SUMMARY:\\n\\n\"\n\nfor scenario_id, info in scenarios_plot.items():\n    flood_depths = buildings[info['depth_col']]\n    damages = buildings[info['damage_col']]\n    \n    # Calculate statistics\n    n_flooded = np.sum(flood_depths &gt; min_dam_depth)\n    total_damage = damages.sum()\n    mean_damage = damages[damages &gt; 0].mean() if np.any(damages &gt; 0) else 0\n    max_depth = flood_depths.max()\n    mean_depth = flood_depths[flood_depths &gt; min_dam_depth].mean() if np.any(flood_depths &gt; min_dam_depth) else 0\n    \n    summary_text += f\"{info['label']}:\\n\"\n    summary_text += f\"  • Buildings flooded: {n_flooded:,}\\n\"\n    summary_text += f\"  • Total damage: ${total_damage/1e6:.1f}M\\n\"\n    summary_text += f\"  • Mean damage: ${mean_damage/1e3:.0f}K\\n\"\n    summary_text += f\"  • Max depth: {max_depth:.2f}m\\n\"\n    summary_text += f\"  • Mean depth: {mean_depth:.2f}m\\n\\n\"\n\n# Add text box with statistics\nsummary_text += f\"Total buildings: {len(buildings)}\"\nax2.text(0.8, 0.2, summary_text, transform=ax2.transAxes,\n            fontsize=12,\n            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))\n\nax1.tick_params(labelsize=12)\nax2.tick_params(labelsize=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNow we compare different flood mitigation options. We will make very rudimentary assumptions about costs and benefits for different actions. While the text table does not render colors on the website, checkout what happens in your Jupyter notebook!\n\n\nCode\ndef analyze_mitigation_strategies_multi_scenario(buildings):\n    \"\"\"\n    Evaluates mitigation strategies across both flood scenarios.\n    \"\"\"\n    \n    # Calculate baseline damages for both scenarios\n    baseline_moderate = buildings['damage_moderate'].sum()\n    baseline_extreme = buildings['damage_extreme'].sum()\n    \n    print(f\"Baseline damages:\")\n    print(f\"  Moderate flood: ${baseline_moderate:,.0f}\")\n    print(f\"  Extreme flood: ${baseline_extreme:,.0f}\")\n    \n    # Define mitigation strategies\n    strategies = {\n        'do_nothing': {\n            'cost': 0,\n            'damage_reduction_moderate': 0,\n            'damage_reduction_extreme': 0,\n            'description': 'Accept current flood risk'\n        },\n        'early_warning': {\n            'cost': 500_000,\n            'damage_reduction_moderate': 0.15,  # 15% reduction through evacuation\n            'damage_reduction_extreme': 0.12,   # Less effective in extreme events\n            'description': 'Flood warning system + evacuation plans'\n        },\n        'levees': {\n            'cost': 25_000_000,\n            'damage_reduction_moderate': 0.70,  # Very effective for moderate floods\n            'damage_reduction_extreme': 0.45,   # Less effective for extreme floods\n            'description': 'Construct flood levees along river'\n        },\n        'buyouts': {\n            'cost': 15_000_000,  # Simplified cost\n            'damage_reduction_moderate': 0.35,  # Remove highest-risk buildings\n            'damage_reduction_extreme': 0.40,   # More effective in extreme events\n            'description': 'Buy out highest-risk properties'\n        },\n        'green_infrastructure': {\n            'cost': 12_000_000,\n            'damage_reduction_moderate': 0.30,  # Natural storage\n            'damage_reduction_extreme': 0.35,   # Better performance in extreme events\n            'description': 'Wetland restoration + permeable surfaces'\n        },\n        'combined_approach': {\n            'cost': 18_000_000,\n            'damage_reduction_moderate': 0.50,  # Combines multiple strategies\n            'damage_reduction_extreme': 0.6,   # Better performance in extremes\n            'description': 'Early warning + green infrastructure + targeted buyouts'\n        }\n    }\n    \n    # Calculate metrics for each strategy\n    strategy_analysis = []\n    \n    for name, strategy in strategies.items():\n        # Calculate avoided damages for both scenarios\n        avoided_moderate = baseline_moderate * strategy['damage_reduction_moderate']\n        avoided_extreme = baseline_extreme * strategy['damage_reduction_extreme']\n        \n        # Calculate net benefits\n        net_benefit_moderate = avoided_moderate - strategy['cost']\n        net_benefit_extreme = avoided_extreme - strategy['cost']\n\n        strategy_analysis.append({\n            'strategy': name,\n            'description': strategy['description'],\n            'cost': strategy['cost'],\n            'avoided_damage_moderate': avoided_moderate,\n            'avoided_damage_extreme': avoided_extreme,\n            'net_benefit_moderate': net_benefit_moderate,\n            'net_benefit_extreme': net_benefit_extreme\n        })\n    \n    return pd.DataFrame(strategy_analysis)\n\n# Run the multi-scenario analysis\nstrategy_results = analyze_mitigation_strategies_multi_scenario(buildings)\n\n# Create a nicely formatted table with colors\ndef print_strategy_table(strategy_df):\n    \"\"\"\n    Pretty print the strategy analysis table with colored output.\n    Red for costs, black for benefits.\n    \"\"\"\n    \n    # ANSI color codes\n    RED = '\\033[91m'\n    GREEN = '\\033[92m'\n    BLUE = '\\033[94m'\n    BOLD = '\\033[1m'\n    END = '\\033[0m'\n    \n    print(f\"\\n{BOLD} MITIGATION STRATEGY ANALYSIS - MULTI-SCENARIO{END}\")\n    print(\"=\"*100)\n    \n    # Table header\n    print(f\"{BOLD}{'Strategy':&lt;20} {'Cost':&lt;12} {'Avoided Damage':&lt;25} {'Net Benefit':&lt;25}{END}\")\n    print(f\"{BOLD}{'':20} {'($M)':&lt;12} {'Mod/Ext ($M)':&lt;25} {'Mod/Ext ($M)':&lt;25}{END}\")\n    print(\"-\" * 100)\n    \n    for idx, row in strategy_df.iterrows():\n        strategy_name = row['strategy'].replace('_', ' ').title()\n        \n        # Format costs (red)\n        cost_str = f\"{RED}${row['cost']/1e6:.1f}{END}\"\n        \n        # Format avoided damages (green if positive)\n        avoided_mod = row['avoided_damage_moderate'] / 1e6\n        avoided_ext = row['avoided_damage_extreme'] / 1e6\n        avoided_str = f\"{GREEN}${avoided_mod:.1f}/${avoided_ext:.1f}{END}\"\n        \n        # Format net benefits (green if positive, red if negative)\n        net_mod = row['net_benefit_moderate'] / 1e6\n        net_ext = row['net_benefit_extreme'] / 1e6\n        \n        net_mod_color = GREEN if net_mod &gt; 0 else RED\n        net_ext_color = GREEN if net_ext &gt; 0 else RED\n        net_str = f\"{net_mod_color}${net_mod:.1f}{END}/{net_ext_color}${net_ext:.1f}{END}\"\n        \n        print(f\"{strategy_name:&lt;20} {cost_str:&lt;20} {avoided_str:&lt;35} {net_str:&lt;35}\")\n    \n    print(\"\\n\" + \"=\"*100)\n    print(f\"{BOLD}Legend:{END}\")\n    print(f\"  • {GREEN}Green{END} = Positive value (benefit)\")\n    print(f\"  • {RED}Red{END} = Negative value (cost)\")\n    print(f\"  • Mod/Ext = Moderate/Extreme flood scenarios\")\n\n# Print the formatted table\nprint_strategy_table(strategy_results)\n\n\nBaseline damages:\n  Moderate flood: $3,071,919\n  Extreme flood: $8,794,570\n\n MITIGATION STRATEGY ANALYSIS - MULTI-SCENARIO\n====================================================================================================\nStrategy             Cost         Avoided Damage            Net Benefit              \n                     ($M)         Mod/Ext ($M)              Mod/Ext ($M)             \n----------------------------------------------------------------------------------------------------\nDo Nothing           $0.0        $0.0/$0.0                  $0.0/$0.0        \nEarly Warning        $0.5        $0.5/$1.1                  $-0.0/$0.6       \nLevees               $25.0       $2.2/$4.0                  $-22.8/$-21.0    \nBuyouts              $15.0       $1.1/$3.5                  $-13.9/$-11.5    \nGreen Infrastructure $12.0       $0.9/$3.1                  $-11.1/$-8.9     \nCombined Approach    $18.0       $1.5/$5.3                  $-16.5/$-12.7    \n\n====================================================================================================\nLegend:\n  • Green = Positive value (benefit)\n  • Red = Negative value (cost)\n  • Mod/Ext = Moderate/Extreme flood scenarios\n\n\n\n\nWrapping up the analysis\n\nSave your figures in the figures/ subdirectory (check the resolution)\nFinalize a comprehensive .README\nPush final changes to GitHub (hopefully you made commits throughout)\nConvert your notebook to pdf and submit on Canvas\n\nNote: If you would like to experiment with code, I am happy to review and provide feedback! For example, you might want to create a new notebook where you initialize a different terrain and/or flood model, building representation, etc., You might want to try adding figures to your main notebook, or flood scenarios. If you want practice and have some extra time, go for it!"
  },
  {
    "objectID": "labs/lab06-dps.html",
    "href": "labs/lab06-dps.html",
    "title": "Lab 6: Implementing Direct Policy Search",
    "section": "",
    "text": "Today we’re going to compare open loop intertemporal and closed loop (via direct policy search with a radial basis function) optimization. This lab replicates and adapts various analyses in Quinn, J.D., Reed, P.M., Keller, K. (2017). Direct policy search for robust multi-objective management of deeply uncertain socio-ecological tipping points. Environmental Modelling & Software 92, 125-141.\nToday’s objectives:\n\nLearn about Pareto optimal strategies for the simple lake problem obtained using the open loop and closed loop approaches\n\nEvaluate and explain the solution dynamics from various strategies from both approaches\nCalculate performance metrics for different strategies\nDemonstrate your conceptual understanding of dynamic planning and how it relates to robustness\nDescribe how you could incorporate dynamic planning into your decision analysis"
  },
  {
    "objectID": "labs/lab06-dps.html#overview",
    "href": "labs/lab06-dps.html#overview",
    "title": "Lab 6: Implementing Direct Policy Search",
    "section": "",
    "text": "Today we’re going to compare open loop intertemporal and closed loop (via direct policy search with a radial basis function) optimization. This lab replicates and adapts various analyses in Quinn, J.D., Reed, P.M., Keller, K. (2017). Direct policy search for robust multi-objective management of deeply uncertain socio-ecological tipping points. Environmental Modelling & Software 92, 125-141.\nToday’s objectives:\n\nLearn about Pareto optimal strategies for the simple lake problem obtained using the open loop and closed loop approaches\n\nEvaluate and explain the solution dynamics from various strategies from both approaches\nCalculate performance metrics for different strategies\nDemonstrate your conceptual understanding of dynamic planning and how it relates to robustness\nDescribe how you could incorporate dynamic planning into your decision analysis"
  },
  {
    "objectID": "labs/lab06-dps.html#lab-workflow",
    "href": "labs/lab06-dps.html#lab-workflow",
    "title": "Lab 6: Implementing Direct Policy Search",
    "section": "Lab Workflow",
    "text": "Lab Workflow\n\nRepository Setup\nYou can call the repository dps_lab/.\nIf you need a reminder on how to set up your project directory and create a GitHub repository, check the workflow and directory structure from lab 2.\nYou can directly export these lab instructions as a .ipynb file (look at the top right of the page). I put this file file directly under my dps_lab/ dps directory. If you organize your directory different, please update relative filepaths in the code below accordingly.\n\n\nEnvironment Setup\nEnvironment wise, you can use the one we set up for the previous lab.\nBe sure to include instructions in your README.md about how others can prepare the computational environment to run your notebook.\n\n\nOur Analysis\nMake sure your environment is set up and active to run the code cells below.\n\nRevised lake problem overview\nQuinn et al., (2017) share the general concept of our problem statement in Lab 4 but introduce a different representation of system dynamics, new planning objectives, and new policy search approaches.\nDue to ongoing economic activity, a town emits phosphorous into a shallow lake (with a concentration of \\(a_t\\)), which also receives non-point source runoff (concentration \\(y_t\\)) from the surrounding area. The concentration of the lake at time \\(t+1\\) is given by \\[X_{t+1} = X_t + a_t + y_t + \\frac{X_t^q}{1+X_t^q} - bX_t,\\]\nwhere:\n\n\n\nParameter\nValue\n\n\n\n\n\\(a_t\\)\npoint-source phosphorous concentration from the town\n\n\n\\(y_t\\)\nnon-point-source phosphorous concentration\n\n\n\\(q\\)\nrate at which phosphorous is recycled from sediment\n\n\n\\(b\\)\nrate at which phosphorous leaves the lake\n\n\n\nand \\(X_0 = 0\\), \\(y_t \\sim LogNormal(\\log(0.03), 0.25)\\), \\(q=2.5\\), and \\(b=0.4\\).\nThe goal of the optimization is to meet several objectices:\n\n\n\n\n\n\n\n\nObjective\nDescription\nPreference\n\n\n\n\nEconomic Benefits\nDiscounted economic benefits, assumed proportional to discounted P emissions\nMax\n\n\nLake P Concentration\nMeasure of the lake quality, where lower P concentrations correspond to clearer lakes\nMin\n\n\nPolicy Inertia\nMeasure of the stability of the control policy, where stable policies are favored\nMax\n\n\nReliability\nPercentage of simulations that the lake P concentration is below the critical P threshold\nMax\n\n\n\nPlease consult Section 3.1 of the paper to see the mathematical formulation of the objectives.\n\n\nNon-linear dynamics in the shallow lake problem\nLet’s take a closer look at the non-linear dynamics of the shallow lake problem.\nEach time step, the P concentration in the lake changes due to a natural inflow (+), an anthropogenic contribution (+), P recycled from sediments (+), and losses of P through outflow or sediment absorption (-). The critical P threshold exists where natural P recycling exceeds natural P losses as a function of the current P level in the lake. The paper has this helpful figure that illustrates the dynamics and critical P thresholds as a function of changing parameter values that determine natural recycling and loss rates.\n\nAs posed in this and related studies, b & q are deeply uncertain parameters. In a given state of the world, we can calculate critical P by finding the roots (the values of X that make the equation equal to 0) of: \\(\\frac{X_t^q}{1+X_t^q} - bX_t\\).\nThe intertemporal apporach will test out many possible combinations of \\(a_t\\) over 100 years. With enough training, this approach could find the sequences of \\(a_t\\) that are Pareto optimal and avoid the critical P threshold of the considered SOW. However, keep in mind that there are 100 decision variables! With \\(.01 &lt; a_t &lt; .1, \\forall t\\), there are an infeasible number of candidate solutions to test.\nIn contrast, by using direct policy search with radial basis functions, we completely transform the decision space. Following Section 3.2.2, we now represent the decision levers as: \\[\na_{t, i} = \\min(\\max(\\sum_{j=1}^n w_j |\\frac{X_{t,i} - c_j}{r_j}|^3, 0.01),0.1) \\quad \\forall {t, i}\n\\]\nThis is a cubic radial basis function that parameterizes how to map P concentrations to P release decisions. \\(c_j\\), \\(r_j\\) and \\(w_j\\) are the centers, radii, and weights of n cubic radial basis functions. The decision variables are these \\(3*n\\) parameters, rather than the T decision variables in the intertemporal optimization. The authors used 2 radial basis functions in this study, so the DPS and intertemporal approaches consider 6 vs. 100 decision variables, respectively. The authors emphasize – and this is important! – that with the DPS approach, different P release decisions can be made in each of the N simulations (note the i indexing) because the decisions are informed by the lake P concentrations in a time step. We are searching for the values of \\(c_1\\), \\(r_1\\), \\(w_1\\), \\(c_2\\), \\(r_2\\), and \\(w_2\\).\nIn addition to evaluating open loop vs. closed loop strategies for their expected performance on objectives (and how robustly they do well on objectives across SOWs), we are going to pay close attention to how the various strategies act as the P level in the lake gets close to the critical P threshold in both the SOW the policy was trained on and in new SOWs.\n\n\nComparing Intertemporal and DPS\nInstead of running the optimization (which takes a very long time), we are fortunate that we can directly use the reuslts from Quinn et al., (2017) due to their very well-formatted repository. Download the repo, which you can do here, and extract the data from the DataInPaper subdirectory. I created a subdirectory called data which is at the same level in the lab directory as my .ipynb file. I put all the contents of DataInPaper into data.\nTo help build intuition about how the optimization works, we’ll break down the process a bit.\nFirt, let’s orient ourselves to the state of the world the optimization takes place in.\n\nimport math\nimport numpy as np\n\n# Lake model parameters, number of years simulated, and number of samples generated\nq = 2\nb = 0.42\nalpha = 0.4\ndelta = 0.98\nmu = 0.03\nsigma = np.sqrt(10**(-5.0))\nlake0 = 0\nnYears = 100\nnSamples = 100\n\nWe can find the critical P threshold by identifying the value of X at which the natural P recycling and losses are equal to each other: \\(\\frac{X^q}{1+X^q} - bX = 0\\) -&gt; \\(\\frac{X^2}{1+X^2} - .42*X = 0\\). There are several options to calculate this in python. We will use Brent’s method as implemented in scipy.\n\n\nCode\nfrom scipy.optimize import brentq\nimport matplotlib.pyplot as plt\n\nxs = np.arange(0, 2.5, 0.01)\n\ndef recycling(x, q=q):\n    return x**q / (1.0 + x**q)\n\ndef losses(x, b=b):\n    return b * x\n\nR = recycling(xs)\nL = losses(xs)\n\np_crit = brentq(lambda x: x**q / (1 + x**q) - b * x, 0.01, 1.5)\n\nfig, ax = plt.subplots(dpi=300)\nax.plot(xs, R, label=\"Recycling when q={}\".format(q), color=\"C0\")\nax.plot(xs, L, label=\"Losses when b={}\".format(b), color=\"C1\")\nax.axvline(p_crit, color=\"k\", linestyle=\"--\", alpha=0.8)\nax.scatter([p_crit], [recycling(p_crit)], color=\"red\", zorder=5, label=f\"Critical P = {p_crit:.3f}\")\nax.set_xlabel(\"P concentration\", size=12)\nax.set_ylabel(\"P Flux\", size=12)\nax.legend(fontsize='large')\n\nprint(f\"Critical P: {p_crit:.4f}\")\n\n\nCritical P: 0.5445\n\n\n\n\n\n\n\n\n\nPay attention to the gap between losses and recycling in this state of the world to the left of the unstable equilibrium.\nWhen we do intertemporal optimization, we are considering anthropogenic releases between 0.01 and 0.1 at each time step. Crucially, we are considering the full sequence of those releases over 100 time steps. In the MOEA framework, the optimization considers a candidate sequence of 100 releases and tests this against each simulation of natural inflows (because this is stochastic). The optimization looks for the candidate sequences that do “best” on the expected value of the multiple objectives. Then, it continues searching based on the EA design.\nLet’s take a closer look at the intertemporal strategies highlighted in the paper that are best on the reliability and benefits objectives, respecitvely.\n\n\nCode\nimport scipy.stats as ss\n\n# Intertemporal lake problem\n# We set a seed for consistent results with the paper\n# We pass in the 100 actions for the strategy\ndef LakeModel_IT(seed, actions):\n    # Set inflow distribution parameters\n    log_std = np.sqrt(np.log(1+sigma**2/mu**2))\n    log_mu = np.log(mu) - 0.5*(log_std**2)\n    \n    # Initialize arrays to store P level in the lake at each time step\n    lake_state = np.zeros([nYears+1])\n\n    # Randomly generate nSamples of nYears of natural P inflows\n    natFlow = np.zeros([nYears])\n    np.random.seed(seed)\n    natFlow= np.exp(ss.norm.rvs(log_mu, log_std, nYears))\n    \n    # Run lake model simulation\n    lake_state[0] = lake0\n    for i in range(nYears):\n        lake_state[i+1] = lake_state[i]*(1-b) + (lake_state[i]**q)/(1+(lake_state[i]**q)) + actions[i] + natFlow[i]\n\n    # Return natural inflows and actions as well      \n    return (lake_state, natFlow, actions)\n\n# Load the intertemporal results\n\n# The rows are different strategies\n# The first 100 columns are the actions\n# The remaining columns are objective values, calculated\n# as expected values over the simulated natural inflows\n# The last column is the reliability objective\nIT = np.loadtxt('./data/Intertemporal/Intertemporal.resultfile')\n\n# The best reliability strategy minimizes the percentage of\n# simulations within a SOW where we pass critical P\nITmostRel = np.argmin(IT[:,103]) \n# The best benefits strategy maximizes net present value\nITmostBen = np.argmin(IT[:,100])\n\nyears = np.arange(nYears + 1)\n\nmostRel_states_all = []\nmostBen_states_all = []\n\nfig, ax = plt.subplots(figsize=(10, 6),\n                       dpi=300)\n\nfor sim in range(nSamples):\n    IT_states_mostRel = LakeModel_IT(sim, IT[ITmostRel, 0:100])\n    IT_states_mostBen = LakeModel_IT(sim, IT[ITmostBen, 0:100])\n    mostRel_states_all.append(IT_states_mostRel[0])\n    mostBen_states_all.append(IT_states_mostBen[0])\n    \n    # Plot total P time series with low alpha for clarity\n    ax.plot(years, IT_states_mostRel[0], color='tab:green', alpha=0.01)\n    ax.plot(years, IT_states_mostBen[0], color='tab:green', alpha=0.01)\n\n# Plot mean total P for both strategies\nax.plot(years, np.array(mostRel_states_all).mean(axis=0), color='tab:green', lw=2, ls='-')\nax.plot(years, np.array(mostBen_states_all).mean(axis=0), color='tab:green', ls='--', lw=2)\n\n# Plot actions (anthropogenic emissions) for both strategies\nax.plot(years[:-1], IT_states_mostRel[2], color='tab:blue', lw=2, ls='-')\nax.plot(years[:-1], IT_states_mostBen[2], color='tab:blue', lw=2, ls='--')\n\n# Critical phosphorus concentration line\nax.axhline(p_crit, color='red', ls='-.', label='Critical P (threshold)')\n\n# Labels and title\nax.set_xlabel('Year', size=12)\nax.set_ylabel('Phosphorus concentration / Emission flow', size=12)\nax.set_ylim([0, 1])\n\n# Custom legend\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\n\nlegend_elements = [Line2D([0], [0], color='gray', lw=2, ls='-', label='Best Reliability Strategy'),\n                   Line2D([0], [0], color='gray', lw=2, ls='--', label='Best Benefits Strategy'),\n                   Patch(facecolor='tab:blue', edgecolor='tab:blue',\n                         label='Anthropogenic P'),\n                   Patch(facecolor='tab:green', edgecolor='tab:green',\n                         label='Total P'),\n                   Line2D([0], [0], color='red', lw=2, ls='-.', label='Critical P'),]\n\nax.legend(handles=legend_elements,\n          loc='upper left',\n          fontsize='large')\n\n\n\n\n\n\n\n\n\nNote that in order to see the anthropogenic P lines, I clipped the upper y limit, which goes well above 1 for Total P in the best benefits strategy case!\nBoth strategies are assessed over identical natural P inflow realizations. What do you recognize about the different strategy behaviors?\nAs an exercise, add subplots the P Flux from the natural P recycling and losses for each strategy across each realization. What do you notice about how the best benefits strategy manages the non-linear dynamics past the critical P threshold? Check equations 6-9 in the paper, which describe the intertemporal optimizaton, and explain how mathematical formulation choices influence the dynamics of the best benefits strategy. (Hint: consider constraints and the time horizon).\nNow, let’s take a look at the analagous policies the authors found with direct policy search. We’ll look at the state-action mappings from each as well as how that plays out over time.\nTo start, we’ll plot the best reliability and benefits strategies:\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\n\n# Getting the actions from a given DPS parameterization\ndef DPSpolicy(lake_state, vars):\n    # Determine centers, radii and weights of RBFs\n    C = vars[0::3]\n    B = vars[1::3]\n    W = vars[2::3]\n    newW = np.zeros(len(W))\n    \n    # Normalize weights to sum to 1\n    total = sum(W)\n    if total != 0.0:\n        for i in range(len(W)):\n            newW[i] = W[i]/total\n    else:\n        for i in range(len(W)):\n            newW[i] = 1/n\n    \n    # Determine pollution emission decision, Y\n    Y = 0\n    for i in range(len(C)):\n        if B[i] != 0:\n            Y = Y + W[i]*((np.absolute(lake_state-C[i])/B[i])**3)\n            \n    Y = min(0.1,max(Y,0.01))\n    \n    return Y\n\n# Load the intertemporal results\n\n# The rows are different strategies\n# The first 100 columns are the actions\n# The remaining columns are objective values, calculated\n# as expected values over the simulated natural inflows\n# The last column is the reliability objective\nDPS = np.loadtxt('./data/DPS/DPS.resultfile')\n\n# The best reliability strategy minimizes the percentage of\n# simulations within a SOW where we pass critical P\n# There are only 6 decision variables so the \n# reliability objective is in the 10th column (index 9)\nDPSmostRel = np.argmin(DPS[:,9]) \n# The best benefits strategy maximizes net present value\nDPSmostBen = np.argmin(DPS[:,6])\n\n# Get the policy curves from the solutions\nlake_states = np.arange(0, 2.5, 0.01)\n\nrbf_list = []\nfor pol, var in zip(['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)'], [DPSmostRel, DPSmostBen, 26]):\n    rbf = pd.DataFrame()\n    rbf['Lake P'] = pd.Series(lake_states)\n    rbf['Anthropogenic P'] = rbf['Lake P'].apply(lambda x: DPSpolicy(x, DPS[var, 0:6]))\n    rbf['Strategy'] = pol\n    rbf_list.append(rbf)\n\nrbfs = pd.concat(rbf_list, axis=0)\n\nfig, ax = plt.subplots(figsize=(10, 5),\n                       dpi=300)\n\nsns.lineplot(data=rbfs, x='Lake P', y='Anthropogenic P', hue='Strategy', style='Strategy', ax=ax)\nax.set_xlim([0, 1])\n\nax.axvline(p_crit, color='red', ls='-.', lw=3, label='Critical P')\n\nax.legend(fontsize='large', loc='lower right')\nax.tick_params('both', labelsize=12)\nax.set_ylabel('Anthropogenic P', size=14)\nax.set_xlabel('Lake P', size=14)\n\n\nText(0.5, 0, 'Lake P')\n\n\n\n\n\n\n\n\n\nIt turns out that many Pareto optimal strategies achieve the best reliability possible. We will consider an arbitrary strategy with the best reliability possible (blue) and the one used in the paper (green) for teaching purposes.\nLet’s see what Lake P we take actions on based on a single stochastic realization of natural inflows with these various control policies. You can run this code with different seeds to see different results.\n\n\nCode\ndef LakeModel_DPS_adapted(seed, vars):\n    log_std = np.sqrt(np.log(1 + sigma**2 / mu**2))\n    log_mu = np.log(mu) - 0.5*(log_std**2)\n    \n    lake_state = np.zeros([nYears + 1])\n    np.random.seed(seed)\n    natFlow = np.exp(ss.norm.rvs(log_mu, log_std, nYears))\n    \n    recycling = np.zeros(nYears)  # Store recycling term at each step\n    loss = np.zeros(nYears)       # Store loss term (B * X_t) at each step\n    \n    lake_state[0] = lake0\n    Y = np.zeros(nYears)\n    Y[0] = DPSpolicy(lake_state[0], vars)\n    \n    for i in range(nYears):\n        recycling[i] = (lake_state[i]**q) / (1 + (lake_state[i]**q))\n        loss[i] = lake_state[i] * b  # assuming 'b' is the retention/loss factor\n        lake_state[i + 1] = lake_state[i] * (1 - b) + recycling[i] + Y[i] + natFlow[i]\n        if i &lt; nYears - 1:\n            Y[i + 1] = DPSpolicy(lake_state[i + 1], vars)\n    \n    # Return lake_state, natural inflows, emissions, recycling, and loss arrays\n    return lake_state, natFlow, Y, recycling, loss\n\nseed = 100\n\nlake_state_rel, natFlow_rel, Y_rel, recycling_rel, loss_rel = LakeModel_DPS_adapted(seed, DPS[DPSmostRel, 0:6])\nlake_state_ben, natFlow_ben, Y_ben, recycling_ben, loss_ben = LakeModel_DPS_adapted(seed, DPS[DPSmostBen, 0:6])\nlake_state_paper, natFlow_paper, Y_paper, recycling_paper, loss_paper = LakeModel_DPS_adapted(seed, DPS[26, 0:6])\n\npolicies = ['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)']\nnat_flows = [natFlow_rel, natFlow_ben, natFlow_paper]\nrec_flows = [recycling_rel, recycling_ben, recycling_paper]\nloss_flows = [loss_rel, loss_ben, loss_paper]\nemissions = [Y_rel, Y_ben, Y_paper]\nlake_states_list = [lake_state_rel, lake_state_ben, lake_state_paper]\n\n# Create figure with 3 columns and 3 rows: histograms above & below, curves in middle\nheight_ratios = [0.75, 1, 0.75]  # lake P hist, policy curve, natural inflows hist\n\nfig, axes = plt.subplots(3, 3, figsize=(8, 8), sharex='col', sharey='row',\n                         gridspec_kw={'height_ratios': height_ratios, 'hspace': 0.1})\n\npolicies = ['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)']\n\n# Top row: Lake P histograms (less tall)\nfor i, ax in enumerate(axes[0]):\n    sns.histplot(lake_states_list[i], binwidth=.1, color='tab:blue', alpha=0.7, ax=ax)\n    ax.set_title(f\"{policies[i]}\", fontsize=12)\n    ax.set_ylabel('Lake P Histogram')\n    ax.tick_params(axis='x', labelbottom=False)  # hide x labels on top row\n    if i &gt; 0:\n        ax.set_ylabel('')\n\n# Middle row: Control policy curves (taller)\nfor i, ax in enumerate(axes[1]):\n    df = rbf_list[i]\n    sns.lineplot(data=df, x='Lake P', y='Anthropogenic P', color='tab:green', ax=ax)\n    ax.axvline(p_crit, color='red', linestyle='--', lw=2, label='Critical P')\n    ax.set_ylabel('Anthropogenic P')\n    ax.tick_params(axis='x', labelbottom=False)\n    ax.legend(fontsize=9, loc='lower right')\n    if i &gt; 0:\n        ax.set_ylabel('')\n\n# Bottom row: Natural inflows histograms (less tall)\nfor i, ax in enumerate(axes[2]):\n    sns.histplot(nat_flows[i] + rec_flows[i] + loss_flows[i], binwidth=.1, color='tab:orange', alpha=0.7, ax=ax)\n    ax.set_xlabel('P Concentration')\n    ax.set_ylabel('Non Anthropogenic P Histogram')\n    if i &gt; 0:\n        ax.set_ylabel('')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\nI intentionally chose a seed that leads to very high concentrations for the most benefits strategy. Check out a few other seeds. What seeds did you choose and do they produce the same patterns for each strategy?\nHere’s an example with a different seed:\n\n\nCode\nseed = 42\n\nlake_state_rel, natFlow_rel, Y_rel, recycling_rel, loss_rel = LakeModel_DPS_adapted(seed, DPS[DPSmostRel, 0:6])\nlake_state_ben, natFlow_ben, Y_ben, recycling_ben, loss_ben = LakeModel_DPS_adapted(seed, DPS[DPSmostBen, 0:6])\nlake_state_paper, natFlow_paper, Y_paper, recycling_paper, loss_paper = LakeModel_DPS_adapted(seed, DPS[26, 0:6])\n\npolicies = ['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)']\nnat_flows = [natFlow_rel, natFlow_ben, natFlow_paper]\nrec_flows = [recycling_rel, recycling_ben, recycling_paper]\nloss_flows = [loss_rel, loss_ben, loss_paper]\nemissions = [Y_rel, Y_ben, Y_paper]\nlake_states_list = [lake_state_rel, lake_state_ben, lake_state_paper]\n\n# Create figure with 3 columns and 3 rows: histograms above & below, curves in middle\nheight_ratios = [0.75, 1, 0.75]  # lake P hist, policy curve, natural inflows hist\n\nfig, axes = plt.subplots(3, 3, figsize=(8, 8), sharex='col', sharey='row',\n                         gridspec_kw={'height_ratios': height_ratios, 'hspace': 0.1})\n\npolicies = ['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)']\n\n# Top row: Lake P histograms (less tall)\nfor i, ax in enumerate(axes[0]):\n    sns.histplot(lake_states_list[i], binwidth=.1, color='tab:blue', alpha=0.7, ax=ax)\n    ax.set_title(f\"{policies[i]}\", fontsize=12)\n    ax.set_ylabel('Lake P Histogram')\n    ax.tick_params(axis='x', labelbottom=False)  # hide x labels on top row\n    if i &gt; 0:\n        ax.set_ylabel('')\n\n# Middle row: Control policy curves (taller)\nfor i, ax in enumerate(axes[1]):\n    df = rbf_list[i]\n    sns.lineplot(data=df, x='Lake P', y='Anthropogenic P', color='tab:green', ax=ax)\n    ax.axvline(p_crit, color='red', linestyle='--', lw=2, label='Critical P')\n    ax.set_ylabel('Anthropogenic P')\n    ax.tick_params(axis='x', labelbottom=False)\n    ax.legend(fontsize=9, loc='lower right')\n    if i &gt; 0:\n        ax.set_ylabel('')\n\n# Bottom row: Natural inflows histograms (less tall)\nfor i, ax in enumerate(axes[2]):\n    sns.histplot(nat_flows[i] + rec_flows[i] + loss_flows[i], binwidth=.1, color='tab:orange', alpha=0.7, ax=ax)\n    ax.set_xlabel('P Concentration')\n    ax.set_ylabel('Non Anthropogenic P Histogram')\n    if i &gt; 0:\n        ax.set_ylabel('')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\nWhat do you see about the distribution of all observed P concentrations across the realizations you evaluated of both naturally (inflows, recycling, loss summed on bottom row) and including the anthropogenic P concentration (top row)?\nNow, let’s look across many stochastic realizations.\n\n\nCode\nseed = 100\n\nlake_state_rel, natFlow_rel, Y_rel, recycling_rel, loss_rel = LakeModel_DPS_adapted(seed, DPS[DPSmostRel, 0:6])\nlake_state_ben, natFlow_ben, Y_ben, recycling_ben, loss_ben = LakeModel_DPS_adapted(seed, DPS[DPSmostBen, 0:6])\nlake_state_paper, natFlow_paper, Y_paper, recycling_paper, loss_paper = LakeModel_DPS_adapted(seed, DPS[26, 0:6])\n\npolicies = ['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)']\nnat_flows = [natFlow_rel, natFlow_ben, natFlow_paper]\nrec_flows = [recycling_rel, recycling_ben, recycling_paper]\nloss_flows = [loss_rel, loss_ben, loss_paper]\nemissions = [Y_rel, Y_ben, Y_paper]\nlake_states_list = [lake_state_rel, lake_state_ben, lake_state_paper]\n\n# Create figure with 3 columns and 3 rows: histograms above & below, curves in middle\nheight_ratios = [0.75, 1, 0.75]  # lake P hist, policy curve, natural inflows hist\n\nfig, axes = plt.subplots(3, 3, figsize=(8, 8), sharex='col', sharey='row',\n                         gridspec_kw={'height_ratios': height_ratios, 'hspace': 0.1})\n\npolicies = ['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)']\n\n# Top row: Lake P histograms (over all runs) - log scale y-axis\nfor i, ax in enumerate(axes[0]):\n    sns.histplot(lake_states_list[i], binwidth=.1, color='tab:blue', alpha=0.7, ax=ax)\n    ax.set_yscale('log') \n    ax.set_title(f\"{policies[i]}\", fontsize=12)\n    ax.set_ylabel('Lake P Histogram')\n    ax.set_xlim([0, 1])\n    ax.tick_params(axis='x', labelbottom=False)\n    if i &gt; 0:\n        ax.set_ylabel('')\n\n# Middle row: Control policy curves (taller)\nfor i, ax in enumerate(axes[1]):\n    df = rbf_list[i]\n    sns.lineplot(data=df, x='Lake P', y='Anthropogenic P', color='tab:green', ax=ax)\n    ax.axvline(p_crit, color='red', linestyle='--', lw=2, label='Critical P')\n    ax.set_ylabel('Anthropogenic P')\n    ax.tick_params(axis='x', labelbottom=False)\n    ax.legend(fontsize=9, loc='lower right')\n    if i &gt; 0:\n        ax.set_ylabel('')\n\n# Bottom row: Natural inflows histograms (less tall)\nfor i, ax in enumerate(axes[2]):\n    combined = nat_flows[i] + rec_flows[i] + loss_flows[i]\n    sns.histplot(combined, binwidth=.1, color='tab:orange', alpha=0.7, ax=ax)\n    ax.set_yscale('log')\n    ax.set_xlabel('P Concentration')\n    ax.set_ylabel('Non Anthropogenic P Histogram')\n    ax.set_xlim([0, 1])\n    if i &gt; 0:\n        ax.set_ylabel('')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\nWhat do you see about the distribution of all observed P concentrations across all the realizations of both naturally (inflows, recycling, loss summed on bottom row) and including the anthropogenic P concentration (top row)?\nLet’s take a look at these P concentrations across all of these states. Each realization can have a different anthropogenic P sequence, so we’re not going to plot exactly the same way we did for the intertemporal.\n\n\nCode\npolicy_names = ['Most Reliable', 'Most Benefits', 'Most Reliable (From Paper)']\npolicy_indices = [DPSmostRel, DPSmostBen, 26]\n\n# Preallocate storage for states for all simulations and policies\nnSamples = 100  # your number of Monte Carlo simulations\nnYears = 100    # length of simulation\nyears = np.arange(nYears + 1)\n\ntotalP_all = {name: [] for name in policy_names}\nanthP_all = {name: [] for name in policy_names}\nnatP_all = {name: [] for name in policy_names}\nrecP_all = {name: [] for name in policy_names}\nlossP_all = {name: [] for name in policy_names}\n\n# Run all simulations for each policy\nfor sim in range(nSamples):\n    for name, idx in zip(policy_names, policy_indices):\n        lake_state, natFlow, actions, recs, losses = LakeModel_DPS_adapted(sim + 1000, DPS[idx, 0:6])\n        # collect total phosphorus time series (index 0 of returned tuple)\n        totalP_all[name].append(lake_state)\n        # collect anthro P time series\n        anthP_all[name].append(actions)\n        # collect the rest\n        natP_all[name].append(natFlow)\n        recP_all[name].append(recs)\n        lossP_all[name].append(losses)\n\n# Plotting setup\nfig, ax = plt.subplots(figsize=(12, 7), dpi=300)\n\ncolors = {'Most Reliable': 'tab:green',\n          'Most Benefits': 'tab:orange',\n          'Most Reliable (From Paper)': 'tab:purple'}\n\nlinestyles = {'Most Reliable': '-',\n              'Most Benefits': '--',\n              'Most Reliable (From Paper)': '-.'}\n\n# Plot individual simulation total P flows with very low alpha\nfor name in policy_names:\n    for run in totalP_all[name]:\n        ax.plot(years, run, color=colors[name], alpha=0.05)\n\n# Plot mean total P time series for each policy\nfor name in policy_names:\n    mean_totalP = np.mean(totalP_all[name], axis=0)\n    ax.plot(years, mean_totalP, color=colors[name], lw=2, linestyle=linestyles[name], label=f'{name} Mean Total P')\n\n# Plot critical phosphorus concentration line\nax.axhline(p_crit, color='red', linestyle='-.', lw=2, label='Critical P (threshold)')\n\n# Labels and limits\nax.set_xlabel('Year', fontsize=12)\nax.set_ylabel('Phosphorus concentration / Emission flow', fontsize=12)\nax.set_ylim([0, 1])\n\n# Construct a clear custom legend\nlegend_elements = [\n    Line2D([0], [0], color=colors['Most Reliable'], lw=2, ls=linestyles['Most Reliable'], label='Most Reliable Mean Total P'),\n    Line2D([0], [0], color=colors['Most Benefits'], lw=2, ls=linestyles['Most Benefits'], label='Most Benefits Mean Total P'),\n    Line2D([0], [0], color=colors['Most Reliable (From Paper)'], lw=2, ls=linestyles['Most Reliable (From Paper)'], label='Most Reliable (From Paper) Mean Total P'),\n    Line2D([0], [0], color='red', lw=2, ls='-.', label='Critical P (threshold)')\n]\nax.legend(handles=legend_elements, loc='upper right', fontsize='large')\n\nplt.title('Lake Phosphorus Concentrations and Emissions Across DPS Policies', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\n\nHow would you characterize the behavior of the different strategies? Is the mean P a good way to characterize the strategies’ dynamics?\nLet’s take a closer look at those realizations where the max benefit policy leads to eutrophication.\n\n\nCode\nfrom matplotlib.gridspec import GridSpec\n\ntemp = pd.DataFrame(totalP_all['Most Benefits'])\n# Runs that exceed critical P\neutrophic_runs_idx = temp[(temp &gt; p_crit).sum(axis=1) &gt; 0].index\nnon_eutrophic_runs_idx = temp[(temp &gt; p_crit).sum(axis=1) == 0].index\n\n# Extract the eutrophic realizations data\ntotalP_eutrophic = [totalP_all['Most Benefits'][i] for i in eutrophic_runs_idx]\nanthP_eutrophic = [anthP_all['Most Benefits'][i] for i in eutrophic_runs_idx]\nnatFlow_eutrophic = [natP_all['Most Benefits'][i] for i in eutrophic_runs_idx]\nrecycling_eutrophic = [recP_all['Most Benefits'][i] for i in eutrophic_runs_idx]\nloss_eutrophic = [lossP_all['Most Benefits'][i] for i in eutrophic_runs_idx]\n\n# And non eutrophic\ntotalP_non_eutrophic = [totalP_all['Most Benefits'][i] for i in non_eutrophic_runs_idx]\nanthP_non_eutrophic = [anthP_all['Most Benefits'][i] for i in non_eutrophic_runs_idx]\nnatFlow_non_eutrophic = [natP_all['Most Benefits'][i] for i in non_eutrophic_runs_idx]\nrecycling_non_eutrophic = [recP_all['Most Benefits'][i] for i in non_eutrophic_runs_idx]\nloss_non_eutrophic = [lossP_all['Most Benefits'][i] for i in non_eutrophic_runs_idx]\n\nyears = np.arange(len(totalP_eutrophic[0]))\n\n# Net recycling/loss:\nnet_recycling_eutrophic = [r - l for r, l in zip(recycling_eutrophic, loss_eutrophic)]\n\nnet_recycling_non_eutrophic = [r - l for r, l in zip(recycling_non_eutrophic, loss_non_eutrophic)]\n\n# Combine and create figure\nfig = plt.figure(figsize=(10, 8), dpi=300)\ngs = GridSpec(nrows=2, ncols=2, figure=fig, width_ratios=[1, 1], wspace=0.1, hspace=0.25, \n              height_ratios=[.3, 1])\n\n# --- Top panel: RBF control policies ---\nax0 = fig.add_subplot(gs[0, :])\nsns.lineplot(data=rbfs[rbfs['Strategy'] == 'Most Benefits'], x='Lake P', y='Anthropogenic P', hue='Strategy', style='Strategy', ax=ax0)\nax0.axvline(p_crit, color='red', linestyle='-.', lw=3, label='Critical P')\nax0.set_title('RBF Control Policy - Most Benefits', size=14)\nax0.set_ylabel('Anthropogenic P')\nax0.set_xlabel('')\nax0.set_xlim([0, .6])\nax0.legend(fontsize='medium')\n\n# --- Bottom panel, Left: Dynamics of eutrophic runs ---\nax10 = fig.add_subplot(gs[1, 0])\nfor i in range(len(eutrophic_runs_idx)):\n    # Plot each realization with low alpha for clarity:\n    ax10.plot(years, totalP_eutrophic[i], color='tab:orange', alpha=0.1)\n    ax10.plot(years[:-1], anthP_eutrophic[i], color='tab:blue', alpha=0.1)\n    ax10.plot(years[:-1], natFlow_eutrophic[i], color='tab:green', alpha=0.1)\n    ax10.plot(years[:-1], net_recycling_eutrophic[i], color='tab:red', alpha=0.1)\n\n# Bottom panel, right: dynamics of non eutrophic runs\nax11 = fig.add_subplot(gs[1, 1])\nfor i in range(len(eutrophic_runs_idx)):\n    # Plot each realization with low alpha for clarity:\n    ax11.plot(years, totalP_non_eutrophic[i], color='tab:orange', alpha=0.1)\n    ax11.plot(years[:-1], anthP_non_eutrophic[i], color='tab:blue',  alpha=0.1)\n    ax11.plot(years[:-1], natFlow_non_eutrophic[i], color='tab:green', alpha=0.1)\n    ax11.plot(years[:-1], net_recycling_non_eutrophic[i], color='tab:red', alpha=0.1)\n\nax10.axhline(p_crit, color='black', linestyle='--', label='Critical P')\nax10.set_xlabel('Year')\nax10.set_ylabel('P concentration / flux')\nax10.set_title('Eutrophic Realizations')\nax10.set_xlim(0, 20)\nax10.set_ylim([-.1, .6])\n\nax11.plot(years, np.mean(totalP_non_eutrophic, axis=0), color='tab:orange', lw=2, label='Mean Total P')\nax11.plot(years[:-1], np.mean(anthP_non_eutrophic, axis=0), color='tab:blue', lw=2, label='Mean Anthropogenic P')\nax11.plot(years[:-1], np.mean(natFlow_non_eutrophic, axis=0), color='tab:green', lw=2, label='Mean Natural Inflow')\nax11.plot(years[:-1], np.mean(net_recycling_non_eutrophic, axis=0), color='tab:red', lw=2, label='Mean Net Recycling/Loss')\n\nax11.axhline(p_crit, color='black', linestyle='--', label='Critical P')\nax11.set_xlabel('Year')\nax11.set_ylabel('')\nax11.set_title('Non-Eutrophic Realizations')\nax11.legend(fontsize='medium')\nax11.set_xlim(0, 20)\nax11.set_ylim([-.1, .6])\n\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/d2/g0h08s551zb2hz_ws2g4ggh400hbd0/T/ipykernel_30027/1823386331.py:82: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\nCan you see what the difference is between the eutrophic and non-eutrophic realizations? Recall that these are using the exact same control policies. Let’s try zooming in even more on the early years of the problem and focus on the total P trajectories, along with our RBF and where it tells us to emit less. Recall that in our optimization formulation, we set .01 as our minimum allowable emission.\n\n\nCode\n# Combine and create figure\nfig = plt.figure(figsize=(10, 8), dpi=300)\ngs = GridSpec(nrows=2, ncols=2, figure=fig, width_ratios=[1, 1], wspace=0.1, hspace=0.25, \n              height_ratios=[.3, 1])\n\n# --- Top panel: RBF control policies ---\nax0 = fig.add_subplot(gs[0, :])\nsns.lineplot(data=rbfs[rbfs['Strategy'] == 'Most Benefits'], x='Lake P', y='Anthropogenic P', hue='Strategy', style='Strategy', ax=ax0)\nax0.set_title('RBF Control Policy - Most Benefits', size=14)\nax0.set_ylabel('Anthropogenic P')\nax0.set_xlabel('')\nax0.set_xlim([.20, .32])\nax0.legend(fontsize='medium')\n\n# --- Bottom panel, Left: Dynamics of eutrophic runs ---\nax10 = fig.add_subplot(gs[1, 0])\nfor i in range(len(eutrophic_runs_idx)):\n    # Plot each realization with low alpha for clarity:\n    ax10.plot(years, totalP_eutrophic[i], color='tab:orange', alpha=0.1)\n    ax10.plot(years[:-1], anthP_eutrophic[i], color='tab:blue', alpha=0.1)\n    ax10.plot(years[:-1], natFlow_eutrophic[i], color='tab:green', alpha=0.1)\n    ax10.plot(years[:-1], net_recycling_eutrophic[i], color='tab:red', alpha=0.1)\n\n# Bottom panel, right: dynamics of non eutrophic runs\nax11 = fig.add_subplot(gs[1, 1])\nfor i in range(len(eutrophic_runs_idx)):\n    # Plot each realization with low alpha for clarity:\n    ax11.plot(years, totalP_non_eutrophic[i], color='tab:orange', alpha=0.1)\n    ax11.plot(years[:-1], anthP_non_eutrophic[i], color='tab:blue',  alpha=0.1)\n    ax11.plot(years[:-1], natFlow_non_eutrophic[i], color='tab:green', alpha=0.1)\n    ax11.plot(years[:-1], net_recycling_non_eutrophic[i], color='tab:red', alpha=0.1)\n\nax10.axhline(p_crit, color='black', linestyle='--', label='Critical P')\nax10.set_xlabel('Year')\nax10.set_ylabel('P concentration')\nax10.set_title('Eutrophic Realizations')\nax10.set_xlim(0, 5)\nax10.set_ylim([.20, .32])\n\nax11.plot(years, np.mean(totalP_non_eutrophic, axis=0), color='tab:orange', lw=2, label='Mean Total P')\n\nax11.set_xlabel('Year')\nax11.set_ylabel('')\nax11.set_title('Non-Eutrophic Realizations')\nax11.legend(fontsize='medium')\nax11.set_xlim(0, 5)\nax11.set_ylim([.20, .32])\nax11.tick_params(left=False, labelleft=False)\n\nax11.fill_between([0, 5], .24, .27, color='gray', alpha=.1)\nax10.fill_between([0, 5], .24, .27, color='gray', alpha=.1)\nax0.fill_between([.24, .27], 0, .1, color='gray', alpha=.1)\n\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/d2/g0h08s551zb2hz_ws2g4ggh400hbd0/T/ipykernel_30027/1156415406.py:54: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\nWe can roughly highlight a region of P concentration at year 3 where the eutrophic and non-eutrophic realizations begin to look different from one another. The non-eutrophic realizations do not cross .27 in year 3, but many of the eutrophic realizations do.\nLet’s revisit our P recycling and loss plot and zoom in on the area below the threshold. We’ll add a shaded area to indicate the mean natural P inflow so that we can get a better sense for what P concentrations are dangerous.\n\n\nCode\nfig, ax = plt.subplots(dpi=300)\nax.plot(xs, R, label=\"Recycling when q={}\".format(q), color=\"C0\")\nax.plot(xs, L, label=\"Losses when b={}\".format(b), color=\"C1\")\n\nax.fill_between(xs, R, [r + .03 for r in R], label='Recycling + Mean P inflow (.03)', color='gray', alpha=.25)\nax.fill_between(xs, [r + .03 for r in R], [r + .04 for r in R], label='\"\" + small emission (.01)', color='green', alpha=.25)\n\nax.axvline(p_crit, color=\"k\", linestyle=\"--\", alpha=0.8)\nax.scatter([p_crit], [recycling(p_crit)], color=\"red\", zorder=5, label=f\"Critical P = {p_crit:.3f}\")\nax.set_xlabel(\"P concentration\", size=12)\nax.set_ylabel(\"P Flux\", size=12)\nax.set_xlim([0, .55])\nax.set_ylim([0, .25])\nax.legend(fontsize='large')\n\n\n\n\n\n\n\n\n\nThis plot suggests that even at relatively low P concentrations, the net P loss that occurs below the critical P threshold might be too low to support anthropogenic emissions that help us do well on our economic objective. For example, in between around .18 and .3, we see a very small margin of net loss. In some realizations, we might get unlucky and the variance in the natural P inflow will push us into net P gains at relatively low P concentrations. These P concentrations correspond to high emission actions for our max benefits policy, quickly taking us over the critical P threshold.\nNot that in the previous plot, the eutrophic realizations that were below .27 never lose enough P to avoid crossing the threshold (because we set our min P to .01). The interaction of emitting at least .01, along with the very small net loss at P concentrations at around .27 mean that a year with a large positive natural P (due to the variance) will be on its way to the critical P threshold with no turning back. Instead of a small emission, let’s look at the same plot in terms of our max benefits and tw0 reliability policies.\n\n\nCode\nfig, ax = plt.subplots(nrows=3, dpi=300, figsize=(8, 12), sharex=True)\n\nfor i, s_group in enumerate(rbfs.groupby(['Strategy'])):\n\n    ax[i].plot(xs, R, label=\"Recycling when q={}\".format(q), color=\"C0\")\n    ax[i].plot(xs, L, label=\"Losses when b={}\".format(b), color=\"C1\")\n\n    ax[i].fill_between(xs, R, [r + .03 for r in R], label='Recycling + Mean inflow', color='gray', alpha=.25)\n    ax[i].fill_between(xs, [r + .03 for r in R], s_group[1]['Anthropogenic P'].values + np.array(R), label='\"\" + Anthropogenic', color='green', alpha=.25)\n\n    ax[i].axvline(p_crit, color=\"k\", linestyle=\"--\", alpha=0.8)\n    ax[i].scatter([p_crit], [recycling(p_crit)], color=\"red\", zorder=5, label=f\"Critical P = {p_crit:.3f}\")\n    ax[i].set_ylabel(\"P Flux\", size=12)\n    ax[i].set_xlim([0, .55])\n    ax[i].set_ylim([0, .25])\n    ax[i].tick_params('both', labelsize=12)\n    ax[i].set_title(s_group[0][0] + \" Strategy\", size=12)\n    if i == 0:\n        ax[i].legend(fontsize='large', frameon=False)\n    if i == 2:\n        ax[i].set_xlabel(\"P concentration\", size=12)\n\n\n\n\n\n\n\n\n\nNote the P concentration where the most benefits strategy contributes its lowest emission. Do the non-eutrophic realizations ever cross this point?\n\n\n\nWrapping up lab\nIn today’s lab, we scrutinized different strategies from the open loop and closed loop optimization approaches for how they manage the tipping point dynamics. To wrap up your lab, please include the following analysis and any visualizations that help you summarize your findings:\n\nCalculate performance metrics for the different strategies we reviewed (both the open loop and closed loop) over time across realizations.\nCalculate the robustness of the most benefits and most reliable strategies from the different optimization approaches to different SOWs using the same metric as in the paper. I encourage you to check the repository and to reproduce figure 8. Do it for intertemporal most reliability and most benefits. Then do it for closed loop most reliability (you can use either one we looked at - please be explicit) and most benefits.\nConsidering your analysis from #1, our attention to dynamics in lab, and the results in Figure 11 in the paper, explain why the different optimization approaches (i.e., open vs. closed loop) lead to different robustness results.\nConsidering #3, what would you consider for updating the policy search approach to improve robustness while still performing well on key objectives in the reference scenario?\nDiscuss the role that dynamic planning and closed loop optimization can play in your decision analysis.\n\nPlease submit your lab report to me as a pdf. Please include a link to your GitHub repository."
  },
  {
    "objectID": "labs/lab04-rdm.html",
    "href": "labs/lab04-rdm.html",
    "title": "Lab 4: Potential Pitfalls of Predict-Then-Act",
    "section": "",
    "text": "Today we’re going to learn about the potential pitfalls of predicting then acting for problems characterized by deep uncertainty. This lab adapts the analysis from Managing the risk of uncertain threshold responses: comparison of robust, optimum, and precautionary approaches by Lempert and Collins, 2007.\nToday’s objectives:\n\nCalculate the regret of a strategy in individual states and across states\nCalculate a strategy’s satisficing robustness\nVisualize different robustness for various metrics and strategies in insightful ways\nDemonstrate your conceptual understanding of the two robustness criteria from the study\n(Optional) Apply the workflow to your project data"
  },
  {
    "objectID": "labs/lab04-rdm.html#overview",
    "href": "labs/lab04-rdm.html#overview",
    "title": "Lab 4: Potential Pitfalls of Predict-Then-Act",
    "section": "",
    "text": "Today we’re going to learn about the potential pitfalls of predicting then acting for problems characterized by deep uncertainty. This lab adapts the analysis from Managing the risk of uncertain threshold responses: comparison of robust, optimum, and precautionary approaches by Lempert and Collins, 2007.\nToday’s objectives:\n\nCalculate the regret of a strategy in individual states and across states\nCalculate a strategy’s satisficing robustness\nVisualize different robustness for various metrics and strategies in insightful ways\nDemonstrate your conceptual understanding of the two robustness criteria from the study\n(Optional) Apply the workflow to your project data"
  },
  {
    "objectID": "labs/lab04-rdm.html#lab-workflow",
    "href": "labs/lab04-rdm.html#lab-workflow",
    "title": "Lab 4: Potential Pitfalls of Predict-Then-Act",
    "section": "Lab Workflow",
    "text": "Lab Workflow\n\nRepository Setup\nYou can call the repository rdm_lab/.\nIf you need a reminder on how to set up your project directory and create a GitHub repository, check the workflow and directory structure from lab 2.\nYou can directly export these lab instructions as a .ipynb file (look at the top right of the page). Place this file in the noteboks/ subdirectory.\n\n\nEnvironment Setup\nFor today’s lab, you will need to create an environment that has all the packages from lab 2, plus scipy.\nBe sure to include instructions in your README.md about how others can prepare the computational environment to run your notebook.\n\n\nOur Analysis\nMake sure your environment is set up and active to run the code cells below.\n\nLake Problem Overview\nQuoting from Lempert and Collins (2007): “Imagine a small village perched on the shores of a pristine lake. Many of the town’s citizens want economic development, but development will increase the pollution flowing into the lake. The citizens know that lakes such as theirs can exhibit a threshold behavior where a small pollution increase past some level may cause a clear lake to turn suddenly and sometimes irreversibly cloudy. The citizens seek a development plan that preserves the clarity of their lake, cognizant of their widely divergent values regarding environmental quality and growth, as well their divergent opinions about the level where the pollution threshold for their lake might lie. The town’s citizens also know that once permits are issued, buildings built, and pollution begins to flow it may become difficult to reverse course. Nonetheless, they must decide how much pollution they should allow to best balance their economic and environmental goals.”\n\n\n\nShallow Lake Model diagram from Vivek Srikrishnan’s Fall 2025 Environmental Systems Analysis Slides\n\n\nThe lake naturally loses and gains phosphorus (P). The lake can exist in one of two states. The desirable state, called an oligotrophic state, exists with low nutrient inputs, low to moderate levels of plant production, and relatively clear water. The undesirable state, called an eutrophic state, exists with high nutrient levels, high levels of plant production, low biodiversity, and murky water. The introduction of economic activity to the lake increases the risk of eutrophication.\nLempert and Collins (2007) represent the lake problem with the following equation (with a little clarification from revisiting Peterson et al., (2003)): \\[\nX_{t+1} =\n\\begin{cases}\n    BX_t + b_t + L_t & \\text{if } X_t &lt; X^{\\text{crit}} \\\\\n    BX_t + r + b_t + L_t & \\text{if } X_t \\geq X^{\\text{crit}}\n\\end{cases}\n\\]\nwhere \\(X_{t}\\) is the P concentration at time \\(t\\), \\(B\\) is the proportion of P retained in the lake each year, \\(b_{t} = (\\bar{b}, \\omega)\\) is the natural pollution flow into the lake assumed to follow a normal distribution with mean \\(\\bar{b}\\) and standard deviation \\(\\omega\\), \\(L_{t}\\) is the anthropogenic pollution flow, and \\(r\\) is the amount of P recycled and maintained in the lake. \\(X^{\\text{crit}}\\) is the critical P concentration at which P recycling begins.\nThe two alternating equilibrium values are found by letting \\(X_{t+1} = X{t}\\):\n\\[\n\\begin{array}{l}\n    X_{1*} = \\frac{\\bar{b} + L}{1 - B} & \\text{if } X_{1*} &lt; X^{\\text{crit}} \\\\\n    X_{2*} = \\frac{r + \\bar{b} + L}{1 - B} & \\text{if } X_{2*} \\geq X^{\\text{crit}}\n\\end{array}\n\\]\nIf \\(X_{1*} &lt; X^{\\text{crit}}\\) an oligotrophic equilibrium exists, and if \\(X_{2*} \\geq X^{\\text{crit}}\\) a eutrophic equilibrium also exists. Because the equilibrium value of \\(X\\) depends upon the P loading, changes in P loading can cause equilibrium point to appear or disappear. Figure 2 from Peterson et al., (2003), shown below, illustrates how changes in P loading can irreversibly alter the lake system’s outcomes:\n\n\n\nAn illustration of the P dynamics model. In example (A), low P loading results in a single stable oligotrophic equilibrium. In (B), at a higher P loading the same lake has two stable equilibrium values—one oligotrophic (the lower point) and one eutrophic (the higher point)\n\n\nLempert and Collins (2007) assume that the citizens of the town gain utility by adding pollutants to the lake, but suffer a loss of utility if the lake becomes eutrophic. They represent the utility function as:\n\\[\nU'_{t} =\n\\begin{cases}\n    \\alpha L_{t} - \\phi (L_{t} - L_{t-1}) & \\text{if } X_t &lt; X^{\\text{crit}} \\\\\n    \\alpha L_{t} - \\beta - \\phi (L_{t} - L_{t-1}) & \\text{if } X_t \\geq X^{\\text{crit}}\n\\end{cases}\n\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are constants. \\(\\beta &gt;&gt; \\alpha L_{t}\\), representing that the loss of utility in the eutrophic state (e.g., recreational and aesthetic utility) is large compared to the utility gained from pollution. The citizens also may suffer economic losses, such as costs of retrofitting infrastructure with pollution control equipment or underutilization of capital stock, if they are forced to decrease emissions below their previously allowed level. These costs are small compared to those of the eutrophic state, given by:\n\\[\n\\phi (L_{t} - L_{t-1}) =\n\\begin{cases}\n    0 & \\text{if } L_{t} \\geq L_{t-1} \\\\\n    \\varphi & \\text{if } L_{t} &lt; L_{t-1}\n\\end{cases}\n\\]\nwhere \\(\\varphi\\) is a constant and \\(\\varphi &lt;&lt; \\frac{\\beta}{L_{t-1} - L_{t}}\\).\nThe town’s decision problem is to choose a pollution series, \\(\\mathbf{L_t}\\),\nLempert and Collins propose a policy rule that has three decision variables: the initial level of pollution, \\(L_{0}\\), the max amount of pollution per time period, \\(\\Delta L\\), and a safety margin, \\(S\\). This decision strategy takes the following form:\n\\[\nL_{t+1} =\n\\begin{cases}\n    Max\\{0, Min[L_{t} + \\Delta L, L^{\\text{target}}_{t}]\\} & \\text{for } t \\geq 1 \\\\\n    L_{0} & \\text{for } t = 0\n\\end{cases}\n\\]\nwhere the target emission level at time \\(t\\) is given by:\n\\[\nL^{\\text{target}} =\n\\begin{cases}\n    (1 - B) \\langle X^{\\text{crit}}_{t} \\rangle - \\bar{b} - S\\omega  & \\text{if } X_{t} &lt; X^{\\text{crit}} \\\\\n    \\langle X^{\\text{crit}}_{t} \\rangle - BX_{t} - r - \\bar{b} - S\\omega & \\text{if } X_{t} \\geq X^{\\text{crit}}\n\\end{cases}\n\\]\nand \\(\\langle X^{\\text{crit}}_{t} \\rangle\\) is the town’s estimated critical threshold at the current time period. Please refer to the paper for the formulation.\nUnder an expected utility maximization framework for decision-making, the town aims to maximize the present value of utility: \\(PV(U) = \\sum_{t} \\frac{U_{t}}{(1 + d)^{t}}\\), where \\(d\\) is the citizens’ discount rate. Given the available information to estimate \\(\\langle X^{\\text{crit}}_{t} \\rangle\\), the town can find the optimal values for their decision variables. As we’ll see later in the lab (and as you already know from reading the paper), whether this is the right decision to make depends on how much you believe the information you have to estimate \\(\\langle X^{\\text{crit}}_{t} \\rangle\\).\nThe basecase values of model parameters are as follows:\n\n\n\nParameter\nValue\n\n\n\n\nRetained phosphorous (B)\n.2\n\n\nMean of natural emissions (\\(\\hat{b}\\))\n.1\n\n\nInitial P concentration (\\(X_{0}\\))\n\\(\\hat{b}\\)/(1 - B) = .125\n\n\nStd of natural emissions \\(\\omega\\)\n.04\n\n\nRecycling rate (r)\n.25\n\n\nStd of measured \\(X^{\\text{crit}}\\) (\\(\\gamma\\))\n.05\n\n\nDistance from \\(X^{\\text{crit}}\\) required for learning (\\(\\lambda\\))\n.1\n\n\nNoise exponent (q)\n2\n\n\nInitial estimate of observed \\(X^{\\text{crit}}\\) variance\n.02\n\n\nDiscount rate (d)\n.03\n\n\nUtility from pollution (\\(\\alpha\\))\n1\n\n\nEutrophic cost (\\(\\beta\\))\n10\n\n\nEmissions reduction cost (\\(\\varphi\\))\n1\n\n\n\nNote: While the table in the text has \\(\\varphi\\) as 10, the text clearly states that this emissions reduction cost is much smaller than the eutrophic cost so it is a typo. I’m guessing it’s supposed to be 1 (similarly, the initial estimate of observed \\(X^{\\text{crit}}\\) variance is a typo in the paper’s table).\n\n\n\nDecision-making under uncertainty\nNote that I was not able to reproduce the results from Lempert and Collins completely. There are a few typos (e.g., see their Table on parameter values and the reported initial estiamte of observed \\(X_{\\text{crit}}\\) and the one I was able to update based on other text in the paper). There are a few decisions and implementation choices I can’t back out without more guidance, but I was able to reproduce the most relevant behavior of the inflows and policies. I made a few updates relative to the policy rules as written in the manuscript:\n\nI estimate the new best guess for \\(X_{\\text{crit}}\\) based on the current best guess for variance. The notation as currently written suggests we calculate the new variance before updating our mean estimate.\nI limit updates on standard deviation of \\(X_{\\text{crit}}\\) to double our intial guess. If we don’t do this, there is too much noise in early draws and we can’t identify when our initial guess is lower than \\(X^{\\text{true}}_{\\text{crit}}\\). I think this biases our representation a bit more towards trusting our priors than the Lempert and Collins paper, but I can’t otherwise figure out how to get the same learning behavior as them for Strategy A when \\(X^{\\text{true}}_{\\text{crit}}\\) = .9.\n\nAnyone who wants to review the paper and code and reconcile any major differences with explanations or code updates is more than welcome!\nBecause the code below does reproduce key behavior of the inflows and policies, all of the lessons about robustness are the same and that’s the key point of this lab.\nOk! Moving on…\nAdapting the narrative of Lempert and Collins slightly, suppose the town reviewed scientific evidence of nearby lakes and concludes that \\(X_{\\text{crit}}\\) lies between .3 and .9. The current best evidence suggets \\(X_{\\text{crit}} \\sim N(.8, .138)\\).\nIf the town has 100% confidence in this belief, the optimal strategy (called Strategy A) has: \\(L_{0} = .31\\), \\(\\Delta L = .027\\) \\(S = 2.8\\)\nAssume that the true value of \\(X_{\\text{crit}}\\) is .9. What does the pollution concentration of the lake look like over 100 years? What about if the true value is .3?\n\n\nCode\nimport math\nimport numpy as np\nfrom scipy.stats import truncnorm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ------------------------\n# Parameters (Table I / paper)\n# ------------------------\nB = 0.20           # retained phosphorus\nb_bar = 0.10       # mean natural emissions\nomega = 0.04       # std dev natural emissions\nr = 0.25           # recycling jump when X &gt;= Xcrit\ngamma_obs = 0.05   # baseline observation noise std\nlambda_learn = 0.10\nq_noise = 2.0\ninitial_var = 0.02\nalpha = 1.0\nbeta = 10.0\nphi = 1.0 # table says 10 but the text says &lt;&lt; beta\ndiscount_rate = 0.03\n\n# Strategy A (given)\nstrategy_A = {\"L0\": 0.31, \"dL\": 0.027, \"S\": 2.8}\n\n# Simulation controls\nT = 100                # years\nNsim = 100             # ensemble size for mean / quantiles (set e.g., 100 for demo)\nSEED = 42\nrng_global = np.random.default_rng(SEED)\n\n# Initial estimate used by the town\ninitial_est_mean = 0.787\ninitial_est_var = initial_var\nest_lower = .3\nest_upper = .9\n\n# ------------------------\n# Helper functions implementing paper's learning and policy rules\n# ------------------------\ndef gamma_t_func(Xt, Xcrit_true, gamma=gamma_obs, lam=lambda_learn, q=q_noise):\n    \"\"\"Equation (6): observation noise depends on proximity to threshold.\"\"\"\n    if Xt &gt;= Xcrit_true:\n        return float(gamma)\n    exponent = ((Xcrit_true - Xt) / lam) ** q\n    return min(gamma * math.exp(exponent), .1)         \n\ndef kalman_update(mean_prev, var_prev, Zt, gamma_t):\n    \"\"\"Scalar Kalman-like update (Equation (5))\"\"\"\n    K = var_prev / (var_prev + gamma_t)\n    mean_new = mean_prev + K * (Zt - mean_prev)\n    var_new = (var_prev * gamma_t) / (var_prev + gamma_t)\n    return mean_new, var_new\n\ndef target_emission_level(estimated_Xcrit, Xt, Xcrit_true, S):\n    \"\"\"Equation (8) for L_target\"\"\"\n    if Xt &lt; Xcrit_true:\n        target = (1.0 - B) * estimated_Xcrit - b_bar - S * omega\n    else:\n        target = estimated_Xcrit - B * Xt - r - b_bar - S * omega\n    return max(0.0, target)\n\ndef next_L(Lt, L_arg, dL):\n    \"\"\"Equation (7) update for L\"\"\"\n    return max(0.0, min(Lt + dL, L_arg))\n\ndef simulate_one_run_with_learning(Xcrit_true,\n                                   policy,\n                                   T=100,\n                                   rng=None,\n                                   est_mean_init=initial_est_mean, est_var_init=initial_est_var):\n    \"\"\"\n    Simulate one stochastic run with Kalman learning and the adaptive policy (L0, dL, S).\n    Returns time series dict with keys: X, L, est (estimated Xcrit), U (period utilities)\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    L0 = float(policy[\"L0\"]) \n    dL = float(policy[\"dL\"])\n    S = float(policy[\"S\"])\n    X = np.empty(T)\n    L = np.empty(T)\n    Z = np.empty(T)\n    est = np.empty(T)\n    U = np.empty(T)\n    Xt = b_bar / (1.0 - B)  # initial concentration per paper\n    L_prev = L0\n    L_curr = L0\n    est_mean = est_mean_init\n    est_var = est_var_init\n\n    for t in range(T):\n        # natural inflow\n        bt = rng.normal(b_bar, omega)\n        # update concentration (Eqn 1)\n        if Xt &gt;= Xcrit_true:\n            Xt_next = B * Xt + r + bt + L_curr\n        else:\n            Xt_next = B * Xt + bt + L_curr\n        # observation for learning: Zt ~ N(Xcrit_true, gamma_t)\n        gamma_t = gamma_t_func(Xt, Xcrit_true)\n        Zt = rng.normal(loc=Xcrit_true, scale=gamma_t)\n        # Kalman update\n        est_mean, est_var = kalman_update(est_mean, est_var, Zt, gamma_t)\n        # utility for this period (using current Xt and current emissions L_curr)\n        reduction_cost = phi * max(0.0, L_prev - L_curr)\n        eutrophic_penalty = beta if Xt &gt;= Xcrit_true else 0.0\n        U[t] = alpha * L_curr - reduction_cost - eutrophic_penalty\n        # record\n        X[t] = Xt\n        L[t] = L_curr\n        est[t] = est_mean\n        # compute target and update emissions\n        L_targ = target_emission_level(est_mean, Xt, Xcrit_true, S)\n        L_next = next_L(L_curr, L_targ, dL)\n        L_prev = L_curr\n        L_curr = L_next\n        Xt = Xt_next\n\n    return {\"X\": X, \"L\": L, \"est\": est, \"U\": U}\n\n# ------------------------\n# Run simulation \n# ------------------------\nall_X = np.zeros((Nsim, T))\nall_L = np.zeros((Nsim, T))\nall_est = np.zeros((Nsim, T))\nall_U = np.zeros((Nsim, T))\n\nall_X2 = np.zeros((Nsim, T))\nall_L2 = np.zeros((Nsim, T))\nall_est2 = np.zeros((Nsim, T))\nall_U2 = np.zeros((Nsim, T))\n\n# True Xcrits for this illustration\nxcrit_list = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nn_x = len(xcrit_list)\n\n# pre-allocate arrays: shape (n_x, Nsim, T)\nall_X = np.zeros((n_x, Nsim, T))\nall_L = np.zeros((n_x, Nsim, T))\nall_est = np.zeros((n_x, Nsim, T))\nall_U = np.zeros((n_x, Nsim, T))\n\n# Pre-generate natural inflows bt_samples for each Monte Carlo run (CRN across Xcrit)\nrng_global = np.random.default_rng(SEED)\nbt_samples = rng_global.normal(loc=b_bar, scale=omega, size=(Nsim, T))\n\n# Run ensemble\nfor i in range(Nsim):\n    # create per-run rng for observation draws (do not re-seed inside loop)\n    rng_i = np.random.default_rng(SEED + 1000 + i)\n    for j, xcrit in enumerate(xcrit_list):\n        out = simulate_one_run_with_learning(Xcrit_true=xcrit,\n                                             policy=strategy_A,\n                                             T=T,\n                                             rng=rng_i,\n                                             est_mean_init=initial_est_mean,\n                                             est_var_init=initial_est_var)\n        all_X[j, i, :] = out[\"X\"]\n        all_L[j, i, :] = out[\"L\"]\n        all_est[j, i, :] = out[\"est\"]\n        all_U[j, i, :] = out[\"U\"]\n\n# ------------------------\n# Plot results: mean concentration, emissions, and estimated Xcrit\n# ------------------------\nyears = np.arange(1, T+1)\n\nfig, ax = plt.subplots(2, 1, figsize=(12, 8))\n\n# Top: concentration and estimated threshold\nax[0].plot(years, all_X[-1].mean(axis=0), color=\"C0\", label=\"Mean P concentration (X_t)\")\nax[0].fill_between(years,\n                   np.percentile(all_X[-1], 5, axis=0),\n                   np.percentile(all_X[-1], 95, axis=0),\n                   alpha=.2,\n                   color=\"C0\", label=\"95% bounds across realizations\")\nax[0].plot(years, all_L[-1].mean(axis=0), color=\"C2\", label=\"Mean anthropogenic flow (L_t)\")\nax[0].plot(years, all_est[-1].mean(axis=0), color=\"C1\", linestyle=\"--\", label=\"Mean estimated Xcrit\")\nax[0].axhline(xcrit_list[-1], color=\"k\", linestyle=\":\", label=f\"True Xcrit = {xcrit_list[-1]}\")\nax[0].set_ylabel(\"Phosphorus concentration\")\nax[0].set_title(\"Strategy A (L0=0.31, ΔL=0.027, S=2.8)\")\nax[0].legend()\n\nax[1].plot(years, all_X[0].mean(axis=0), color=\"C0\", label=\"Mean P concentration (X_t)\")\nax[1].fill_between(years,\n                   np.percentile(all_X[0], 5, axis=0),\n                   np.percentile(all_X[0], 95, axis=0),\n                   alpha=.2,\n                   color=\"C0\", label=\"95% bounds across realizations\")\nax[1].plot(years, all_L[0].mean(axis=0), color=\"C2\", label=\"Mean anthropogenic flow (L_t)\")\nax[1].plot(years, all_est[0].mean(axis=0), color=\"C1\", linestyle=\"--\", label=\"Mean estimated Xcrit\")\nax[1].axhline(xcrit_list[0], color=\"k\", linestyle=\":\", label=f\"True Xcrit = {xcrit_list[0]}\")\nax[1].set_ylabel(\"Phosphorus concentration\")\nax[1].set_title(\"Strategy A (L0=0.31, ΔL=0.027, S=2.8)\")\nax[1].legend(loc='lower right')\n\n\n\n\n\n\n\n\n\nWhen the neighboring lakes’ statistics are represenative of the town’s lake (top panel, true Xcrit = 0.9), Strategy A lets emissions ramp up and stabilizes at a moderate level that keeps the mean phosphorus concentration under the town’s safety buffer. When the assessment is badly wrong (bottom panel, true Xcrit = 0.3), the initial policy overcommits — the emissions (and then concentration) rise, the town’s estimate of \\(X_{\\text{crit}}\\) is revised downward as observations arrive, and emissions are sharply cut only after the system has already been driven into the eutrophic equilibrium. This illustrates how a predict‑then‑act rule that trusts a single prior can be fragile under deep uncertainty.\nHow much would the town regret going with Strategy A if \\(X^{\\text{true}}_{\\text{crit}}\\) = .3?\nTo estimate this, we are going to consider the town’s uncertainty about their estimate of the value of \\(X_{\\text{crit}}\\) as well as epistemic uncertainty about whether this is the correct distribution to characterize \\(X_{\\text{crit}}\\).\nRecall from lecture that the regret of action a in state x is:\n\\(R_{a}(x) = Max_{a'}[PV(U_{a'}(x))] - PV(U_{a}(x))\\),\nthe difference between the present value expected utility of the optimal action in a considered state of the world and the considered action’s present value expected utility in that state of the world.\nConsidering the probability distribution \\(p_{i}(x)\\) over possible states of the world, the expected regret of action a on distribution i is:\n\\[\n\\bar{R}_{a, i} = \\int_{x} R_{a}(x)p_{i}(x)dx\n\\]\nI use actions, a, as opposed to strategies in this formulation because it is consistent with the original derivation in Savage (1951). Otherwise, the notation is consistent with the formulation in Lempert and Collins. x refers to the possible values for \\(X_{\\text{crit}}\\) and i refers to a specific prior distribution for \\(X_{\\text{crit}}\\).\nFor different values of \\(X_{\\text{crit}}\\), we can estimate the town’s expected regret for placing all of their trust in the commissioned science assessment. Lempert and Collins provide several other strategies, B-G. For different values of \\(X_{\\text{crit}}\\), we can calculate which of the strategies A-G is optimal and then calculate the expected regret of choosing Strategy A based on the town’s prior distribution for A. Lempert and Collins presumably identify the optimal strategy for each \\(X_{\\text{crit}}\\) and calculate regret based on that, but we are just going to use the strategies provided to simplify things.\nWe’ll start with the example of \\(X_{\\text{crit}}\\) = .3, which corresponds to the bottom panel of the figure we generated above. At \\(X_{\\text{crit}}\\) = .3, Strategy G is optimal out of the strategies under consideration. Lempert and Collins don’t report the decision variables for this strategy, but we can get a very similar strategy to G by looking at Figure 6. It looks like \\(L_{0} = 0.01\\), \\(\\Delta L = 0.01\\) and \\(S = 5.8\\).\nWe’ll build up slowly to expected regret over a prior distribution by first calculating the regret of Strategy A in the scenario where \\(X_{\\text{crit}}\\) = 0.3. We need to calculate:\n\\(R_{A}(0.3) = PV(U_{G}(0.3)) - PV(U_{A}(0.3))\\)\nWe’ll calculate the expected present value of utility of each strategy when \\(X_{\\text{crit}}\\) = .3 and then the difference.\n\n\nCode\n# Strategy G (guessed from Fig. 6)\nstrategy_G = {\"L0\": 0.01, \"dL\": 0.01, \"S\": 5.8}\n\n# Create the utility array for strategy G\nstrat_g_U = np.zeros((n_x, Nsim, T))\n\n# Run ensemble for Strategy G\nfor i in range(Nsim):\n    # create per-run rng for observation draws (do not re-seed inside loop)\n    rng_i = np.random.default_rng(SEED + 1000 + i)\n    for j, xcrit in enumerate(xcrit_list):\n        out = simulate_one_run_with_learning(Xcrit_true=xcrit,\n                                             policy=strategy_G,\n                                             T=T,\n                                             rng=rng_i,\n                                             est_mean_init=initial_est_mean,\n                                             est_var_init=initial_est_var)\n        strat_g_U[j, i, :] = out[\"U\"]\n\n# Compute present value of utility.\n# Discount factors (length T)\nt = np.arange(T)\ndiscount_factors = 1.0 / ((1.0 + discount_rate) ** t)\n\npv_A_samples = np.sum(all_U[0, :, :] * discount_factors, axis=1) \npv_G_samples = np.sum(strat_g_U[0, :, :] * discount_factors, axis=1)\n\n# Calculate expected values and report both and difference\nmean_pv_A = pv_A_samples.mean()\nmean_pv_G = pv_G_samples.mean()\n\ng_best_regret = mean_pv_G - mean_pv_A\n\nprint(\"Strategy A: mean PV = {:.2f}\".format(mean_pv_A))\nprint(\"Strategy G: mean PV = {:.2f}\".format(mean_pv_G))\nprint(\" Regret = {:.2f}\".format(g_best_regret))\n\n\nStrategy A: mean PV = -310.81\nStrategy G: mean PV = -17.38\n Regret = 293.43\n\n\nAnd we can do the same for when \\(X^{\\text{crit}}\\) = .9 but this time we treat Strategy A as the optimal policy.\n\n\nCode\n# Compute present value of utility.\n# Discount factors (length T)\nt = np.arange(T)\ndiscount_factors = 1.0 / ((1.0 + discount_rate) ** t)\n\npv_A_samples = np.sum(all_U[-1, :, :] * discount_factors, axis=1) \npv_G_samples = np.sum(strat_g_U[-1, :, :] * discount_factors, axis=1)\n\n# Calculate expected values and report both and difference\nmean_pv_A = pv_A_samples.mean()\nmean_pv_G = pv_G_samples.mean()\n\na_best_regret = mean_pv_A - mean_pv_G\n\nprint(\"Strategy A: mean PV = {:.2f}\".format(mean_pv_A))\nprint(\"Strategy G: mean PV = {:.2f}\".format(mean_pv_G))\nprint(\" Regret = {:.2f}\".format(a_best_regret))\n\n\nStrategy A: mean PV = 15.24\nStrategy G: mean PV = 7.27\n Regret = 7.98\n\n\n\nCriterion 1: trading-off optimality for less sensitivity to unverifiable assumptions\nHow do we reconcile the differences in present value expected utility and expected regret? Lempert and Collins define a robust strategy as one that will have a smaller value of the weighted average of the best and worst case regrets compared to the strategy that optimizes expected utility, which we can represent as:\n\\[\nV_{s} = z*\\bar{R}_{s, \\text{best}} + (1-z)*\\bar{R}_{s, \\text{worst}}\n\\]\nwhere \\(0 \\leq z \\leq 1\\) and represents our belief in \\(p_{\\text{best}(x)}\\).\nWhen \\(z\\) = 1, there is only one probability distribution under consideration - the town’s best guess. In this case, the equation above preserves the ordering of strategies produced by an expected utility calculation. When \\(z\\) = 0, the criterion is equivalent to avoiding worst-case utility. When \\(z\\) is in between, it is possible to identify the strategy that minimizes expected regret as a function of a decisionmaker’s odds that the initial probability distribution is correct (Fig. 7 in the paper). The odds are \\(\\frac{z}{(1-z)}\\).\nHow confident does the town have to be in the scientific assessment to stick with Strategy A? To do this, we’ll calculate \\(V_{s}\\) for strategies A and G over a wide range of values for \\(z\\). Since we are only considering strategies A and G:\n\\(\\bar{R}_{A, \\text{best}}\\) is 0, \\(\\bar{R}_{A, \\text{worst}}\\) is the regret we calculated when \\(X_{crit}\\) = 0.3, \\(\\bar{R}_{G, \\text{best}}\\) is the regret we calculated when \\(X_{crit}\\) = 0.9, and \\(\\bar{R}_{G, \\text{worst}}\\) is 0.\n\n\nCode\n# Calculated V_s\ndef weighted_regret(z, best_regret, worst_regret):\n    return z*(best_regret) + (1-z)*(worst_regret)\n\n# Odds from 0.01 to 100\n# Construct odds grid (log-spaced) and convert to z = odds/(1+odds)\nodds = np.logspace(-2, 3, 500)\nz_list = odds / (1.0 + odds)\n\n# Calculate V_A and V_G over odds\nV_A = z_list * 0 + (1 - z_list) * g_best_regret\nV_G = z_list * a_best_regret + (1 - z_list) * 0\n\n# Plot odds against weighted regret\nfig, ax = plt.subplots()\nax.plot(odds, V_A, label='V_A (weighted regret for A)', linestyle='-')\nax.plot(odds, V_G, label='V_G (weighted regret for G)', linestyle='--')\n\nax.set_ylabel(\"V_s (weighted expected regret)\", size=14)\n\nax.set_xscale('log')\n# integer ticks to show (only those within the odds range will be used)\ninteger_ticks = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n# Choose x limits: show more of odds &gt; 10 — e.g., start at 0.1 to include small odds too\nax.set_xlim(0.1, 1000)\n# Keep only ticks that fall inside the x limits\nticks_in_range = [t for t in integer_ticks if (t &gt;= ax.get_xlim()[0] and t &lt;= ax.get_xlim()[1])]\nax.set_xticks(ticks_in_range)\n# Label ticks as integers (no decimals)\nax.set_xticklabels([str(int(t)) for t in ticks_in_range])\nax.set_xlabel(\"Odds of Strategy A's Priors\", size=14)\n\nax.set_ylim([0, 50])\n\nax.tick_params(labelsize=12)\n\nax.legend(fontsize='large')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nBased on this figure and fig. 7 in the paper, we can make a few observations about robust decision-making with this criterion. First, because of how poorly strategy A performs under low \\(X^{true}_{crit}\\), the town should be very confident in the scientific assessment to go with strategy A. Second, having more strategy options (fig. 7 in the paper) can help decisionmakers make more robust decisions under a wide range of beliefs.\n\n\nCriterion 2: performing well over a wide range of plausible futures\nWe’ll now shift our attention to a second robustness criterion: satisficing over a wide range of futures. While decisionmakers may want to identify satisficing strategies over multiple objectives, Lempert and Collins use regret for this criterion. This translates to finding a strategy that has low regret for many values of \\(X^{true}_{crit}\\). In the paper, they define arbitrary cutoffs for what constitutes “good enough” regret. In practice, analysts and decisionmakers often iterate on defining a satisficing threshold based on visualizations about strategy performance over a wide range of futures and refined sets of strategies.\nWe’re going to consider both the net present value of expected utility and regret for strategies A and G.\n\n\nCode\n# Helpful for getting the pv for multiple xcrit\ndef compute_pvs_from_U(U_array, discount_rate):\n    U = np.asarray(U_array)\n    t = np.arange(U.shape[2])\n    discount_factors = 1.0 / ((1.0 + discount_rate) ** t)\n    pv = (U * discount_factors).sum(axis=2) \n    return pv\n\npv_A_samples = compute_pvs_from_U(all_U, discount_rate)     \npv_G_samples = compute_pvs_from_U(strat_g_U, discount_rate)  \n\n# Mean present value over each xcrit\n# Same index as all_U and strat_g_U\nmean_pv_A = pv_A_samples.mean(axis=1)  \nmean_pv_G = pv_G_samples.mean(axis=1) \n\n# Calculate regret\n# Two strategies under consideration, so regret is\n# 0 when one of the strategies is better for a xcrit value\npv_best_by_x = np.maximum(mean_pv_A, mean_pv_G) \nregret_A = pv_best_by_x - mean_pv_A\nregret_G = pv_best_by_x - mean_pv_G\n\n# Plot regret over critical x values in one panel\n# Plot present value expected utility in the other\n# x values (e.g., xcrit_list)\nx = np.array(xcrit_list)\nn_x = len(x)\n\n# bar width and positions\nwidth = 0.35\nxpos = np.arange(n_x)\n\nfig, ax = plt.subplots(nrows=2, sharex=True, figsize=(10,6))\n\n# Top: expected PV bars\nax0 = ax[0]\nax0.bar(xpos - width/2, mean_pv_A, width=width, color='C0', label='Strategy A')\nax0.bar(xpos + width/2, mean_pv_G, width=width, color='C1', label='Strategy G')\nax0.set_ylabel('Present value utility', size=14)\nax0.legend()\nax0.grid(axis='y', linestyle=':', alpha=0.6)\n\n# Bottom: regret bars\nax1 = ax[1]\nax1.bar(xpos - width/2, regret_A, width=width, color='C0')\nax1.bar(xpos + width/2, regret_G, width=width, color='C1')\nax1.set_ylabel('Regret', size=14)\nax1.grid(axis='y', linestyle=':', alpha=0.6)\n\n# x ticks and labels\nax1.set_xticks(xpos)\nax1.set_xticklabels([f\"{val:.2f}\" for val in x])\nax1.set_xlabel('Xcrit', size=14)\n\nax[0].set_yscale('symlog')\nax[1].set_yscale('symlog')\n\nax0.tick_params(labelsize=12)\nax1.tick_params(labelsize=12)\n\n\n\n\n\n\n\n\n\nThis kind of visualization helps put into context the relative performance of the strategies for different scenarios of \\(X_{crit}\\). For example, Lempert and Collins suggest that regret &lt;= 12 is acceptable. According to our calculations, unacceptable regret only becomes an issue if the true \\(X_{crit}\\) is less than .6. From a satisficing perspective, if we wanted to ensure acceptable regret, we would prefer Strategy G because it robustly achieves that goal. We could also consider an alternative threshold, such as positive utility. Strategy G would be more robust than strategy A because it has positive utility over more scenarios of \\(X_{crit}\\). Note that the satisficing approach does not account for beliefs about the probability of different values for \\(X_{crit}\\). This may not be an appropriate assumption for all situations, but it can still be helpful to represent robustness in multiple ways.\n\n\n\nWrapping up lab\nThere is no separate lab report this week. The project progress report will ask you to think hard about this lab, but I would prefer you spend your time and effort on your project than this toy example.\nYou are free to experiment with the code from this lab, or adapt any of the code to expand the analysis or test your understanding. I am happy to take a look and provide feedback or talk through challenging concepts."
  },
  {
    "objectID": "labs/lab01-setup.html",
    "href": "labs/lab01-setup.html",
    "title": "Lab 1: Computing Setup",
    "section": "",
    "text": "Today we’re going to install (or double check installations for) the computing tools we will use in this course and practice the lab report submission workflow. If you are an experienced programmer, you are free to follow your own workflow and/or use your own toolkit for implementing labs - but there is a lower probability that I can help you troubleshoot any issues you run into. Today’s lab is not graded.\nToday we will:\n\nInstall VS Code, Mamba, Python, and git.\nSetup our GitHub account and create a repository for today’s lab.\nRun through the setup steps typical of future labs, run some code, export our notebook, and submit a pdf on Canvas."
  },
  {
    "objectID": "labs/lab01-setup.html#overview",
    "href": "labs/lab01-setup.html#overview",
    "title": "Lab 1: Computing Setup",
    "section": "",
    "text": "Today we’re going to install (or double check installations for) the computing tools we will use in this course and practice the lab report submission workflow. If you are an experienced programmer, you are free to follow your own workflow and/or use your own toolkit for implementing labs - but there is a lower probability that I can help you troubleshoot any issues you run into. Today’s lab is not graded.\nToday we will:\n\nInstall VS Code, Mamba, Python, and git.\nSetup our GitHub account and create a repository for today’s lab.\nRun through the setup steps typical of future labs, run some code, export our notebook, and submit a pdf on Canvas."
  },
  {
    "objectID": "labs/lab01-setup.html#lab-workflow",
    "href": "labs/lab01-setup.html#lab-workflow",
    "title": "Lab 1: Computing Setup",
    "section": "Lab Workflow",
    "text": "Lab Workflow\n\nInstallation and Setup\n\nDownload VS Code for your machine if you don’t already have it.\nInstall Mamba for your system if you don’t already have it. We will use Python in individual Mamba environments as opposed to a system-wide distribution.\n\nPlease review key concepts for environment management with Mamba.\n\nInstall the Jupyter & Python VS Code extensions, along with their extension packs.\nDownload git for your machine if you don’t already have it.\nSetup a GitHub account if you don’t already have one. I highly recommend following the instructions for Connecting to GitHub with SSH.\nCreate a new Public repository called ‘ENGS 199.20 Lab 1’. Add a README, .gitignore, and License. Learn about these files if you are unfamiliar with them.\nClone your repository to your local machine. I recommend having a directory for this class and to clone the repository inside that directory.\nDownload this file as a Jupyter notebook (see the upper righthand corner of the page) and add it to your project directory. You can use whatever git workflow you’d like to do version control, but I will provide instructions for using the command line.\n\nRun git remote -v to check that your local repository is tracking the repository on GitHub.\nRun git status from this lab directory’s root to see that the notebook is untracked.\nRun git add . to track the notebook.\nRun git commit -m [message] with your own helpful message.\nUse this simple workflow when you make changes to files. If you don’t want to track a file, you can add the specific file’s location (or use wildcards and file prefixes or suffixes) to avoid tracking.\n\nFor this lab, we are going to create a small environment that allows us to plot using the seaborn package. After following these next instructions, we will be ready to run our notebook.\n\nSometimes the steps above can cause trouble! Please loop me into these struggles so we can troubleshoot together and save you time. It’s also a good opportunity to get to know the very nice and smart folks at IT who are experts with these setup problems!\nCreate a file called env-lab1.yml in your lab1 directory. Add the following contents and save:\nname: lab1\nchannels:\n  - conda-forge\ndependencies:\n  - python\n  - nbconvert\n  - ipykernel\n  - seaborn\n  - jupyter\nRun mamba env create -f env-lab1.yml. Run mamba activate lab1. Set up the kernel for the environment with python -m ipykernel install --user --name lab1 --display-name lab1.\n\n\nRun Code\nBelow, I reproduce a plot from the seaborn Gallery. Make sure the lab1 kernel is set up in the notebook and then run the code block.\n\n\nCode\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)\n\n\n\n\n\n\n\n\n\nAttractive and clear figures are very important in decision analysis. Run the cell below, which makes the figure higher resolution and makes the font more legible. Some changes reflect my personal style preferences.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Set up a figure with higher resolution\nfig, ax = plt.subplots(dpi=300)\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\",\n            y=\"total_bill\",\n            hue=\"smoker\",\n            showfliers=False,\n            palette=[\"m\", \"g\"],\n            showmeans=True,\n            meanprops={'marker': 'D',\n                       'markerfacecolor': 'firebrick',\n                       'markeredgecolor': 'firebrick'},\n            data=tips,\n            ax=ax)\n\n# Rename legend title and increase font size\nsns.move_legend(ax,\n                loc='best',\n                title='Smoker at Table?',\n                alignment='left',\n                fontsize=12,\n                title_fontsize=14)\n\n# Make tick labels more legible\nax.tick_params(labelsize=12)\n\n# Rename axis labels\nax.set_ylabel(\"Total Bill ($)\", size=14)\nax.set_xlabel(\"Day\", size=14)\nplt.show()\n\n\n\n\n\nThe distribution of restaurant bills in dollars reported by a waiter over a period of several months, shown in boxplots. The grouping examines how total bill size is distributed across day of the week (x-axis) and whether the party included a smoker (hue). Red diamonds denote the mean restaurant bill in each group.\n\n\n\n\nFollowing the link in the seaborn example, we can learn more about the tips data here.\nTo complete this lab report, add a code cell where you create a new figure that explores the relationship between variables in this dataset. It does not have to use the variables we used so far, nor do you have to use a boxplot. Make sure your tick and axis labels are at least size 12 and that the figure resolution is at least 300 dots per inch. Make sure your figure includes a caption, which must state the association considered in the exploratory visualization. In the cell after your plot, add a markdown cell where you:\n\nWrite out a caption;\nDescribe the plotted result;\nSuggest further analysis that could help uncover the relationship between these and/or other variables.\n\nSee a text example below for the revised figure from the seaborn example (note the caption underneath the revised figure on the website).\nAnalysis: On average, the waiter’s bills were similar regardless of day of the week or whether a smoker was present in the party. On Saturdays and Sundays, bills tended to be larger and more variable when smokers are in a party.\nFuture work: Lacking information on the relative frequency of bills across days of the week and smokers present in a party, it is difficult to draw strong conclusions about the associations of bill size and the considered variables. To better understand these associations, it would be helpful to show histograms in a second panel under the present plot. In addition, it’s not clear why presence of smokers in a dining party would have any association with the bill size. Other variables in the dataset, such as time of day and size of the party are likely more relevant to consider.\nNote: If you do not have experience with Python, I recommend trying to generate a new plot – the seaborn package does not really require Python knowledge to generate a new plot. However, today’s lab is not graded so you don’t have to generate a new plot, write a caption, etc., I think it’s a good opportunity to practice the lab submission workflow and ask questions now before I start grading labs.\n\n\nSubmit Code and Report\nMake sure your GitHub repository is up to date.\nExport your .ipynb as a .pdf and submit on Canvas. In your submission, share a link to your repository. Please replace “ENGS 199.20” in the top cell with your name and change the filename to include your name.\n\n\n\n\n\n\nPrinting Code to PDF\n\n\n\nBecause we included nbconvert in our computational environment, we should be able to export the .ipynb to .pdf using VS Code and get a nicely formatted file."
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#goals",
    "href": "labs/lab05-tradeoffs.html#goals",
    "title": "Scrutinizing trade-off visualizations",
    "section": "Goals",
    "text": "Goals\n\nScrutinize different trade-off visualization techniques and implementation\nCritique general visualization implementation\nPropose good practices you will follow when making visualizations"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#instructions",
    "href": "labs/lab05-tradeoffs.html#instructions",
    "title": "Scrutinizing trade-off visualizations",
    "section": "Instructions",
    "text": "Instructions\n\nReview each example for 1 minute and rate the figure from 1-5, with 5 being the best\nAfter time is up, write down your rating in the Zoom chat\nThe raters who give the lowest and highest ratings will explain their ratings\nAs a group, discuss what the (potentially) pessimistic/optimistic raters are missing"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs",
    "title": "Scrutinizing trade-off visualizations",
    "section": "2d trade-offs",
    "text": "2d trade-offs\n\n\n\nHegde et al., (2025)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-1",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-1",
    "title": "Scrutinizing trade-off visualizations",
    "section": "3d trade-offs",
    "text": "3d trade-offs\n\nHadka et al. (2015)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-2",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-2",
    "title": "Scrutinizing trade-off visualizations",
    "section": "4d trade-offs",
    "text": "4d trade-offs\n\nHadka et al. (2015)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-with-brushing",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-with-brushing",
    "title": "Scrutinizing trade-off visualizations",
    "section": "6d trade-offs with brushing",
    "text": "6d trade-offs with brushing\n\n\n\nHadjimichael et al. (2020)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#several-2d-trade-offs-via-scatterplots",
    "href": "labs/lab05-tradeoffs.html#several-2d-trade-offs-via-scatterplots",
    "title": "Scrutinizing trade-off visualizations",
    "section": "Several 2d trade-offs via scatterplots",
    "text": "Several 2d trade-offs via scatterplots\n\nDaw et al. (2015)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#several-2d-trade-offs-via-scatterplots-1",
    "href": "labs/lab05-tradeoffs.html#several-2d-trade-offs-via-scatterplots-1",
    "title": "Scrutinizing trade-off visualizations",
    "section": "Several 2d trade-offs via scatterplots",
    "text": "Several 2d trade-offs via scatterplots\n\n\n\nPollack et al., (2025)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#many-2d-trade-offs-via-scatterplots",
    "href": "labs/lab05-tradeoffs.html#many-2d-trade-offs-via-scatterplots",
    "title": "Scrutinizing trade-off visualizations",
    "section": "Many 2d trade-offs via scatterplots",
    "text": "Many 2d trade-offs via scatterplots\n\n\n\nCiullo et al., (2020)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-3d-trade-offs-via-scatterplots",
    "href": "labs/lab05-tradeoffs.html#d-3d-trade-offs-via-scatterplots",
    "title": "Scrutinizing trade-off visualizations",
    "section": "2d & 3d trade-offs via scatterplots",
    "text": "2d & 3d trade-offs via scatterplots\n\n\n\nCiullo et al., (2020)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-with-parallel-axis",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-with-parallel-axis",
    "title": "Scrutinizing trade-off visualizations",
    "section": "4d trade-offs with parallel axis",
    "text": "4d trade-offs with parallel axis\n\n\n\nGold et al. (2023)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-via-parallel-axis",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-via-parallel-axis",
    "title": "Scrutinizing trade-off visualizations",
    "section": "4d trade-offs via parallel axis",
    "text": "4d trade-offs via parallel axis\n\n\n\nHegde et al., (2025)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-via-barplots",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-via-barplots",
    "title": "Scrutinizing trade-off visualizations",
    "section": "2d trade-offs via barplots",
    "text": "2d trade-offs via barplots\n\nGroves et al., (2019)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-via-radial-plot",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-via-radial-plot",
    "title": "Scrutinizing trade-off visualizations",
    "section": "6d trade-offs via radial plot",
    "text": "6d trade-offs via radial plot\n\nPadilha et al., (2024)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#d-trade-offs-via-heatmap",
    "href": "labs/lab05-tradeoffs.html#d-trade-offs-via-heatmap",
    "title": "Scrutinizing trade-off visualizations",
    "section": "7d trade-offs via heatmap",
    "text": "7d trade-offs via heatmap\n\nJafino et al. (2022)"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#what-worked-and-what-didnt",
    "href": "labs/lab05-tradeoffs.html#what-worked-and-what-didnt",
    "title": "Scrutinizing trade-off visualizations",
    "section": "What worked and what didn’t?",
    "text": "What worked and what didn’t?\n\nAre there specific visual languages you prefer?\nDo you think there are objective criteria for saying whether a trade-off plot is effective?\nDo you think there are objective criteria for saying whether a visualization is effective?\nWhat are your takeaways for your decision analysis project?\n\nWhat will be your visualization check list?"
  },
  {
    "objectID": "labs/lab05-tradeoffs.html#next-week",
    "href": "labs/lab05-tradeoffs.html#next-week",
    "title": "Scrutinizing trade-off visualizations",
    "section": "Next week",
    "text": "Next week\n\nMonday: bringing things together with MORDM\nWednesday: case study\nFriday: lab and hw 4 due (Sunday ok)"
  },
  {
    "objectID": "schedule/schedule.html",
    "href": "schedule/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains a schedule of the topics, content, and assignments for the term, organized by module.\n\nSlides : On the schedule, this link contains the presentation for the day’s lecture.\nLab : This page contains a lab report rubric and links to each week’s lab. You can also access each week’s lab through the links in the schedule below. Please check this page to stay on top of the due dates for each lab.\nHW : This page contains the schedule and rubric for the project updates. At the end of each module, you will have one week to submit your responses to guided questions on how you will integrate new concepts into your final project. I highly encourage you to work on these during the module to stay on track with your project. Please check this page to stay on top of the due dates for each progress report. Note that there is no lab report the first week and the first assignment is to define your project’s decision problem.\nReading : On the schedule, this link takes you to a list of required and optional readings for the module. Please read the required reading before class. There will typically be one required reading per week. I suggest you read at least the abstracts of optional readings before class. The optional readings will help you understand lecture and can serve as core references in lab reports and project updates.\n\n\n\n\n\nYou should follow this general process for each module\n\nDo the reading before the corresponding Monday’s Lecture\nComplete the lab report within one week from the assignment date\nSubmit the project progress report one week after the end of a module\n\n\n\n\n\n There are seven modules in this course:\n\n“Overview”: Overview of decision analysis for wicked climate problems\n“Uncertainty”: Dealing with uncertainty in analyzing decisions\n“RDM”: Robust decision-making\n“Trade-offs”: Multiple objectives and navigating trade-offs\n“DMDU”: Decision making under deep uncertainty\n“Gaps”: Gaps between decision analysis and decision support\n“Projects”: Student project presentations\n\nNote that this schedule will be updated as necessary throughout the term.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nSlides\nLab\nHW\nReading\n\n\n\n\n\n\nOverview\n\n\n\n\n\n\n1\n9/15/25\nCourse Overview\n\n\n\n\n\n\n1\n9/16/25\nComputing setup\n\n\n\n\n\n\n1\n9/17/25\nFraming a decision analysis\n\n\n\n\n\n\n1\n9/19/25\nPython and GitHub Bootcamp\n\n\n\n\n\n\n\n\nUncertainty\n\n\n\n\n\n\n2\n9/22/25\nShallow and deep uncertainties\n\n\n\n\n\n\n2\n9/23/25\nPython and GitHub Bootcamp II\n\n\n\n\n\n\n2\n9/24/25\nDeep uncertainty game\n\n\n\n\n\n\n2\n9/26/25\nMSD UC/UQ tutorials\n\n\n\n\n\n\n\n\nRDM\n\n\n\n\n\n\n3\n9/29/25\nRobust decision-making\n\n\n\n\n\n\n3\n10/1/25\nCase study\n\n\n\n\n\n\n3\n10/3/25\nPotential pitfalls of predict-then-act\n\n\n\n\n\n\n\n\nTrade-offs\n\n\n\n\n\n\n4\n10/6/25\nDA in a pluralistic society\n\n\n\n\n\n\n4\n10/8/25\nCase study\n\n\n\n\n\n\n4\n10/10/25\nTechniques for visualizing trade-offs\n\n\n\n\n\n\n\n\nDMDU\n\n\n\n\n\n\n5\n10/13/25\nTaxonomy of DMDU approaches\n\n\n\n\n\n\n5\n10/15/25\nCase study (No Slides)\n\n\n\n\n\n\n5\n10/17/25\nOpen office hours\n\n\n\n\n\n\n6\n10/20/25\nDirect policy search\n\n\n\n\n\n\n6\n10/22/25\nCase study\n\n\n\n\n\n\n6\n10/24/25\nImplementing DPS\n\n\n\n\n\n\n7\n10/27/25\nScenario discovery\n\n\n\n\n\n\n7\n10/29/25\nOptimization tutorial\n\n\n\n\n\n\n7\n10/31/25\nCase Study\n\n\n\n\n\n\n8\n11/3/25\nRevisiting decision analysis framing\n\n\n\n\n\n\n8\n11/5/25\nPractice presentations\n\n\n\n\n\n\n8\n11/7/25\nPractice presentations\n\n\n\n\n\n\n\n\nGaps\n\n\n\n\n\n\n9\n11/10/25\nMore useful decision analysis\n\n\n\n\n\n\n9\n11/12/25\nCase study\n\n\n\n\n\n\n9\n11/14/25\nPresentations\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n10\n11/17/25\nPresentations\n\n\n\n\n\n\n10\n11/18/25\nPresentations",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "This page contains the lab report rubric and a schedule of the lab assignments for the semester.\nNote that for the training, you are not required to do the labs, but it would be great to get your feedback on them. Is the rubric fair? Are the labs relevant and interesting? Are they too much work?",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#general-information",
    "href": "labs.html#general-information",
    "title": "Labs",
    "section": "General Information",
    "text": "General Information\n\nYou can get to the lab assignment through the link on the schedule below (or at the overall term schedule).\nSubmit your lab report by the start of the next lab on Canvas.\nSubmissions must be PDFs. In the first lab, we will go over the workflow for this.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#grading",
    "href": "labs.html#grading",
    "title": "Labs",
    "section": "Grading",
    "text": "Grading\nLabs will be graded on a scale of 0-5:\n\nMissing (0/5): Lab solution is missing or is not responsive to the lab prompt(s);\nNeeds Improvement (1/5): Lab is largely incomplete or is missing key concepts;\nAcceptable (2/5): Lab is mostly complete but may contain major conceptual or implementation errors and writing has substantial room for growth;\nDecent (3/5): Lab is mostly or fully complete without any major errors, but there are some minor errors and the writing/report presentation has room for growth.\nGood (4/5): Lab is fully complete without any major errors, but there are some minor errors or the writing/report presentation has room for growth.\nStrong (5/5): Lab is fully complete without any errors and the writing/report presentation is clear.\n\nNo late submissions will be graded. However, I will provide feedback on all lab reports, even if late. Your lowest lab report grade will be dropped.\n\nHow to get a 5/5 on lab\n\nGet the “right” answer for the “right” reason.\n\nBad work can accidentally lead to poor decisions (and strong analysis may not identify good decisions). It is crucial that you show your work in a traceable and understandable way so that decision-makers understand why your analysis supports certain decisions. I will grade your work for how well it demonstrates why and how you get the right answers. Because most of your code for lab report is provided with lab instructions, this mostly entails accurate references to source material (i.e., readings and code blocks/instructions from lab). However, you will be responsible for processing/presenting/interpreting results generated with lab instructions. Think about the following things when answering lab report questions:\n\nSpecific references to modeling results, figures, references, code blocks, etc., make it easier to demonstrate you got the right answer for the right reason.\nThe cleaner and clearer your code and documentation in your Jupyter notebook, the more likely I can check for correctness.\n\n\nBe as clear as possible.\n\nDecision-makers cannot read your mind. Ambiguity in decision analysis is bad decision support. Submissions that are unclear for any reason, including but not limited to unclear syntax (writing or code), lack of reasoning to support synthesis, or too much detail in writing will not be strong.\n\nThis includes being transparent about values-laden assumptions and limitations, as well as providing careful synthesis that demonstrates understanding and genuine insights.\n\n\nA good figure is worth 1,000 words\n\nVisualizations are the main way to convey results and insights to decision-mamkers. Figures should be correct and highly legible. You must:\n\nChoose the appropriate axes and label them accordingly;\nPlot the correct data series;\nInclude a descriptive legend (if it applies);\nProvide a succint descriptive caption (do not report or interpret results in a caption).\n\n\nTake your time\n\nIt is difficult to meet guidelines 1-3 in one pass. For example, on guideline 2, I tend to revise each draft of scientific manuscripts 3-4 times before sharing with co-authors, a cycle that repeats several times each paper. As another example, on guideline 3, I tend to spend at least one week working on a single figure for scientific manuscripts. This course expects you to spend roughly 3 times the in-person hours outside of course, which equates to ~10 hours. This time corresponds to one required reading, polishing your lab report (1-2 of the in-person hours each week are lab work in a class setting), and cumulatively connecting new concepts to your course project. These activities are highly synergistic, and time you spend on lab reports will pay off big for your project progress reports. Labs are also your main opportunity in the class to demonstrate to an outside audience, through your GitHub page, your decision analysis and science communication skills.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "labs.html#schedule",
    "href": "labs.html#schedule",
    "title": "Labs",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\nLab\nInstructions\nDue Date\n\n\n\n\nLab 1\n\n—\n\n\nLab 2\n\n—\n\n\nLab 3\n\n10/3/2025\n\n\nLab 4\n\n10/10/2025\n\n\nLab 5\n\n—\n\n\nLab 6\n\n11/7/2025",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "updates/overview.html",
    "href": "updates/overview.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Your guiding question for this report is:\n“What decision problem will you focus on for your project?”\nHere are a few examples of decision problem statements:\n\nWhat height, if at all, should a homeowner elevate their home to defend against flooding?\nWhat federal funding rules promote economic efficiency and social equity in disaster-risk reduction outcomes?\n\nIn addition to answering this guiding question, your report should be responsive to the content in lectures 1 and 2. Because of the focus of this class, you must explain why your problem is a wicked climate problem. You should also develop an initial framing for a decision analysis on your problem. The further you get into sketching out the structure and analysis, the better! That said, I don’t expect you to submit a full conceptual model, plan for a ViMM, XLRM, etc., You might benefit from experimenting with these boundary objects, and proposing partial versions, but at this stage it may be better to answer and/or describe your guiding questions for framing (and to ask me questions, such as whether I know of references that can help you learn more about something you think is relevant).\nYour report does not have to be in an essay form. I want you to make cumulative progress on your project throughout the term. So, while you might find it helpful to write in the same form as your final report, you can also complete your report in a structure that works better for you in the planning stages of a project. For example, some students may find it helpful to structure their report in a questions and answers format, or in a detailed outline, or in a hybrid form. While you do not have to write your report as an essay, you should write all ideas as complete sentences (even if you incorporate bullet and sub-bullets into your report structure). My goal is to give you flexibility in planning, but to hold you to a high standard for synthesizing ideas and demonstrating your project’s potential. Figure sketches, and descriptions of the ideas they convey, are your friend (but not expected).\n\n\n\n\n\n\nHints\n\n\n\nYou may find it useful to take a close look at Gregory et al. (2012)\n\n\n\nGregory, R., Failing, L., Harstone, M., Long, G., McDaniels, T., & Ohlson, D. (2012). Structuring environmental management choices. In Structured decision making (pp. 1–20). Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/9781444398557.ch1"
  },
  {
    "objectID": "updates/overview.html#instructions",
    "href": "updates/overview.html#instructions",
    "title": "Project Proposal",
    "section": "",
    "text": "Your guiding question for this report is:\n“What decision problem will you focus on for your project?”\nHere are a few examples of decision problem statements:\n\nWhat height, if at all, should a homeowner elevate their home to defend against flooding?\nWhat federal funding rules promote economic efficiency and social equity in disaster-risk reduction outcomes?\n\nIn addition to answering this guiding question, your report should be responsive to the content in lectures 1 and 2. Because of the focus of this class, you must explain why your problem is a wicked climate problem. You should also develop an initial framing for a decision analysis on your problem. The further you get into sketching out the structure and analysis, the better! That said, I don’t expect you to submit a full conceptual model, plan for a ViMM, XLRM, etc., You might benefit from experimenting with these boundary objects, and proposing partial versions, but at this stage it may be better to answer and/or describe your guiding questions for framing (and to ask me questions, such as whether I know of references that can help you learn more about something you think is relevant).\nYour report does not have to be in an essay form. I want you to make cumulative progress on your project throughout the term. So, while you might find it helpful to write in the same form as your final report, you can also complete your report in a structure that works better for you in the planning stages of a project. For example, some students may find it helpful to structure their report in a questions and answers format, or in a detailed outline, or in a hybrid form. While you do not have to write your report as an essay, you should write all ideas as complete sentences (even if you incorporate bullet and sub-bullets into your report structure). My goal is to give you flexibility in planning, but to hold you to a high standard for synthesizing ideas and demonstrating your project’s potential. Figure sketches, and descriptions of the ideas they convey, are your friend (but not expected).\n\n\n\n\n\n\nHints\n\n\n\nYou may find it useful to take a close look at Gregory et al. (2012)\n\n\n\nGregory, R., Failing, L., Harstone, M., Long, G., McDaniels, T., & Ohlson, D. (2012). Structuring environmental management choices. In Structured decision making (pp. 1–20). Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/9781444398557.ch1"
  },
  {
    "objectID": "updates/overview.html#submitting-the-report",
    "href": "updates/overview.html#submitting-the-report",
    "title": "Project Proposal",
    "section": "Submitting the Report",
    "text": "Submitting the Report\nExport your writeup as a PDF and submit it to me over Slack. You can upload it to our course channel or direct message me, whichever you prefer."
  },
  {
    "objectID": "updates/rdm.html",
    "href": "updates/rdm.html",
    "title": "Robust Decision-Making",
    "section": "",
    "text": "Unlike your previous project updates, this one doesn’t have a single guiding question.\nFor your report this week, challenge yourself to: (i) identify gaps in your understanding about robustness; and (ii) apply your current understanding on robustness to aspects of your decision analysis. Your report does not have to be in a complete narrative form, but it should be logically structured with complete sentences and should be accessible to outside readers to provide feedback. Things to consider in this report include, but are not limited to:\n\nIf you were explaining your research to a non-scientist friend, how would you describe the concepts of robustness, regret, and satisficing? For what types of problems might it be valuable to consider robustness, regret, and/or satisficing?\nHow salient do you find the two robustness criteria we covered in lab from this week? Do you think they add clarity to evaluating the options at hand? Do you think considering robustness would change a decisionmaker’s mind about what option to take?\nWhat limitations do you find with the criteria covered in lab? This can relate to conceptual or mathematical implementation.\n\nWhich kinds of visualizations of robustness metrics (in this paper or beyond) do you find appealing, if any? How come? While some visual languages may be difficult to understand at first, does the effort pay off once you understand the correct interpretation?\nWhat aspects of your decision analysis are deeply uncertain?\nHow do decisionmakers address this deep uncertainty now?\nHow can the concepts of robustness, regret, and satisficing relate to your problem?\nWhich robustness criteria and visualization of metrics (you can think beyond those in the lab, including metrics from this week’s readings) do you think could be salient and helpful for your decision problem?\n\nThis report is open-ended to facilitate your thinking on this crucial concept for your decision analysis. Even if you ultimately decide that the concept of robustness is not appropriate for your decision analysis (good luck convincing Klaus!), you will have to be an expert on this topic in order to prepare and present strong arguments against the increasingly mainstream conceptions of robust decision-making for wicked climate problems. I encourage you to be honest with yourself in this report in documenting your understanding and knowledge gaps, and crosswalking that self-awareness to your decision problem.\n\n\n\n\n\n\nI know that not knowing can feel uncomfortable, and that there is additional pressure in academia about not looking “wrong.” I can give you the most feedback and best help on your project if you work to overcome that discomfort and trust me to work with you on:\n\nunderstanding the important concepts from this module;\nidentifying resources relevant specifically for your project;\nrecommending how to apply concepts in conceptually and technically sound ways to your project; and\nrecognizing areas of general poor understanding or ambiguity and being transparent about defensible but nonverifiable assumptions."
  },
  {
    "objectID": "updates/rdm.html#instructions",
    "href": "updates/rdm.html#instructions",
    "title": "Robust Decision-Making",
    "section": "",
    "text": "Unlike your previous project updates, this one doesn’t have a single guiding question.\nFor your report this week, challenge yourself to: (i) identify gaps in your understanding about robustness; and (ii) apply your current understanding on robustness to aspects of your decision analysis. Your report does not have to be in a complete narrative form, but it should be logically structured with complete sentences and should be accessible to outside readers to provide feedback. Things to consider in this report include, but are not limited to:\n\nIf you were explaining your research to a non-scientist friend, how would you describe the concepts of robustness, regret, and satisficing? For what types of problems might it be valuable to consider robustness, regret, and/or satisficing?\nHow salient do you find the two robustness criteria we covered in lab from this week? Do you think they add clarity to evaluating the options at hand? Do you think considering robustness would change a decisionmaker’s mind about what option to take?\nWhat limitations do you find with the criteria covered in lab? This can relate to conceptual or mathematical implementation.\n\nWhich kinds of visualizations of robustness metrics (in this paper or beyond) do you find appealing, if any? How come? While some visual languages may be difficult to understand at first, does the effort pay off once you understand the correct interpretation?\nWhat aspects of your decision analysis are deeply uncertain?\nHow do decisionmakers address this deep uncertainty now?\nHow can the concepts of robustness, regret, and satisficing relate to your problem?\nWhich robustness criteria and visualization of metrics (you can think beyond those in the lab, including metrics from this week’s readings) do you think could be salient and helpful for your decision problem?\n\nThis report is open-ended to facilitate your thinking on this crucial concept for your decision analysis. Even if you ultimately decide that the concept of robustness is not appropriate for your decision analysis (good luck convincing Klaus!), you will have to be an expert on this topic in order to prepare and present strong arguments against the increasingly mainstream conceptions of robust decision-making for wicked climate problems. I encourage you to be honest with yourself in this report in documenting your understanding and knowledge gaps, and crosswalking that self-awareness to your decision problem.\n\n\n\n\n\n\nI know that not knowing can feel uncomfortable, and that there is additional pressure in academia about not looking “wrong.” I can give you the most feedback and best help on your project if you work to overcome that discomfort and trust me to work with you on:\n\nunderstanding the important concepts from this module;\nidentifying resources relevant specifically for your project;\nrecommending how to apply concepts in conceptually and technically sound ways to your project; and\nrecognizing areas of general poor understanding or ambiguity and being transparent about defensible but nonverifiable assumptions."
  },
  {
    "objectID": "updates/rdm.html#submitting-the-report",
    "href": "updates/rdm.html#submitting-the-report",
    "title": "Robust Decision-Making",
    "section": "Submitting the Report",
    "text": "Submitting the Report\nExport your writeup as a PDF and submit it to me over Slack. You can upload it to our course channel or direct message me, whichever you prefer."
  },
  {
    "objectID": "updates/tradeoffs.html",
    "href": "updates/tradeoffs.html",
    "title": "Navigating Trade-offs",
    "section": "",
    "text": "Your guiding questions for this week are:\n\nWhat objectives are relevant for your decision analysis?\nWhat trade-offs or win-wins are relevant for your decision analysis?\nWhat objectives can you quantify?\nWhat trade-offs and win-wins do you hypothesize?\n\nIn answering these, and related, questions, it is important for you to justify your answers with strong evidence and reasoning. For example, you may invoke policy documents, stakeholder interactions, and other sources that articulate values and goals about a particular problem and management strategies. Our Wednesday case study is an excellent example of connecting broad conceptualizations of types of objectives relevant to environmental problems, and then operationalizing that in a real-world case study based on input from real people about what is relevant. To the extent that you do not feel ready to give confident answers to 1-4, you should explain boundary tools and methods you can employ to answer those questions in the future.\nNote that questions 1 and 2 take a broad view on your decision problem in general, whereas questions 3 and 4 ask you to focus on what is practical. To the extent that there are gaps between what is relevant and what you can measure, you should practice justifying why your decision analysis can still be useful (this will be very important for reviewers of your study, along with demonstrating salience to potential decision-making partners).\nFor guiding question 4, I would like you to submit figure sketches that represent your hypotheses. The most salient hypotheses will respond to compelling research and decision questions. In other words, evaluating 5-dimensional trade-offs and win-wins in a parallel axis plot may (or may not) be most relevant to questions at hand. I expect you to provide strong justification for why the visual language you choose for figures sketches effectively conveys the results of your experiment and answers salient questions."
  },
  {
    "objectID": "updates/tradeoffs.html#instructions",
    "href": "updates/tradeoffs.html#instructions",
    "title": "Navigating Trade-offs",
    "section": "",
    "text": "Your guiding questions for this week are:\n\nWhat objectives are relevant for your decision analysis?\nWhat trade-offs or win-wins are relevant for your decision analysis?\nWhat objectives can you quantify?\nWhat trade-offs and win-wins do you hypothesize?\n\nIn answering these, and related, questions, it is important for you to justify your answers with strong evidence and reasoning. For example, you may invoke policy documents, stakeholder interactions, and other sources that articulate values and goals about a particular problem and management strategies. Our Wednesday case study is an excellent example of connecting broad conceptualizations of types of objectives relevant to environmental problems, and then operationalizing that in a real-world case study based on input from real people about what is relevant. To the extent that you do not feel ready to give confident answers to 1-4, you should explain boundary tools and methods you can employ to answer those questions in the future.\nNote that questions 1 and 2 take a broad view on your decision problem in general, whereas questions 3 and 4 ask you to focus on what is practical. To the extent that there are gaps between what is relevant and what you can measure, you should practice justifying why your decision analysis can still be useful (this will be very important for reviewers of your study, along with demonstrating salience to potential decision-making partners).\nFor guiding question 4, I would like you to submit figure sketches that represent your hypotheses. The most salient hypotheses will respond to compelling research and decision questions. In other words, evaluating 5-dimensional trade-offs and win-wins in a parallel axis plot may (or may not) be most relevant to questions at hand. I expect you to provide strong justification for why the visual language you choose for figures sketches effectively conveys the results of your experiment and answers salient questions."
  },
  {
    "objectID": "updates/tradeoffs.html#submitting-the-report",
    "href": "updates/tradeoffs.html#submitting-the-report",
    "title": "Navigating Trade-offs",
    "section": "Submitting the Report",
    "text": "Submitting the Report\nExport your writeup as a PDF and submit it to me over Slack. You can upload it to our course channel or direct message me, whichever you prefer."
  },
  {
    "objectID": "slides/game01-decisionsfordecade.html",
    "href": "slides/game01-decisionsfordecade.html",
    "title": "Decisions for the Decade",
    "section": "",
    "text": "Decisions for the Decade\nIn the future, the instructors will facilitate the Decisions for the Decade game as taught at The Society for Decision Making Under Deep Uncertainty (DMDU) Annual Summer School. A former facilitator shared their slides with me, along with some suggestions for running the game. Since these materials are not open-source, I will not share these publicly. A very similar alternative is run by the Red Cross, with an overview video and facilitation instructions here. Please checkout the video to get the gist of the game. I’m sorry that we cannot run this for the online training."
  },
  {
    "objectID": "slides/lecture01-intro.html#how-did-you-make-these-decisions",
    "href": "slides/lecture01-intro.html#how-did-you-make-these-decisions",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "How did you make these decisions?",
    "text": "How did you make these decisions?\n\nWhere to go to college?\nWhether/where to go to graduate school?\nWhat car to buy?\nWhat house/apartment to buy/rent?"
  },
  {
    "objectID": "slides/lecture01-intro.html#wicked-planning-problems",
    "href": "slides/lecture01-intro.html#wicked-planning-problems",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Wicked planning problems",
    "text": "Wicked planning problems\n\n\n\n\nThe search for scientific bases for confronting problems of social policy is bound to fail, becuase of the nature of these problems. They are “wicked” problems…\n\n\n[there are no] “optimal solutions” to social problems unless severe qualifications are imposed first…\n\n\n— Rittel, H.W.J., Webber, M.M. Dilemmas in a general theory of planning. Policy Sci 4, 155–169 (1973)."
  },
  {
    "objectID": "slides/lecture01-intro.html#the-stakes-are-high-for-wicked-problems",
    "href": "slides/lecture01-intro.html#the-stakes-are-high-for-wicked-problems",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "The stakes are high for wicked problems",
    "text": "The stakes are high for wicked problems\n\n\n\n“Every solution to a wicked problem is a ‘one-shot operation’; because there is no opportunity to learn by trial-and-error, every attempt counts significantly” — ibid\n\n\n\n\n\n\n\n\nFigure 1: : Urban Renewal in Eastwick Philadelphia/Segregation by Design"
  },
  {
    "objectID": "slides/lecture01-intro.html#inaction-is-a-decision",
    "href": "slides/lecture01-intro.html#inaction-is-a-decision",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Inaction is a decision",
    "text": "Inaction is a decision\nWhat examples can you think of where inaction has negative consequences?\n\n\n\n\n\n\nBurst pipe\nSmall fire in house\nDowned electrity line\nNovel viruses\nClimate change"
  },
  {
    "objectID": "slides/lecture01-intro.html#humans-are-changing-the-climate",
    "href": "slides/lecture01-intro.html#humans-are-changing-the-climate",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Humans are changing the climate",
    "text": "Humans are changing the climate\n\n\nFigure 2: : Figure SPM.2 in IPCC, 2021: Summary for Policymakers."
  },
  {
    "objectID": "slides/lecture01-intro.html#this-intersects-with-many-dimensions-of-global-change",
    "href": "slides/lecture01-intro.html#this-intersects-with-many-dimensions-of-global-change",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "This intersects with many dimensions of global change",
    "text": "This intersects with many dimensions of global change\n\n\n\n\n\n\nFigure 3: : Figure SPM.1 in IPCC, 2022: Summary for Policymakers."
  },
  {
    "objectID": "slides/lecture01-intro.html#many-already-plan-and-make-decisions-to-manage-their-risks",
    "href": "slides/lecture01-intro.html#many-already-plan-and-make-decisions-to-manage-their-risks",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Many already plan and make decisions to manage their risks",
    "text": "Many already plan and make decisions to manage their risks\n\n\nInternational agreements to reduce emissions\nNational investments in decarbonization\nLarge-scale public infrastructure projects\nHousehold decisions to relocate or harden\nAnd don’t forget: inaction is a decision\n\n\n\n\nAre actors making good decisions? Can they do better?"
  },
  {
    "objectID": "slides/lecture01-intro.html#in-this-course-you-will-learn-how-to",
    "href": "slides/lecture01-intro.html#in-this-course-you-will-learn-how-to",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "In this course, you will learn how to:",
    "text": "In this course, you will learn how to:\n\n\nFrame climate decision problems in a helpful way.\nApply multi-objective robust decision-making frameworks to real-world problems using open source software tools.\nEvaluate trade-offs between competing objectives using appropriate quantitative techniques.\nIdentify actionable insights for addressing problems characterized by deep uncertainties.\nCommunicate complex technical analyses clearly and effectively.\nDevelop professional-quality deliverables.\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe are studying an approach to how decisions could be made. We are not studying how decisions are made. Closing the gap between these two areas of focus might help make decision analysis more useful. That’s the topic of an upcoming Thayer course!"
  },
  {
    "objectID": "slides/lecture01-intro.html#a-few-recent-examples-from-our-group",
    "href": "slides/lecture01-intro.html#a-few-recent-examples-from-our-group",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "A few recent examples from our group",
    "text": "A few recent examples from our group\n\n\n\n\n\nSource: Biden White House\n\n\n\nPollack et al. (2025): What funding rules promote equity in climate adaptation outcomes?\n\n\n\n\n\n\nSource: Sharon Karr/FEMA\n\n\n\nZarekarizi et al. (2020): How high to elevate a home?"
  },
  {
    "objectID": "slides/lecture01-intro.html#meet-your-instructor",
    "href": "slides/lecture01-intro.html#meet-your-instructor",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Meet your instructor",
    "text": "Meet your instructor\n\n\n\n\n\n\n\n\nFigure 4: Dr. Adam Pollack (right), your instructor!\n\n\n\n\n\nI research:\n\nFlood-risk management\nProblems where people don’t agree on success\nWays to make decision analysis more useful\n\nFrom Port Washington, NY (via Boston, MA and Centerport, NY)\nI used to own a popcorn business"
  },
  {
    "objectID": "slides/lecture01-intro.html#meet-each-other",
    "href": "slides/lecture01-intro.html#meet-each-other",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Meet each other",
    "text": "Meet each other\n\nWhat is your name?\nWhat is your year of study?\nWhat decision problem interests you the most?\nWhat are you looking to get out of this course?\nWhat is one fun fact about you?"
  },
  {
    "objectID": "slides/lecture01-intro.html#our-course-is-fully-online",
    "href": "slides/lecture01-intro.html#our-course-is-fully-online",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Our course is fully online!",
    "text": "Our course is fully online!\n\nCheck out the website\nCheck out the syllabus\nCheck out the schedule"
  },
  {
    "objectID": "slides/lecture01-intro.html#expectations-from-students",
    "href": "slides/lecture01-intro.html#expectations-from-students",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Expectations from students",
    "text": "Expectations from students\n\ncome prepared to class (e.g., by carefully reading and synthesizing the reading assignments before class and being ready to present their synthesis in class);\nactively contribute to the group discussions and activities;\nsubmit the assignments on time; and\nschedule office hours as needed.\n\nStudents should expect to spend roughly three times the in-course hours outside the classroom for readings and assignments.\n\n\n\n\n\n\nNote\n\n\nFor the training, I also expect you to provide structured feedback at the end of each module on how we can refine the course to make it better for students in the future. In exchange, you will get co-development credit on the final published version of this Fall 25 open-source course."
  },
  {
    "objectID": "slides/lecture01-intro.html#assignments-and-grading",
    "href": "slides/lecture01-intro.html#assignments-and-grading",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Assignments and grading",
    "text": "Assignments and grading\n\nActive Participation (10%)\nComputational Labs (30%)\nCourse Project\n\nProgress Reports (20%)\nPresentation (20%)\nReport (20%)"
  },
  {
    "objectID": "slides/lecture01-intro.html#there-are-seven-modules-in-this-course",
    "href": "slides/lecture01-intro.html#there-are-seven-modules-in-this-course",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "There are seven modules in this course",
    "text": "There are seven modules in this course\n\n“Overview”: Overview of decision analysis for wicked climate problems\n“Uncertainty”: Uncertainty and Monte Carlo analysis\n“RDM”: Robust decision-making\n“Trade-offs”: Multiple objectives and navigating trade-offs\n“DMDU”: Decision making under deep uncertainty\n“Gaps”: Gaps between decision analysis and decision support\n“Projects”: Student project presentations"
  },
  {
    "objectID": "slides/lecture01-intro.html#class-structure",
    "href": "slides/lecture01-intro.html#class-structure",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Class structure",
    "text": "Class structure\nEmulating the 2 Spot (MWF 2:00-2:55pm, No Tu X-hr)\n\n\n\n\n\n\nNote\n\n\nLet’s discuss how we do the labs for this training\n\n\n\n\nMondays\n\nLecture\nTake notes and ask questions\nSlides posted by Sunday night (often earlier)\n\n\n\nWednesdays\n\nSerious (but fun!) game or student-led journal club\nBe prepared to synthesize lessons into your lab report and project update\nSometimes we will start longer labs on Wednesday\n\n\n\nFridays\n\nLab\nMake substantial progress together\nLab report due before next lab"
  },
  {
    "objectID": "slides/lecture01-intro.html#communication",
    "href": "slides/lecture01-intro.html#communication",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Communication",
    "text": "Communication\n\nQuestions during class is best\nSlack is great - not as good as in class, but still promotes open discussion\nOffice hours are great (including when the issue is urgent and/or private)\nEmail is ok if issue is urgent and/or private and you can’t make office hours"
  },
  {
    "objectID": "slides/lecture01-intro.html#overall-guidelines",
    "href": "slides/lecture01-intro.html#overall-guidelines",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Overall guidelines",
    "text": "Overall guidelines\n\nCollaboration is highly encouraged and a skill we will practice in course, but all work must reflect your own understanding.\n\nSee GenAI policy on syllabus\nAlways cite external references\n\nA rubric will be available for all graded assignments and you can easily find it from the schedule webpage.\nAssume good faith of others and engage in good faith yourself.\n\n\n\n\n\n\n\nWarning\n\n\nDecision analysis is values-laden and good decision analysis is explicit about normative assumptions. Please do not outsource the opportunity to learn this vital skill."
  },
  {
    "objectID": "slides/lecture01-intro.html#this-week",
    "href": "slides/lecture01-intro.html#this-week",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "This week",
    "text": "This week\nClass\n\nTuesday (No Meeting): computing setup (do on your own if you want to work through labs)\nWednesday: lecture on framing a decision analysis\nFriday (No Meeting): getting comfortable with our computing setup\n\nAssignments\n\nCatch up on Readings\nProject progress report 1\n\nWhat decision problem will you focus on this term?\nSee here for more guidance\nDue next Friday"
  },
  {
    "objectID": "slides/lecture01-intro.html#next-week",
    "href": "slides/lecture01-intro.html#next-week",
    "title": "Welcome to Decision Analysis for Wicked Climate Problems!",
    "section": "Next week",
    "text": "Next week\n\nStarting Uncertainty module\nMore practice with our computing setup\nFirst serious game of the term\nFirst lab and lab report of the term"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#defining-challenges",
    "href": "slides/lecture06-dmdu.html#defining-challenges",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Defining challenges",
    "text": "Defining challenges\nComing to grips with irreducible uncertainty:\n\nLimits in predictability of complex systems\nDifferent perspectives on the problem, system, and objectives\nDynamic and unknowable futures"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#finding-strategies-that-are-robust-to-irreducible-uncertainties",
    "href": "slides/lecture06-dmdu.html#finding-strategies-that-are-robust-to-irreducible-uncertainties",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Finding strategies that are robust to irreducible uncertainties",
    "text": "Finding strategies that are robust to irreducible uncertainties\nThis entails:\n\nExploratory modeling\nAdaptive planning\nDecision support"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#weve-seen-this-before",
    "href": "slides/lecture06-dmdu.html#weve-seen-this-before",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "We’ve seen this before!",
    "text": "We’ve seen this before!\nLempert and Collins (2007)\n\n\nExploratory modeling\n\n\nSimple model of lake dynamics\nEvaluates decisions optimal for one critical P over a wide range of critical P scenarios\n\n\nAdaptive planning\n\n\nThe town learns about critical P over time and determines their anthropogenic inflows based on observed P and best-guess critical P\n\n\nDecision support\n\n\nCalculates two robustness metrics and visualizes results in several ways"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#exploratory-modeling",
    "href": "slides/lecture06-dmdu.html#exploratory-modeling",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Exploratory modeling",
    "text": "Exploratory modeling\n\n\nWhat?\n\nScenario-based, what-if modeling\n\nHow?\n\nRunning computational models (often simple) many times\nOften search for strategies using optimization algorithms\n\nWhy?\n\nDeriving the consequences of actions on complex systems with mental models alone is infeasible\nConsidering only pre-specified strategies limits the ability of decision-makers to learn about the problem and identify robust solutions"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#adaptive-planning",
    "href": "slides/lecture06-dmdu.html#adaptive-planning",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Adaptive planning",
    "text": "Adaptive planning\n\n\nWhat?\n\nStrategies guide future decisions based on how the future unfolds\n\nHow?\n\nExplore a wide variety of futures during plan design\nIdentify actions that are best suited to different futures, as well as what signals ensure the timely implementation of those actions\n\nWhy?\n\nAdaptive strategies are flexible, which contributes to robustness in the face of deep and dynamic uncertainties"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#decision-support",
    "href": "slides/lecture06-dmdu.html#decision-support",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Decision support",
    "text": "Decision support\n\n\nWhat?\n\nAn iterative, collaborative approach that facilitates learning across alternative framings of the problem and learning about stakeholder preferences and trade-offs\n\nHow?\n\nGood question! This is an area with little guidance in the field…\n\nWhy?\n\nUnder deep uncertainty, decision support moves away from trying to define what is the “right” choice and instead enable decision-makers to converge on an acceptable decision"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#high-level-overview",
    "href": "slides/lecture06-dmdu.html#high-level-overview",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "High-level overview",
    "text": "High-level overview\n\nBartholomew and Kwakkel (2020)"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#what-these-approaches-share",
    "href": "slides/lecture06-dmdu.html#what-these-approaches-share",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "What these approaches share",
    "text": "What these approaches share\n\nAll three building blocks discussed earlier\nThe use of a multi-objective evolutionary algorithm (MOEA) to identify a large, diverse set of strategies that capture trade-offs"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#whats-wrong-with-mordm",
    "href": "slides/lecture06-dmdu.html#whats-wrong-with-mordm",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "What’s “wrong” with MORDM?",
    "text": "What’s “wrong” with MORDM?\n\n\nThe main idea for Multi-Scenario MORDM or Many-Objective Robust Optimization comes the observation that solutions found through optimization for a reference scenario can have very poor performance in other scenarios.\n\n\n\n\nRemember lab?"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#why-multi-scenario-mordm",
    "href": "slides/lecture06-dmdu.html#why-multi-scenario-mordm",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Why Multi-Scenario MORDM?",
    "text": "Why Multi-Scenario MORDM?\n\nThe main innovation of MORDM is using a MOEA to obtain a large, diverse set of strategies that capture trade-offs\nMulti-Scenario MORDM builds on this by adding more reference scenarios (based on scenario discovery) to enhance diversity of strategy alternatives that are Pareto optimal strategies under different scenarios\n\n\nBartholomew and Kwakkel (2020) recommend this approach as a good balance for identifying candidates that are optimal within scenarios, robust across scenarios, and not too computationally challenging to find."
  },
  {
    "objectID": "slides/lecture06-dmdu.html#why-moro",
    "href": "slides/lecture06-dmdu.html#why-moro",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Why MORO?",
    "text": "Why MORO?\n\n\nSince the overarching aim of supporting decision making under deep uncertainty is the identification of robust strategies that offer an acceptable performance across multiple competing objectives, why not include these robustness considerations already in the search phase for candidate solutions?\n\n\nBartholomew and Kwakkel (2020)\n\nBut this approach is computationally intense and can sacrifice identifying strategies that are optimal in any given scenario!"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#approaches-that-go-by-different-names-but-fit-into-the-above-classes",
    "href": "slides/lecture06-dmdu.html#approaches-that-go-by-different-names-but-fit-into-the-above-classes",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Approaches that go by different names, but fit into the above classes",
    "text": "Approaches that go by different names, but fit into the above classes\n\nDeeply uncertain pathways, a variant of MORO, by Trindade et al. (2019)\nA novel approach to scenario discovery to enhance Multi-Scenario MORDM by Shavazipour et al. (2021)"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#key-features-to-consider-in-reviewing-studies",
    "href": "slides/lecture06-dmdu.html#key-features-to-consider-in-reviewing-studies",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "Key features to consider in reviewing studies",
    "text": "Key features to consider in reviewing studies\n\n\nIdentify how the optimization handles “shallow” uncertainties\nIdentify whether robustness is an objective in the MOEA\n\nEven when robustness is an objective, studies often evaluate robustness on a new set of samples as a form of “validation”\n\nIn evaluating the performance of strategies, do they assume all scenarios are equally likely?\n\nMulti-scenario MORDM and MORO make it really important to understand how a study calculates objective values for strategies"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#what-you-shouldnt-implement",
    "href": "slides/lecture06-dmdu.html#what-you-shouldnt-implement",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "What you shouldn’t implement",
    "text": "What you shouldn’t implement\nDo not:\n\n\nUse an approach you don’t understand and can’t write clearly about\nEvaluate objectives across scenarios without an explicit probability distribution (but then is your problem really deeply uncertain?)\nNeglect well-characterized uncertainties"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#what-should-you-implement",
    "href": "slides/lecture06-dmdu.html#what-should-you-implement",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "What should you implement?",
    "text": "What should you implement?\nThe uncomfortable truth is that the literature does not provide clear guidance on this question, and that’s probably a major reason you were all interested in taking this course.\n\nAs always, you can justify your choice with strong framing :)\n\n\nAnd this is what the strong papers and analyses do. Reach out to someone whose work you admire!"
  },
  {
    "objectID": "slides/lecture06-dmdu.html#this-week",
    "href": "slides/lecture06-dmdu.html#this-week",
    "title": "Quantitative approaches for supporting decision-making under deep uncertainty",
    "section": "This week",
    "text": "This week\n\nCase study Wednesday\nPossibly a tutorial Friday, otherwise open Q&A/take stock"
  },
  {
    "objectID": "slides/case03-dps.html#summarize-the-study",
    "href": "slides/case03-dps.html#summarize-the-study",
    "title": "Direct policy search",
    "section": "Summarize the study",
    "text": "Summarize the study\nSuccintly, please :)\n\n\nWhat is the study about?\nHow does it address its research question(s)?\nWhat does the study find?\nWhat strengths did you find?\nWhat weaknesses did you find?\nHow will you think about this paper going forward with your decision analysis?"
  },
  {
    "objectID": "slides/case03-dps.html#optimization-concepts",
    "href": "slides/case03-dps.html#optimization-concepts",
    "title": "Direct policy search",
    "section": "Optimization concepts",
    "text": "Optimization concepts\nDefine the following, and explain how it connects to the study:\n\n\nDeeply uncertain tipping point\nOpen loop, intertemporal optimization\nCurse of dimensionality\nDirect policy search\nMulti-objective evolutionary algorithms\nPolicy inertia\nHypervolume\nRobustness analysis\nScenario discovery"
  },
  {
    "objectID": "slides/case03-dps.html#discuss-the-studys-approach",
    "href": "slides/case03-dps.html#discuss-the-studys-approach",
    "title": "Direct policy search",
    "section": "Discuss the study’s approach",
    "text": "Discuss the study’s approach\nAnswer the following questions:\n\n\nHow did the authors implement MORDM?\nWhich optimization approach converges faster?\nWhich optimization approach is “better?” How?\nHow robust are any of the “optimal” strategies?\nAre there reasons to prefer DPS beyond the study’s results?"
  },
  {
    "objectID": "slides/case03-dps.html#which-strategy-is-better-in-the-reference-scenario",
    "href": "slides/case03-dps.html#which-strategy-is-better-in-the-reference-scenario",
    "title": "Direct policy search",
    "section": "Which strategy is better in the reference scenario?",
    "text": "Which strategy is better in the reference scenario?\n\n\n\nQuinn et al., (2017)"
  },
  {
    "objectID": "slides/case03-dps.html#how-do-the-strategies-manage-the-tipping-point",
    "href": "slides/case03-dps.html#how-do-the-strategies-manage-the-tipping-point",
    "title": "Direct policy search",
    "section": "How do the strategies manage the tipping point?",
    "text": "How do the strategies manage the tipping point?\n\n\n\nQuinn et al., (2017)"
  },
  {
    "objectID": "slides/case03-dps.html#why-do-the-dps-strategies-fail",
    "href": "slides/case03-dps.html#why-do-the-dps-strategies-fail",
    "title": "Direct policy search",
    "section": "Why do the DPS strategies fail?",
    "text": "Why do the DPS strategies fail?\n\n\n\nQuinn et al., (2017)"
  },
  {
    "objectID": "slides/case03-dps.html#open-discussion",
    "href": "slides/case03-dps.html#open-discussion",
    "title": "Direct policy search",
    "section": "Open discussion",
    "text": "Open discussion\nWhat would you like to discuss about the paper and/or optimization/DPS connections to your project?"
  },
  {
    "objectID": "slides/case03-dps.html#friday",
    "href": "slides/case03-dps.html#friday",
    "title": "Direct policy search",
    "section": "Friday",
    "text": "Friday\n\nDPS tutorial"
  },
  {
    "objectID": "slides/case03-dps.html#next-week",
    "href": "slides/case03-dps.html#next-week",
    "title": "Direct policy search",
    "section": "Next week",
    "text": "Next week\n\nMonday: Scenario discovery lecture\nWednesday: Prabhat leads optimization tutorial\nFriday: case study"
  },
  {
    "objectID": "slides/lecture02-framing.html#how-did-you-make-these-decisions",
    "href": "slides/lecture02-framing.html#how-did-you-make-these-decisions",
    "title": "Framing a decision analysis",
    "section": "How did you make these decisions?",
    "text": "How did you make these decisions?\n\nWhat did you eat for your last meal?\nWhat colleges did you apply to?\nHow did you choose your PhD program?\n\n\nhttps://xkcd.com/1445/"
  },
  {
    "objectID": "slides/lecture02-framing.html#framing-refers-to-how-you-structure-a-decision-problem",
    "href": "slides/lecture02-framing.html#framing-refers-to-how-you-structure-a-decision-problem",
    "title": "Framing a decision analysis",
    "section": "Framing refers to how you structure a decision problem",
    "text": "Framing refers to how you structure a decision problem\nFraming breakfast decisions\n\n\nI want to eat a healthy breakfast every morning that gives me energy\nIt needs to be affordable, tasty, low in sugar, etc.,\nOatmeal with peanut butter and frozen berries, toast with peanut butter and bananas, etc.,"
  },
  {
    "objectID": "slides/lecture02-framing.html#framing-informs-how-you-analyze-a-decision",
    "href": "slides/lecture02-framing.html#framing-informs-how-you-analyze-a-decision",
    "title": "Framing a decision analysis",
    "section": "Framing informs how you analyze a decision",
    "text": "Framing informs how you analyze a decision\nFraming breakfast decisions, continued\n\n\nI can calculate the costs, macronutrients, etc.,\nI can map out which breakfasts do better/worse on my objectives"
  },
  {
    "objectID": "slides/lecture02-framing.html#getting-theoretical-about-it",
    "href": "slides/lecture02-framing.html#getting-theoretical-about-it",
    "title": "Framing a decision analysis",
    "section": "Getting theoretical about it",
    "text": "Getting theoretical about it\n\n\n\n\n\n\n\nG\n\n\n\nClarify the\\ndecision\\ncontext\n\nClarify the\ndecision\ncontext\n\n\n\nDefine\\nobjectives\n\nDefine\nobjectives\n\n\n\nClarify the\\ndecision\\ncontext-&gt;Define\\nobjectives\n\n\n\n\n\nDevelop\\nalternatives\n\nDevelop\nalternatives\n\n\n\nDefine\\nobjectives-&gt;Develop\\nalternatives\n\n\n\n\n\nEstimate\\nmetrics\n\nEstimate\nmetrics\n\n\n\nDevelop\\nalternatives-&gt;Estimate\\nmetrics\n\n\n\n\n\nEvaluate\\ntrade-offs\n\nEvaluate\ntrade-offs\n\n\n\nEstimate\\nmetrics-&gt;Evaluate\\ntrade-offs\n\n\n\n\n\nImplement,\\nmonitor,\\nreview\n\nImplement,\nmonitor,\nreview\n\n\n\nEvaluate\\ntrade-offs-&gt;Implement,\\nmonitor,\\nreview\n\n\n\n\n\nImplement,\\nmonitor,\\nreview-&gt;Clarify the\\ndecision\\ncontext\n\n\n\n\n\n\n\n\n\n\n\nAdapted from Gregory et al. (2012)"
  },
  {
    "objectID": "slides/lecture02-framing.html#framing-is-iterative-and-deliberative",
    "href": "slides/lecture02-framing.html#framing-is-iterative-and-deliberative",
    "title": "Framing a decision analysis",
    "section": "Framing is iterative and deliberative",
    "text": "Framing is iterative and deliberative\n\n\n\nFrom Lempert (2019)"
  },
  {
    "objectID": "slides/lecture02-framing.html#how-should-i-travel-from-hanover-to-philadelphia",
    "href": "slides/lecture02-framing.html#how-should-i-travel-from-hanover-to-philadelphia",
    "title": "Framing a decision analysis",
    "section": "How should I travel from Hanover to Philadelphia?",
    "text": "How should I travel from Hanover to Philadelphia?"
  },
  {
    "objectID": "slides/lecture02-framing.html#how-to-preserve-access-to-clean-water-in-the-growing-us-southwest",
    "href": "slides/lecture02-framing.html#how-to-preserve-access-to-clean-water-in-the-growing-us-southwest",
    "title": "Framing a decision analysis",
    "section": "How to preserve access to clean water in the growing US Southwest?",
    "text": "How to preserve access to clean water in the growing US Southwest?"
  },
  {
    "objectID": "slides/lecture02-framing.html#why-is-framing-water-management-more-challenging-than-travel-recommendations",
    "href": "slides/lecture02-framing.html#why-is-framing-water-management-more-challenging-than-travel-recommendations",
    "title": "Framing a decision analysis",
    "section": "Why is framing water management more challenging than travel recommendations?",
    "text": "Why is framing water management more challenging than travel recommendations?\nA few hints from our examples:\n\nAnalyzing travel options was relatively simple\nWe got to the bottom of my travel goals quickly\nI have (most of) the decision-making agency and chose my favorite option"
  },
  {
    "objectID": "slides/lecture02-framing.html#conceptual-models",
    "href": "slides/lecture02-framing.html#conceptual-models",
    "title": "Framing a decision analysis",
    "section": "Conceptual models",
    "text": "Conceptual models\n\nConceptual model for addressing inland flood risk from Helgeson et al. (2024)"
  },
  {
    "objectID": "slides/lecture02-framing.html#another-conceptual-model",
    "href": "slides/lecture02-framing.html#another-conceptual-model",
    "title": "Framing a decision analysis",
    "section": "Another conceptual model",
    "text": "Another conceptual model\n\nUnpublished conceptual model for federal funding rules to reduce flood risk"
  },
  {
    "objectID": "slides/lecture02-framing.html#values-informed-mental-models",
    "href": "slides/lecture02-framing.html#values-informed-mental-models",
    "title": "Framing a decision analysis",
    "section": "Values-informed mental models",
    "text": "Values-informed mental models\n\nValues mapping on execerpt of the conceptual model from Helgeson et al. (2024)"
  },
  {
    "objectID": "slides/lecture02-framing.html#xlrm-diagrams",
    "href": "slides/lecture02-framing.html#xlrm-diagrams",
    "title": "Framing a decision analysis",
    "section": "XLRM diagrams",
    "text": "XLRM diagrams\n\nFrom Pollack et al. (2025)"
  },
  {
    "objectID": "slides/lecture02-framing.html#next-week",
    "href": "slides/lecture02-framing.html#next-week",
    "title": "Framing a decision analysis",
    "section": "Next week",
    "text": "Next week\n\nDo the 9/22 readings\nStarting Uncertainty module\nWe will only meet Monday next week - prioritize your project proposal and schedule office hours as needed\nLet me know if you are testing out the labs and need any help"
  },
  {
    "objectID": "slides/lecture02-framing.html#works-cited",
    "href": "slides/lecture02-framing.html#works-cited",
    "title": "Framing a decision analysis",
    "section": "Works cited",
    "text": "Works cited\n\n\n\n\nGregory, R., Failing, L., Harstone, M., Long, G., McDaniels, T., & Ohlson, D. (2012). Structuring environmental management choices. In Structured decision making (pp. 1–20). Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/9781444398557.ch1\n\n\nHelgeson, C., Keller, K., Nicholas, R. E., Srikrishnan, V., Cooper, C., Smithwick, E. A. H., & Tuana, N. (2024). Integrating values to improve the relevance of climate‐risk research. Earths Future, 12(10), e2022EF003025. https://doi.org/10.1029/2022ef003025\n\n\nLempert, R. J. (2019). Robust decision making (RDM). In Decision making under deep uncertainty (pp. 23–51). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05252-2\\_2\n\n\nPollack, A. B., Santamaria-Aguilar, S., Maduwantha, P., Helgeson, C., Wahl, T., & Keller, K. (2025). Funding rules that promote equity in climate adaptation outcomes. Proceedings of the National Academy of Sciences, 122(2), e2418711121. https://doi.org/10.1073/pnas.2418711121"
  },
  {
    "objectID": "slides/lecture04-rdm.html#the-dmdu-argument-against-predict-then-act",
    "href": "slides/lecture04-rdm.html#the-dmdu-argument-against-predict-then-act",
    "title": "Robust decision-making",
    "section": "The DMDU Argument Against Predict-Then-Act",
    "text": "The DMDU Argument Against Predict-Then-Act\n\n\n\nwicked problems are not well-bounded, are framed differently by various stakeholders, and are not well-understood until after formulation of a solution. Using predictions to adjudicate such problems skews attention toward the proverbial lamp post, not the true location of the keys to a policy solution.\n\n\n— Lempert, 2019. Robust Decision Making (RDM)."
  },
  {
    "objectID": "slides/lecture04-rdm.html#finding-robust-solutions-that-perform-well-over-a-wide-range-of-futures",
    "href": "slides/lecture04-rdm.html#finding-robust-solutions-that-perform-well-over-a-wide-range-of-futures",
    "title": "Robust decision-making",
    "section": "Finding robust solutions that “perform well” over a “wide range” of futures",
    "text": "Finding robust solutions that “perform well” over a “wide range” of futures\n\nRob Lempert, PhD; & Michelle Miro, PhD. Online Training for Water Utilities—WUCA / Chapter 3: Plan"
  },
  {
    "objectID": "slides/lecture04-rdm.html#measuring-robustness",
    "href": "slides/lecture04-rdm.html#measuring-robustness",
    "title": "Robust decision-making",
    "section": "Measuring robustness",
    "text": "Measuring robustness\n\n\n\nMcPhail et al. (2019)"
  },
  {
    "objectID": "slides/lecture04-rdm.html#regret-the-cost-of-choosing-incorrectly",
    "href": "slides/lecture04-rdm.html#regret-the-cost-of-choosing-incorrectly",
    "title": "Robust decision-making",
    "section": "Regret: The “cost” of choosing “incorrectly”",
    "text": "Regret: The “cost” of choosing “incorrectly”\nAdaptating notation in Lempert and Collins (2007), the regret of action a in state x is:\n\\(R_{a}(x) = Max_{a'}[PV(U_{a'}(x))] - PV(U_{a}(x))\\),\nthe difference between the present value expected utility of the optimal action in a considered state of the world and the considered action’s present value expected utility in that state of the world.\nNote: There are many ways to go from an action’s regret metric in one state (i.e., \\(T_{1}\\)) to evaluating its overall robustness (i.e., \\(T_{2}\\) and \\(T_{3}\\))."
  },
  {
    "objectID": "slides/lecture04-rdm.html#satisficing",
    "href": "slides/lecture04-rdm.html#satisficing",
    "title": "Robust decision-making",
    "section": "Satisficing",
    "text": "Satisficing\nFollowing McPhail et al. (2019), this approach “seeks to meet a minimum performance threshold.”\nLooking across states (i.e., \\(T_{2}\\) and \\(T_{3}\\)), the domain criterion is a common approach (e.g., see Herman et al., (2015)), which analysts often implement as the fraction of states of the world in which the performance threshold is met."
  },
  {
    "objectID": "slides/lecture04-rdm.html#schematic",
    "href": "slides/lecture04-rdm.html#schematic",
    "title": "Robust decision-making",
    "section": "Schematic",
    "text": "Schematic\n\nLempert, 2019. Robust Decision Making (RDM)."
  },
  {
    "objectID": "slides/lecture04-rdm.html#framing",
    "href": "slides/lecture04-rdm.html#framing",
    "title": "Robust decision-making",
    "section": "Framing",
    "text": "Framing\n\nGroves et al., 2019. Robust Decision Making (RDM): Application to Water Planning and Climate Policy."
  },
  {
    "objectID": "slides/lecture04-rdm.html#identifying-vulnerabilities",
    "href": "slides/lecture04-rdm.html#identifying-vulnerabilities",
    "title": "Robust decision-making",
    "section": "Identifying Vulnerabilities",
    "text": "Identifying Vulnerabilities\n\nGroves et al., 2019. Robust Decision Making (RDM): Application to Water Planning and Climate Policy."
  },
  {
    "objectID": "slides/lecture04-rdm.html#designing-strategies-adaptive-to-vulnerabilities",
    "href": "slides/lecture04-rdm.html#designing-strategies-adaptive-to-vulnerabilities",
    "title": "Robust decision-making",
    "section": "Designing Strategies Adaptive to Vulnerabilities",
    "text": "Designing Strategies Adaptive to Vulnerabilities\n\nGroves et al., 2019. Robust Decision Making (RDM): Application to Water Planning and Climate Policy."
  },
  {
    "objectID": "slides/lecture04-rdm.html#tradeoff-analysis",
    "href": "slides/lecture04-rdm.html#tradeoff-analysis",
    "title": "Robust decision-making",
    "section": "Tradeoff Analysis",
    "text": "Tradeoff Analysis\n\nGroves et al., 2019. Robust Decision Making (RDM): Application to Water Planning and Climate Policy."
  },
  {
    "objectID": "slides/lecture04-rdm.html#evaluating-robustness",
    "href": "slides/lecture04-rdm.html#evaluating-robustness",
    "title": "Robust decision-making",
    "section": "Evaluating Robustness",
    "text": "Evaluating Robustness\n\nGroves et al., 2019. Robust Decision Making (RDM): Application to Water Planning and Climate Policy."
  },
  {
    "objectID": "slides/lecture04-rdm.html#iterating-with-decision-makers",
    "href": "slides/lecture04-rdm.html#iterating-with-decision-makers",
    "title": "Robust decision-making",
    "section": "Iterating with Decision-makers",
    "text": "Iterating with Decision-makers\nMany potential conditions where current strategies insufficient:\n\n\n\nGroves et al., 2019. Robust Decision Making (RDM): Application to Water Planning and Climate Policy."
  },
  {
    "objectID": "slides/lecture04-rdm.html#we-are-not-completely-ignorant-about-future-probabilities",
    "href": "slides/lecture04-rdm.html#we-are-not-completely-ignorant-about-future-probabilities",
    "title": "Robust decision-making",
    "section": "We are not completely ignorant about future probabilities",
    "text": "We are not completely ignorant about future probabilities\n\n\n\nHausfather and Peters (2020). RCP8.5 is a problematic scenario for near-term emissions"
  },
  {
    "objectID": "slides/lecture04-rdm.html#alternative-synthesize-subjective-beliefs-about-deep-uncertainties",
    "href": "slides/lecture04-rdm.html#alternative-synthesize-subjective-beliefs-about-deep-uncertainties",
    "title": "Robust decision-making",
    "section": "Alternative: Synthesize subjective beliefs about deep uncertainties",
    "text": "Alternative: Synthesize subjective beliefs about deep uncertainties\n\nDoss-Gollin and Keller (2023)"
  },
  {
    "objectID": "slides/lecture04-rdm.html#this-week",
    "href": "slides/lecture04-rdm.html#this-week",
    "title": "Robust decision-making",
    "section": "This week",
    "text": "This week\n\nCase study on Wednesday\nTutorial on RDM on Friday"
  },
  {
    "objectID": "slides/lecture04-rdm.html#next-week",
    "href": "slides/lecture04-rdm.html#next-week",
    "title": "Robust decision-making",
    "section": "Next week",
    "text": "Next week\n\nLecture on Monday - area I have most experience in re: MORDM\nJournal Club Wednesday\nTutorial on visualizing tradeoffs"
  },
  {
    "objectID": "project/overview.html",
    "href": "project/overview.html",
    "title": "Course Project",
    "section": "",
    "text": "The goal of the course project is to develop and report on an actionable plan for implementing a quantitative decision analysis. This project can be highly synergistic with thesis projects.",
    "crumbs": [
      "Course Project"
    ]
  },
  {
    "objectID": "project/overview.html#overview",
    "href": "project/overview.html#overview",
    "title": "Course Project",
    "section": "",
    "text": "The goal of the course project is to develop and report on an actionable plan for implementing a quantitative decision analysis. This project can be highly synergistic with thesis projects.",
    "crumbs": [
      "Course Project"
    ]
  },
  {
    "objectID": "project/overview.html#schedule",
    "href": "project/overview.html#schedule",
    "title": "Course Project",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\nMilestone\nInstructions\nDue Date\n\n\n\n\nProgress Reports\n\nPlease see within page\n\n\nPresentation\n\nNov. 16, 2025\n\n\nReport\n\nNov. 26, 2025",
    "crumbs": [
      "Course Project"
    ]
  },
  {
    "objectID": "project/updates.html",
    "href": "project/updates.html",
    "title": "Project Progress Reports",
    "section": "",
    "text": "At the end of each module, you will have one week to submit your responses to guided questions on how you will integrate new concepts into your final project. I highly encourage you to work on these during the module to stay on track with your project.\nNote that the Gaps module does not have a progress report.",
    "crumbs": [
      "Course Project",
      "Module Reports"
    ]
  },
  {
    "objectID": "project/updates.html#overview",
    "href": "project/updates.html#overview",
    "title": "Project Progress Reports",
    "section": "",
    "text": "At the end of each module, you will have one week to submit your responses to guided questions on how you will integrate new concepts into your final project. I highly encourage you to work on these during the module to stay on track with your project.\nNote that the Gaps module does not have a progress report.",
    "crumbs": [
      "Course Project",
      "Module Reports"
    ]
  },
  {
    "objectID": "project/updates.html#schedule",
    "href": "project/updates.html#schedule",
    "title": "Project Progress Reports",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\nModule\nInstructions\nDue Date\n\n\n\n\nOverview\n\n9/26/25\n\n\nUncertainty\n\n10/3/25\n\n\nRDM\n\n10/10/25\n\n\nTrade-offs\n\n10/17/25\n\n\nDMDU\n\n10/31/25",
    "crumbs": [
      "Course Project",
      "Module Reports"
    ]
  },
  {
    "objectID": "project/updates.html#rubric",
    "href": "project/updates.html#rubric",
    "title": "Project Progress Reports",
    "section": "Rubric",
    "text": "Rubric\nYour grade is based on your perceived effort. Even if you do not fully understand how to apply lessons from a module to your project, you can receive full credit for a report if you understand your knowledge gaps and form strong questions for me to provide clarification and direction for you. You can receive 0-3 points on each assignment.\n\n0: No perceived effort\n1: Low perceived effort\n\nE.g., incomplete, no synthesis, no self awareness of knowledge gaps, no questions for me\n\n2: Moderate perceived effort\n\nE.g., Complete but synthesis has important shortcomings and questions are too open ended for me to provide helpful guidance\n\n3: High perceived effort\n\nE.g., Complete and synthesis has important shortcomings but questions are specific and allow me to provide helpful guidance.\n\n\nThe instructions for each report will include examples of specific questions about the module that you can use for free in your report to help boost your grade. I will not be able to anticipate all questions that apply to your understanding and decision problem, so it is no guarantee you receive a 3 if you use some of these example questions.\nI will provide as much detailed feedback on each report as I can. This means that the more thought and effort you put into your report, the more guidance you will receive. This will help you on your project presentation and report. However, you must apply discretion when writing your report and should not include too much or irrelevant details. Concision is a very important feature of strong synthesis.\nLate submissions receive a 25% downgrading of the full credit score for each late day. Extension requests must be made 24 hours before the submission deadline with a submission of what you have achieved so far.",
    "crumbs": [
      "Course Project",
      "Module Reports"
    ]
  },
  {
    "objectID": "readings/readings03.html",
    "href": "readings/readings03.html",
    "title": "Week 3 Readings",
    "section": "",
    "text": "R. J. Lempert (2019)\n\nI usually like to recommend seminal papers where possible, but I think this later piece is a better starting place.\n\nRobert J. Lempert & Collins (2007)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one to give you a working example on how to lead a case study review."
  },
  {
    "objectID": "readings/readings03.html#required-reading",
    "href": "readings/readings03.html#required-reading",
    "title": "Week 3 Readings",
    "section": "",
    "text": "R. J. Lempert (2019)\n\nI usually like to recommend seminal papers where possible, but I think this later piece is a better starting place.\n\nRobert J. Lempert & Collins (2007)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one to give you a working example on how to lead a case study review."
  },
  {
    "objectID": "readings/readings03.html#optional-reading",
    "href": "readings/readings03.html#optional-reading",
    "title": "Week 3 Readings",
    "section": "Optional Reading",
    "text": "Optional Reading\nYou might have better intuition and instincts than me, but it took me many, many readings of the above, papers below, and external material (e.g., presentations, webinars, conversations, etc.,) to feel comfortable with the concept of robustness in the context of robust decision-making - and I still make conceptual mistakes and have a lot to learn. I keep the optional readings to just a few papers because I know some of you will push yourself to read all required and optional readings each week, and I don’t want you to burnout when it’s going to take more than just one reading to feel comfortable with this concept.\n\nGroves et al. (2019)\nHerman Jonathan D. et al. (2015)\nQuinn et al. (2020)\nDoss-Gollin & Keller (2023)\n\n\nReferences\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective bayesian framework for synthesizing deep uncertainties in climate risk management. Earths Future, 11(1). https://doi.org/10.1029/2022ef003044\n\n\nGroves, D. G., Molina-Perez, E., Bloom, E., & Fischbach, J. R. (2019). Robust decision making (RDM): Application to water planning and climate policy. In Decision making under deep uncertainty (pp. 135–163). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05252-2\\_7\n\n\nHerman Jonathan D., Reed Patrick M., Zeff Harrison B., & Characklis Gregory W. (2015). How should robustness be defined for water systems planning under change? Journal of Water Resources Planning and Management, 141(10), 04015012. https://doi.org/10.1061/(ASCE)WR.1943-5452.0000509\n\n\nLempert, R. J. (2019). Robust decision making (RDM). In Decision making under deep uncertainty (pp. 23–51). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05252-2\\_2\n\n\nLempert, Robert J., & Collins, M. T. (2007). Managing the risk of uncertain threshold responses: Comparison of robust, optimum, and precautionary approaches. Risk Anal., 27(4), 1009–1026. https://doi.org/10.1111/j.1539-6924.2007.00940.x\n\n\nQuinn, J. D., Hadjimichael, A., Reed, P. M., & Steinschneider, S. (2020). Can exploratory modeling of water scarcity vulnerabilities and robustness be scenario neutral? Earths Future, 8(11). https://doi.org/10.1029/2020ef001650"
  },
  {
    "objectID": "readings/readings01.html",
    "href": "readings/readings01.html",
    "title": "Week 1 Readings",
    "section": "",
    "text": "Syllabus and Schedule \nKeller et al. (2021)\n\nThis review article provides an accessible overview on climate-risk management, spanning many of the topics that we will cover in the course. I will encourage you to revisit this paper throughout the term.\n\nGregory et al. (2012)\n\nThis textbook chapter provides a nice overview to structuring environmental decision making problems. In my experience interacting with decision scientists at federal agencies, this textbook forms the foundation of their decision analysis knowledge base. This course will try to crosswalk between this literature and the climate-risk-management literature."
  },
  {
    "objectID": "readings/readings01.html#required-reading",
    "href": "readings/readings01.html#required-reading",
    "title": "Week 1 Readings",
    "section": "",
    "text": "Syllabus and Schedule \nKeller et al. (2021)\n\nThis review article provides an accessible overview on climate-risk management, spanning many of the topics that we will cover in the course. I will encourage you to revisit this paper throughout the term.\n\nGregory et al. (2012)\n\nThis textbook chapter provides a nice overview to structuring environmental decision making problems. In my experience interacting with decision scientists at federal agencies, this textbook forms the foundation of their decision analysis knowledge base. This course will try to crosswalk between this literature and the climate-risk-management literature."
  },
  {
    "objectID": "readings/readings01.html#optional-reading",
    "href": "readings/readings01.html#optional-reading",
    "title": "Week 1 Readings",
    "section": "Optional Reading",
    "text": "Optional Reading\nI want to provide you with opportunities to go into various focus areas of Keller et al. (2021) in more detail. I don’t expect you to read all of these in detail. However, I think it will benefit you to at least read all the abstracts and choose one to read in detail that speaks to you.\nMore on wicked problems: Rittel & Webber (1973)\nMore on analyzing complex systems: Reed et al. (2022)\nMore on robust decision-making: Lempert (2002)\nMore on framing a decision analysis: Helgeson et al. (2024)\n\nReferences\n\n\nGregory, R., Failing, L., Harstone, M., Long, G., McDaniels, T., & Ohlson, D. (2012). Structuring environmental management choices. In Structured decision making (pp. 1–20). Chichester, UK: John Wiley & Sons, Ltd. https://doi.org/10.1002/9781444398557.ch1\n\n\nHelgeson, C., Keller, K., Nicholas, R. E., Srikrishnan, V., Cooper, C., Smithwick, E. A. H., & Tuana, N. (2024). Integrating values to improve the relevance of climate‐risk research. Earths Future, 12(10), e2022EF003025. https://doi.org/10.1029/2022ef003025\n\n\nKeller, K., Helgeson, C., & Srikrishnan, V. (2021). Climate risk management. Annual Review of Earth and Planetary Sciences, 49(1), 95–116. https://doi.org/10.1146/annurev-earth-080320-055847\n\n\nLempert, R. J. (2002). A new decision sciences for complex systems. Proc. Natl. Acad. Sci. U. S. A., 99 Suppl 3(Suppl 3), 7309–7313. https://doi.org/10.1073/pnas.082081699\n\n\nReed, P. M., Hadjimichael, A., Moss, R. H., Brelsford, C., Burleyson, C. D., Cohen, S., et al. (2022). Multisector dynamics: Advancing the science of complex adaptive human‐earth systems. Earths Future, 10(3). https://doi.org/10.1029/2021ef002621\n\n\nRittel, H. W. J., & Webber, M. M. (1973). Dilemmas in a general theory of planning. Policy Sci., 4(2), 155–169. https://doi.org/10.1007/BF01405730"
  },
  {
    "objectID": "readings/readings04.html",
    "href": "readings/readings04.html",
    "title": "Week 4 Readings",
    "section": "",
    "text": "Daw et al. (2015)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one."
  },
  {
    "objectID": "readings/readings04.html#required-reading",
    "href": "readings/readings04.html#required-reading",
    "title": "Week 4 Readings",
    "section": "",
    "text": "Daw et al. (2015)\n\nThis is the paper we’re going to review for our case study on Wednesday. I’ll lead this one."
  },
  {
    "objectID": "readings/readings04.html#optional-reading",
    "href": "readings/readings04.html#optional-reading",
    "title": "Week 4 Readings",
    "section": "Optional Reading",
    "text": "Optional Reading\nI higly recommend reading Banzhaf (2009). Here is a stable URL to the pdf. Benefit-cost analysis (BCA) is a requirement for many public resource allocation decisions. This history of the intellectual debate and political considerations between two visions for BCA in the U.S. – multi-objective and single-objective analysis – is illuminating. You will learn about theoretical and practical strengths/limitations of each approach and how that relates to different views of the role of the scientist/analyst in conducting a decision analysis to inform decisions. In addition, because BCA is a prevailing paradigm, you should be familiar with its conceptual underpinnings and assumptions.\nI also recommend reading the papers below to get exposure to papers that focus on trade-offs and synergies as key framing points of the decision analysis.\n\nPollack et al. (2025)\nJafino et al. (2022)\nCiullo et al. (2020)\nHadka et al. (2015)\n\n\nReferences\n\n\nBanzhaf, H. (2009). Objective or multi-objective? Two historically competing visions for benefit-cost analysis. Land Econ., 85(1), 23–23. https://doi.org/10.3368/le.85.1.3\n\n\nCiullo, A., Kwakkel, J. H., De Bruijn, K. M., Doorn, N., & Klijn, F. (2020). Efficient or fair? Operationalizing ethical principles in flood risk management: A case study on the dutch-german rhine. Risk Anal., 40(9), 1844–1862. https://doi.org/10.1111/risa.13527\n\n\nDaw, T. M., Coulthard, S., Cheung, W. W. L., Brown, K., Abunge, C., Galafassi, D., et al. (2015). Evaluating taboo trade-offs in ecosystems services and human well-being. Proc. Natl. Acad. Sci. U. S. A., 112(22), 6949–6954. https://doi.org/10.1073/pnas.1414900112\n\n\nHadka, D., Herman, J., Reed, P., & Keller, K. (2015). An open source framework for many-objective robust decision making. Environmental Modelling & Software, 74, 114–129. https://doi.org/10.1016/j.envsoft.2015.07.014\n\n\nJafino, B. A., Kwakkel, J. H., & Klijn, F. (2022). Evaluating the distributional fairness of alternative adaptation policies: A case study in vietnam’s upper mekong delta. Clim. Change, 173(3), 17. https://doi.org/10.1007/s10584-022-03395-y\n\n\nPollack, A. B., Santamaria-Aguilar, S., Maduwantha, P., Helgeson, C., Wahl, T., & Keller, K. (2025). Funding rules that promote equity in climate adaptation outcomes. Proceedings of the National Academy of Sciences, 122(2), e2418711121. https://doi.org/10.1073/pnas.2418711121"
  },
  {
    "objectID": "readings/readings07.html",
    "href": "readings/readings07.html",
    "title": "Week 7 Readings",
    "section": "",
    "text": "Bryant & Lempert (2010)\n\nThis is a seminal paper on the role of scenario discovery in the RDM workflow.\n\nHadjimichael et al. (2023)\n\nThese are the slides from one of the top 3 research presentations I have seen. It really transformed my understanding and appreciation for the role of scenario discovery in the RDM workflow. As far as I know, there is no recording of this presentation, but the slides are so well-designed that you can make a lot of the key takeaways. You may reference the corresponding paper Hadjimichael et al. (2024) for more detail, but it is not required reading due to its length.\n\nJafino & Kwakkel (2021)\n\nThis will be the case study paper for the week. You don’t have to read it by Monday, but come prepared to discuss it for Friday’s session."
  },
  {
    "objectID": "readings/readings07.html#required-reading",
    "href": "readings/readings07.html#required-reading",
    "title": "Week 7 Readings",
    "section": "",
    "text": "Bryant & Lempert (2010)\n\nThis is a seminal paper on the role of scenario discovery in the RDM workflow.\n\nHadjimichael et al. (2023)\n\nThese are the slides from one of the top 3 research presentations I have seen. It really transformed my understanding and appreciation for the role of scenario discovery in the RDM workflow. As far as I know, there is no recording of this presentation, but the slides are so well-designed that you can make a lot of the key takeaways. You may reference the corresponding paper Hadjimichael et al. (2024) for more detail, but it is not required reading due to its length.\n\nJafino & Kwakkel (2021)\n\nThis will be the case study paper for the week. You don’t have to read it by Monday, but come prepared to discuss it for Friday’s session."
  },
  {
    "objectID": "readings/readings07.html#optional-reading",
    "href": "readings/readings07.html#optional-reading",
    "title": "Week 7 Readings",
    "section": "Optional Reading",
    "text": "Optional Reading\nScenario discovery is highly context specific and I think it’s helpful to be exposed to many different examples. Below, I highlight a few additional papers focused on scenario discovery, as well papers that employ scenario discovery as part of a larger analysis, to provide you with a broad set of examples to draw from in attempting to apply these methods to your problem.\n\nGroves et al. (2019)\n\nSee section 7.3.2 and 7.3.7.\n\nQuinn et al. (2017)\n\nSee section 5.3.2.\n\nDavid F. Gold et al. (2023)\n\nSee section 5.4.\n\nD. F. Gold et al. (2019)\n\nCreative application of scenario discovery to consider the influence of implementation uncertainty on decision performance.\n\nLamontagne et al. (2018) and Wessel et al. (2024)\n\nGood demonstrations of how to conduct an insightful analysis when you have a large ensemble of scenarios.\n\n\n\nReferences\n\n\nBryant, B. P., & Lempert, R. J. (2010). Thinking inside the box: A participatory, computer-assisted approach to scenario discovery. Technol. Forecast. Soc. Change, 77(1), 34–49. https://doi.org/10.1016/j.techfore.2009.08.002\n\n\nGold, D. F., Reed, P. M., Trindade, B. C., & Characklis, G. W. (2019). Identifying actionable compromises: Navigating multi‐city robustness conflicts to discover cooperative safe operating spaces for regional water supply portfolios. Water Resour. Res., 55(11), 9024–9050. https://doi.org/10.1029/2019wr025462\n\n\nGold, David F., Reed, P. M., Gorelick, D. E., & Characklis, G. W. (2023). Advancing regional water supply management and infrastructure investment pathways that are equitable, robust, adaptive, and cooperatively stable. Water Resour. Res., 59(9), e2022WR033671. https://doi.org/10.1029/2022wr033671\n\n\nGroves, D. G., Molina-Perez, E., Bloom, E., & Fischbach, J. R. (2019). Robust decision making (RDM): Application to water planning and climate policy. In Decision making under deep uncertainty (pp. 135–163). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05252-2\\_7\n\n\nHadjimichael, A., Reed, P. M., Quinn, J. D., Vernon, C. R., & Thurber, T. (2023, October). Advancing scenario discovery to identify impacts and consequential dynamics for complex multi-actor human-natural systems. Zenodo. https://doi.org/10.5281/zenodo.8400589\n\n\nHadjimichael, A., Reed, P. M., Quinn, J. D., Vernon, C. R., & Thurber, T. (2024). Scenario storyline discovery for planning in multi‐actor human‐natural systems confronting change. Earths Future, 12(9), e2023EF004252. https://doi.org/10.1029/2023ef004252\n\n\nJafino, B. A., & Kwakkel, J. H. (2021). A novel concurrent approach for multiclass scenario discovery using multivariate regression trees: Exploring spatial inequality patterns in the vietnam mekong delta under uncertainty. Environmental Modelling & Software, 145, 105177. https://doi.org/10.1016/j.envsoft.2021.105177\n\n\nLamontagne, J. R., Reed, P. M., Link, R., Calvin, K. V., Clarke, L. E., & Edmonds, J. A. (2018). Large ensemble analytic framework for consequence‐driven discovery of climate change scenarios. Earths Future, 6(3), 488–504. https://doi.org/10.1002/2017ef000701\n\n\nQuinn, J. D., Reed, P. M., & Keller, K. (2017). Direct policy search for robust multi-objective management of deeply uncertain socio-ecological tipping points. Environmental Modelling & Software, 92, 125–141. https://doi.org/10.1016/j.envsoft.2017.02.017\n\n\nWessel, J., Iyer, G., Wild, T., Ou, Y., McJeon, H., & Lamontagne, J. (2024). Large ensemble exploration of global energy transitions under national emissions pledges. Earths Future, 12(10), e2024EF004754. https://doi.org/10.1029/2024ef004754"
  }
]