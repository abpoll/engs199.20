{
  "hash": "5315d2a97671ccccec9c987933d50087",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lab 4: Potential Pitfalls of Predict-Then-Act\"\nauthor: \"ENGS 199.20\"\ndate: \"10/10/2025\"\npublished-title: \"Due Date:\"\nformat:\n    html:        \n        warning: true\n        error: true\n        code-fold: true\n    ipynb:\n        warning: true\n        error: true\nexecute: \n    echo: true\n    enabled: true\njupyter: engs199\nengine: jupyter\nsidebar: main\n---\n\n\n\n\n\n\n## Overview\nToday we're going to learn about the potential pitfalls of predicting then acting for problems characterized by deep uncertainty. This lab adapts the analysis from [Managing the risk of uncertain threshold responses: comparison of robust, optimum, and precautionary approaches](http://dx.doi.org/10.1111/j.1539-6924.2007.00940.x) by Lempert and Collins, 2007. \n\nToday's objectives:\n\n1. Calculate the regret of a strategy in individual states and across states\n2. Calculate a strategy's satisficing robustness\n3. Visualize different robustness for various metrics and strategies in insightful ways\n4. Demonstrate your conceptual understanding of the two robustness criteria from the study\n5. (Optional) Apply the workflow to your project data\n\n## Lab Workflow\n\n### Repository Setup\nYou can call the repository `rdm_lab/`. \n\nIf you need a reminder on how to set up your project directory and create a GitHub repository, check the workflow and directory structure from lab 2. \n\nYou can directly export these lab instructions as a `.ipynb` file (look at the top right of the page). Place this file in the `noteboks/` subdirectory. \n\n### Environment Setup\nFor today's lab, you will need to create an environment that has all the packages from lab 2, plus `scipy`. \n\nBe sure to include instructions in your `README.md` about how others can prepare the computational environment to run your notebook. \n\n### Our Analysis\nMake sure your environment is set up and active to run the code cells below. \n\n#### Lake Problem Overview\nQuoting from Lempert and Collins (2007): \"Imagine a small village perched on the shores of a pristine lake. Many of the town’s citizens want economic  development,  but  development  will  increase the pollution flowing into the lake. The citizens know that lakes such as theirs can exhibit a threshold behavior where a small pollution increase past some level may cause a clear lake to turn suddenly and sometimes irreversibly cloudy. The citizens seek a development plan that preserves the clarity of their lake, cognizant of their widely divergent values regarding environmental  quality  and  growth,  as  well  their  divergent opinions about the level where the pollution threshold for their lake might lie. The town’s citizens also know that once permits are issued, buildings built, and pollution begins to flow it may become difficult to reverse course. Nonetheless, they must decide how much pollution they should allow to best balance their economic and environmental goals.\"\n\n![Shallow Lake Model diagram from [Vivek Srikrishnan's Fall 2025 Environmental Systems Analysis Slides](https://envsys.viveks.me/fall2025/slides/lecture03-2-equilibria-bifurcations.html#/shallow-lake-model-1)](../_assets/img/eutrophication-system.png)\n\nThe lake naturally loses and gains phosphorus (P). The lake can exist in one of two states. The desirable state, called an oligotrophic state, exists with low nutrient inputs, low to moderate levels of plant production, and relatively clear water. The undesirable state, called an eutrophic state, exists with high nutrient  levels,  high  levels  of  plant  production,  low biodiversity, and murky water. The introduction of economic activity to the lake increases the risk of eutrophication.\n\nLempert and Collins (2007) represent the lake problem with the following equation (with a little clarification from revisiting [Peterson et al., (2003)](https://doi.org/10.1890/0012-9658(2003)084[1403:UATMOM]2.0.CO;2)):\n$$\nX_{t+1} = \n\\begin{cases}\n    BX_t + b_t + L_t & \\text{if } X_t < X^{\\text{crit}} \\\\\n    BX_t + r + b_t + L_t & \\text{if } X_t \\geq X^{\\text{crit}}\n\\end{cases}\n$$\n\nwhere $X_{t}$ is the P concentration at time $t$, $B$ is the proportion of P retained in the lake each year, $b_{t} = (\\bar{b}, \\omega)$ is the natural pollution flow into the lake assumed to follow a normal distribution with mean $\\bar{b}$ and standard deviation $\\omega$, $L_{t}$ is the anthropogenic pollution flow, and $r$ is the amount of P recycled and maintained in the lake. $X^{\\text{crit}}$ is the critical P concentration at which P recycling begins. \n\nThe two alternating equilibrium values are found by letting $X_{t+1} = X{t}$:\n\n$$\n\\begin{array}{l}\n    X_{1*} = \\frac{\\bar{b} + L}{1 - B} & \\text{if } X_{1*} < X^{\\text{crit}} \\\\\n    X_{2*} = \\frac{r + \\bar{b} + L}{1 - B} & \\text{if } X_{2*} \\geq X^{\\text{crit}}\n\\end{array}\n$$\n\nIf $X_{1*} < X^{\\text{crit}}$ an oligotrophic equilibrium exists, and if $X_{2*} \\geq X^{\\text{crit}}$ a eutrophic equilibrium also exists. Because the equilibrium value of $X$ depends upon the P loading, changes in P loading can cause equilibrium point to appear or disappear. Figure 2 from Peterson et al., (2003), shown below, illustrates how changes in P loading can irreversibly alter the lake system's outcomes: \n\n![An illustration of the P dynamics model. In example (A), low P loading results in a single stable oligotrophic equilibrium. In (B), at a higher P loading the same lake has two stable equilibrium values—one oligotrophic (the lower point) and one eutrophic (the higher point)](../_assets/img/lake_two_eq.jpg)\n\nLempert and Collins (2007) assume that the citizens of the town gain utility by adding pollutants to the lake, but suffer a loss of utility if the lake becomes eutrophic. They represent the utility function as:\n\n$$\nU'_{t} = \n\\begin{cases}\n    \\alpha L_{t} - \\phi (L_{t} - L_{t-1}) & \\text{if } X_t < X^{\\text{crit}} \\\\\n    \\alpha L_{t} - \\beta - \\phi (L_{t} - L_{t-1}) & \\text{if } X_t \\geq X^{\\text{crit}}\n\\end{cases}\n$$\n\nwhere $\\alpha$ and $\\beta$ are constants. $\\beta >> \\alpha L_{t}$, representing that the loss of utility in the eutrophic state (e.g., recreational and aesthetic utility) is large compared to the utility gained from pollution. The citizens also may suffer economic losses, such as costs of retrofitting infrastructure with pollution control equipment or underutilization of capital stock, if they are forced to decrease emissions below their previously allowed level. These costs are small compared to those of the eutrophic state, given by:\n\n$$\n\\phi (L_{t} - L_{t-1}) = \n\\begin{cases}\n    0 & \\text{if } L_{t} \\geq L_{t-1} \\\\\n    \\varphi & \\text{if } L_{t} < L_{t-1}\n\\end{cases}\n$$\n\nwhere $\\varphi$ is a constant and $\\varphi << \\frac{\\beta}{L_{t-1} - L_{t}}$.\n\nThe town's decision problem is to choose a pollution series, $\\mathbf{L_t}$, \n\nLempert and Collins propose a policy rule that has three decision variables: the initial level of pollution, $L_{0}$, the max amount of pollution per time period, $\\Delta L$, and a safety margin, $S$. This decision strategy takes the following form:\n\n$$\nL_{t+1} = \n\\begin{cases}\n    Max\\{0, Min[L_{t} + \\Delta L, L^{\\text{target}}_{t}]\\} & \\text{for } t \\geq 1 \\\\\n    L_{0} & \\text{for } t = 0\n\\end{cases}\n$$\n\nwhere the target emission level at time $t$ is given by:\n\n$$\nL^{\\text{target}} = \n\\begin{cases}\n    (1 - B) \\langle X^{\\text{crit}}_{t} \\rangle - \\bar{b} - S\\omega  & \\text{if } X_{t} < X^{\\text{crit}} \\\\\n    \\langle X^{\\text{crit}}_{t} \\rangle - BX_{t} - r - \\bar{b} - S\\omega & \\text{if } X_{t} \\geq X^{\\text{crit}}\n\\end{cases}\n$$\n\nand $\\langle X^{\\text{crit}}_{t} \\rangle$ is the town's estimated critical threshold at the current time period. Please refer to the paper for the formulation. \n\nUnder an expected utility maximization framework for decision-making, the town aims to maximize the present value of utility: $PV(U) = \\sum_{t} \\frac{U_{t}}{(1 + d)^{t}}$, where $d$ is the citizens' discount rate. Given the available information to estimate $\\langle X^{\\text{crit}}_{t} \\rangle$, the town can find the optimal values for their decision variables. As we'll see later in the lab (and as you already know from reading the paper), whether this is the *right* decision to make depends on how much you believe the information you have to estimate $\\langle X^{\\text{crit}}_{t} \\rangle$. \n\nThe basecase values of model parameters are as follows:\n\n| Parameter | Value |\n| :-------: | :---: |\n|Retained phosphorous (B)|.2|\n|Mean of natural emissions ($\\hat{b}$)|.1|\n|Initial P concentration ($X_{0}$)|$\\hat{b}$/(1 - B) = .125|\n|Std of natural emissions $\\omega$|.04|\n|Recycling rate (r)|.25|\n|Std of measured $X^{\\text{crit}}$ ($\\gamma$)|.05|\n|Distance from $X^{\\text{crit}}$ required for learning ($\\lambda$)|.1|\n|Noise exponent (q)|2|\n|Initial estimate of observed $X^{\\text{crit}}$ variance|.02|\n|Discount rate (d)|.03|\n|Utility from pollution ($\\alpha$)|1|\n|Eutrophic cost ($\\beta$)|10|\n|Emissions reduction cost ($\\varphi$)|1|\n\n*Note*: While the table in the text has $\\varphi$ as 10, the text clearly states that this emissions reduction cost is much smaller than the eutrophic cost so it is a typo. I'm guessing it's supposed to be 1 (similarly, the initial estimate of observed $X^{\\text{crit}}$ variance is a typo in the paper's table).\n\n### Decision-making under uncertainty\n**Note** that I was not able to reproduce the results from Lempert and Collins completely. There are a few typos (e.g., see their Table on parameter values and the reported initial estiamte of observed $X_{\\text{crit}}$ and the one I was able to update based on other text in the paper). There are a few decisions and implementation choices I can't back out without more guidance, but I was able to reproduce the most relevant behavior of the inflows and policies. I made a few updates relative to the policy rules as written in the manuscript:\n\n1. I estimate the new best guess for $X_{\\text{crit}}$ based on the current best guess for variance. The notation as currently written suggests we calculate the new variance before updating our mean estimate. \n2. I limit updates on standard deviation of $X_{\\text{crit}}$ to double our intial guess. If we don't do this, there is too much noise in early draws and we can't identify when our initial guess is lower than $X^{\\text{true}}_{\\text{crit}}$. I think this biases our representation a bit more towards trusting our priors than the Lempert and Collins paper, but I can't otherwise figure out how to get the same learning behavior as them for Strategy A when $X^{\\text{true}}_{\\text{crit}}$ = .9. \n\nAnyone who wants to review the paper and code and reconcile any major differences with explanations or code updates is more than welcome! \n\nBecause the code below does reproduce key behavior of the inflows and policies, all of the lessons about robustness are the same and that's the key point of this lab. \n\nOk! Moving on...\n\nAdapting the narrative of Lempert and Collins slightly, suppose the town reviewed scientific evidence of nearby lakes and concludes that $X_{\\text{crit}}$ lies between .3 and .9. The current best evidence suggets $X_{\\text{crit}} \\sim N(.8, .138)$. \n\nIf the town has 100% confidence in this belief, the optimal strategy (called Strategy A) has:\n$L_{0} = .31$,\n$\\Delta L = .027$\n$S = 2.8$\n\n\nAssume that the true value of $X_{\\text{crit}}$ is .9. What does the pollution concentration of the lake look like over 100 years? What about if the true value is .3?\n\n::: {#13d0a80f .cell execution_count=1}\n``` {.python .cell-code}\nimport math\nimport numpy as np\nfrom scipy.stats import truncnorm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ------------------------\n# Parameters (Table I / paper)\n# ------------------------\nB = 0.20           # retained phosphorus\nb_bar = 0.10       # mean natural emissions\nomega = 0.04       # std dev natural emissions\nr = 0.25           # recycling jump when X >= Xcrit\ngamma_obs = 0.05   # baseline observation noise std\nlambda_learn = 0.10\nq_noise = 2.0\ninitial_var = 0.02\nalpha = 1.0\nbeta = 10.0\nphi = 1.0 # table says 10 but the text says << beta\ndiscount_rate = 0.03\n\n# Strategy A (given)\nstrategy_A = {\"L0\": 0.31, \"dL\": 0.027, \"S\": 2.8}\n\n# Simulation controls\nT = 100                # years\nNsim = 100             # ensemble size for mean / quantiles (set e.g., 100 for demo)\nSEED = 42\nrng_global = np.random.default_rng(SEED)\n\n# Initial estimate used by the town\ninitial_est_mean = 0.787\ninitial_est_var = initial_var\nest_lower = .3\nest_upper = .9\n\n# ------------------------\n# Helper functions implementing paper's learning and policy rules\n# ------------------------\ndef gamma_t_func(Xt, Xcrit_true, gamma=gamma_obs, lam=lambda_learn, q=q_noise):\n    \"\"\"Equation (6): observation noise depends on proximity to threshold.\"\"\"\n    if Xt >= Xcrit_true:\n        return float(gamma)\n    exponent = ((Xcrit_true - Xt) / lam) ** q\n    return min(gamma * math.exp(exponent), .1)         \n\ndef kalman_update(mean_prev, var_prev, Zt, gamma_t):\n    \"\"\"Scalar Kalman-like update (Equation (5))\"\"\"\n    K = var_prev / (var_prev + gamma_t)\n    mean_new = mean_prev + K * (Zt - mean_prev)\n    var_new = (var_prev * gamma_t) / (var_prev + gamma_t)\n    return mean_new, var_new\n\ndef target_emission_level(estimated_Xcrit, Xt, Xcrit_true, S):\n    \"\"\"Equation (8) for L_target\"\"\"\n    if Xt < Xcrit_true:\n        target = (1.0 - B) * estimated_Xcrit - b_bar - S * omega\n    else:\n        target = estimated_Xcrit - B * Xt - r - b_bar - S * omega\n    return max(0.0, target)\n\ndef next_L(Lt, L_arg, dL):\n    \"\"\"Equation (7) update for L\"\"\"\n    return max(0.0, min(Lt + dL, L_arg))\n\ndef simulate_one_run_with_learning(Xcrit_true,\n                                   policy,\n                                   T=100,\n                                   rng=None,\n                                   est_mean_init=initial_est_mean, est_var_init=initial_est_var):\n    \"\"\"\n    Simulate one stochastic run with Kalman learning and the adaptive policy (L0, dL, S).\n    Returns time series dict with keys: X, L, est (estimated Xcrit), U (period utilities)\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    L0 = float(policy[\"L0\"]) \n    dL = float(policy[\"dL\"])\n    S = float(policy[\"S\"])\n    X = np.empty(T)\n    L = np.empty(T)\n    Z = np.empty(T)\n    est = np.empty(T)\n    U = np.empty(T)\n    Xt = b_bar / (1.0 - B)  # initial concentration per paper\n    L_prev = L0\n    L_curr = L0\n    est_mean = est_mean_init\n    est_var = est_var_init\n\n    for t in range(T):\n        # natural inflow\n        bt = rng.normal(b_bar, omega)\n        # update concentration (Eqn 1)\n        if Xt >= Xcrit_true:\n            Xt_next = B * Xt + r + bt + L_curr\n        else:\n            Xt_next = B * Xt + bt + L_curr\n        # observation for learning: Zt ~ N(Xcrit_true, gamma_t)\n        gamma_t = gamma_t_func(Xt, Xcrit_true)\n        Zt = rng.normal(loc=Xcrit_true, scale=gamma_t)\n        # Kalman update\n        est_mean, est_var = kalman_update(est_mean, est_var, Zt, gamma_t)\n        # utility for this period (using current Xt and current emissions L_curr)\n        reduction_cost = phi * max(0.0, L_prev - L_curr)\n        eutrophic_penalty = beta if Xt >= Xcrit_true else 0.0\n        U[t] = alpha * L_curr - reduction_cost - eutrophic_penalty\n        # record\n        X[t] = Xt\n        L[t] = L_curr\n        est[t] = est_mean\n        # compute target and update emissions\n        L_targ = target_emission_level(est_mean, Xt, Xcrit_true, S)\n        L_next = next_L(L_curr, L_targ, dL)\n        L_prev = L_curr\n        L_curr = L_next\n        Xt = Xt_next\n\n    return {\"X\": X, \"L\": L, \"est\": est, \"U\": U}\n\n# ------------------------\n# Run simulation \n# ------------------------\nall_X = np.zeros((Nsim, T))\nall_L = np.zeros((Nsim, T))\nall_est = np.zeros((Nsim, T))\nall_U = np.zeros((Nsim, T))\n\nall_X2 = np.zeros((Nsim, T))\nall_L2 = np.zeros((Nsim, T))\nall_est2 = np.zeros((Nsim, T))\nall_U2 = np.zeros((Nsim, T))\n\n# True Xcrits for this illustration\nxcrit_list = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nn_x = len(xcrit_list)\n\n# pre-allocate arrays: shape (n_x, Nsim, T)\nall_X = np.zeros((n_x, Nsim, T))\nall_L = np.zeros((n_x, Nsim, T))\nall_est = np.zeros((n_x, Nsim, T))\nall_U = np.zeros((n_x, Nsim, T))\n\n# Pre-generate natural inflows bt_samples for each Monte Carlo run (CRN across Xcrit)\nrng_global = np.random.default_rng(SEED)\nbt_samples = rng_global.normal(loc=b_bar, scale=omega, size=(Nsim, T))\n\n# Run ensemble\nfor i in range(Nsim):\n    # create per-run rng for observation draws (do not re-seed inside loop)\n    rng_i = np.random.default_rng(SEED + 1000 + i)\n    for j, xcrit in enumerate(xcrit_list):\n        out = simulate_one_run_with_learning(Xcrit_true=xcrit,\n                                             policy=strategy_A,\n                                             T=T,\n                                             rng=rng_i,\n                                             est_mean_init=initial_est_mean,\n                                             est_var_init=initial_est_var)\n        all_X[j, i, :] = out[\"X\"]\n        all_L[j, i, :] = out[\"L\"]\n        all_est[j, i, :] = out[\"est\"]\n        all_U[j, i, :] = out[\"U\"]\n\n# ------------------------\n# Plot results: mean concentration, emissions, and estimated Xcrit\n# ------------------------\nyears = np.arange(1, T+1)\n\nfig, ax = plt.subplots(2, 1, figsize=(12, 8))\n\n# Top: concentration and estimated threshold\nax[0].plot(years, all_X[-1].mean(axis=0), color=\"C0\", label=\"Mean P concentration (X_t)\")\nax[0].fill_between(years,\n                   np.percentile(all_X[-1], 5, axis=0),\n                   np.percentile(all_X[-1], 95, axis=0),\n                   alpha=.2,\n                   color=\"C0\", label=\"95% bounds across realizations\")\nax[0].plot(years, all_L[-1].mean(axis=0), color=\"C2\", label=\"Mean anthropogenic flow (L_t)\")\nax[0].plot(years, all_est[-1].mean(axis=0), color=\"C1\", linestyle=\"--\", label=\"Mean estimated Xcrit\")\nax[0].axhline(xcrit_list[-1], color=\"k\", linestyle=\":\", label=f\"True Xcrit = {xcrit_list[-1]}\")\nax[0].set_ylabel(\"Phosphorus concentration\")\nax[0].set_title(\"Strategy A (L0=0.31, ΔL=0.027, S=2.8)\")\nax[0].legend()\n\nax[1].plot(years, all_X[0].mean(axis=0), color=\"C0\", label=\"Mean P concentration (X_t)\")\nax[1].fill_between(years,\n                   np.percentile(all_X[0], 5, axis=0),\n                   np.percentile(all_X[0], 95, axis=0),\n                   alpha=.2,\n                   color=\"C0\", label=\"95% bounds across realizations\")\nax[1].plot(years, all_L[0].mean(axis=0), color=\"C2\", label=\"Mean anthropogenic flow (L_t)\")\nax[1].plot(years, all_est[0].mean(axis=0), color=\"C1\", linestyle=\"--\", label=\"Mean estimated Xcrit\")\nax[1].axhline(xcrit_list[0], color=\"k\", linestyle=\":\", label=f\"True Xcrit = {xcrit_list[0]}\")\nax[1].set_ylabel(\"Phosphorus concentration\")\nax[1].set_title(\"Strategy A (L0=0.31, ΔL=0.027, S=2.8)\")\nax[1].legend(loc='lower right')\n\n```\n\n::: {.cell-output .cell-output-display}\n![](lab04-rdm_files/figure-ipynb/cell-2-output-1.png){}\n:::\n:::\n\n\nWhen the neighboring lakes' statistics are represenative of the town's lake (top panel, true Xcrit = 0.9), Strategy A lets emissions ramp up and stabilizes at a moderate level that keeps the mean phosphorus concentration under the town’s safety buffer. When the assessment is badly wrong (bottom panel, true Xcrit = 0.3), the initial policy overcommits — the emissions (and then concentration) rise, the town’s estimate of $X_{\\text{crit}}$ is revised downward as observations arrive, and emissions are sharply cut only after the system has already been driven into the eutrophic equilibrium. This illustrates how a predict‑then‑act rule that trusts a single prior can be fragile under deep uncertainty.\n\nHow much would the town regret going with Strategy A if $X^{\\text{true}}_{\\text{crit}}$ = .3?\n\nTo estimate this, we are going to consider the town's uncertainty about their estimate of the value of $X_{\\text{crit}}$ as well as epistemic uncertainty about whether this is the *correct* distribution to characterize $X_{\\text{crit}}$.\n\nRecall from lecture that the regret of action *a* in state *x* is:\n\n$R_{a}(x) = Max_{a'}[PV(U_{a'}(x))] - PV(U_{a}(x))$, \n\nthe difference between the present value expected utility of the *optimal* action in a considered state of the world and the *considered* action's present value expected utility in that state of the world. \n\nConsidering the probability distribution $p_{i}(x)$ over possible states of the world, the expected regret of action *a* on distribution *i* is: \n\n$$\n\\bar{R}_{a, i} = \\int_{x} R_{a}(x)p_{i}(x)dx\n$$\n\nI use actions, *a*, as opposed to strategies in this formulation because it is consistent with the original derivation in [Savage (1951)](http://dx.doi.org/10.1080/01621459.1951.10500768). Otherwise, the notation is consistent with the formulation in Lempert and Collins. *x* refers to the possible values for $X_{\\text{crit}}$ and *i* refers to a specific prior distribution for $X_{\\text{crit}}$. \n\nFor different values of $X_{\\text{crit}}$, we can estimate the town's expected regret for placing all of their trust in the commissioned science assessment. Lempert and Collins provide several other strategies, B-G. For different values of $X_{\\text{crit}}$, we can calculate which of the strategies A-G is optimal and then calculate the expected regret of choosing Strategy A based on the town's prior distribution for A. Lempert and Collins presumably identify the optimal strategy for each $X_{\\text{crit}}$ and calculate regret based on that, but we are just going to use the strategies provided to simplify things. \n\nWe'll start with the example of $X_{\\text{crit}}$ = .3, which corresponds to the bottom panel of the figure we generated above. At $X_{\\text{crit}}$ = .3, Strategy G is optimal out of the strategies under consideration. Lempert and Collins don't report the decision variables for this strategy, but we can get a very similar strategy to G by looking at Figure 6. It looks like $L_{0} = 0.01$, $\\Delta L = 0.01$ and $S = 5.8$. \n\nWe'll build up slowly to expected regret over a prior distribution by first calculating the regret of Strategy A in the scenario where $X_{\\text{crit}}$ = 0.3. We need to calculate:\n\n$R_{A}(0.3) = PV(U_{G}(0.3)) - PV(U_{A}(0.3))$ \n\nWe'll calculate the expected present value of utility of each strategy when $X_{\\text{crit}}$ = .3 and then the difference. \n\n::: {#00080abc .cell execution_count=2}\n``` {.python .cell-code}\n# Strategy G (guessed from Fig. 6)\nstrategy_G = {\"L0\": 0.01, \"dL\": 0.01, \"S\": 5.8}\n\n# Create the utility array for strategy G\nstrat_g_U = np.zeros((n_x, Nsim, T))\n\n# Run ensemble for Strategy G\nfor i in range(Nsim):\n    # create per-run rng for observation draws (do not re-seed inside loop)\n    rng_i = np.random.default_rng(SEED + 1000 + i)\n    for j, xcrit in enumerate(xcrit_list):\n        out = simulate_one_run_with_learning(Xcrit_true=xcrit,\n                                             policy=strategy_G,\n                                             T=T,\n                                             rng=rng_i,\n                                             est_mean_init=initial_est_mean,\n                                             est_var_init=initial_est_var)\n        strat_g_U[j, i, :] = out[\"U\"]\n\n# Compute present value of utility.\n# Discount factors (length T)\nt = np.arange(T)\ndiscount_factors = 1.0 / ((1.0 + discount_rate) ** t)\n\npv_A_samples = np.sum(all_U[0, :, :] * discount_factors, axis=1) \npv_G_samples = np.sum(strat_g_U[0, :, :] * discount_factors, axis=1)\n\n# Calculate expected values and report both and difference\nmean_pv_A = pv_A_samples.mean()\nmean_pv_G = pv_G_samples.mean()\n\ng_best_regret = mean_pv_G - mean_pv_A\n\nprint(\"Strategy A: mean PV = {:.2f}\".format(mean_pv_A))\nprint(\"Strategy G: mean PV = {:.2f}\".format(mean_pv_G))\nprint(\" Regret = {:.2f}\".format(g_best_regret))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStrategy A: mean PV = -310.81\nStrategy G: mean PV = -17.38\n Regret = 293.43\n```\n:::\n:::\n\n\nAnd we can do the same for when $X^{\\text{crit}}$ = .9 but this time we treat Strategy A as the optimal policy. \n\n::: {#a0647341 .cell execution_count=3}\n``` {.python .cell-code}\n# Compute present value of utility.\n# Discount factors (length T)\nt = np.arange(T)\ndiscount_factors = 1.0 / ((1.0 + discount_rate) ** t)\n\npv_A_samples = np.sum(all_U[-1, :, :] * discount_factors, axis=1) \npv_G_samples = np.sum(strat_g_U[-1, :, :] * discount_factors, axis=1)\n\n# Calculate expected values and report both and difference\nmean_pv_A = pv_A_samples.mean()\nmean_pv_G = pv_G_samples.mean()\n\na_best_regret = mean_pv_A - mean_pv_G\n\nprint(\"Strategy A: mean PV = {:.2f}\".format(mean_pv_A))\nprint(\"Strategy G: mean PV = {:.2f}\".format(mean_pv_G))\nprint(\" Regret = {:.2f}\".format(a_best_regret))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStrategy A: mean PV = 15.24\nStrategy G: mean PV = 7.27\n Regret = 7.98\n```\n:::\n:::\n\n\n#### Criterion 1: trading-off optimality for less sensitivity to unverifiable assumptions\n\nHow do we reconcile the differences in present value expected utility and expected regret? Lempert and Collins define a robust strategy as one that will have a smaller value of the weighted average of the best and worst case regrets compared to the strategy that optimizes expected utility, which we can represent as:\n\n$$\nV_{s} = z*\\bar{R}_{s, \\text{best}} + (1-z)*\\bar{R}_{s, \\text{worst}}\n$$\n\nwhere $0 \\leq z \\leq 1$ and represents our belief in $p_{\\text{best}(x)}$.\n\nWhen $z$ = 1, there is only one probability distribution under consideration - the town's best guess. In this case, the equation above preserves the ordering of strategies produced by an expected  utility  calculation. When $z$ = 0, the criterion is equivalent to avoiding worst-case utility. When $z$ is in between, it is possible to identify the strategy that minimizes expected regret as a function of a decisionmaker's odds that the initial probability distribution is correct (Fig. 7 in the paper). The odds are $\\frac{z}{(1-z)}$. \n\nHow confident does the town have to be in the scientific assessment to stick with Strategy A? To do this, we'll calculate $V_{s}$ for strategies A and G over a wide range of values for $z$. Since we are only considering strategies A and G:\n\n$\\bar{R}_{A, \\text{best}}$ is 0, $\\bar{R}_{A, \\text{worst}}$ is the regret we calculated when $X_{crit}$ = 0.3, $\\bar{R}_{G, \\text{best}}$ is the regret we calculated when $X_{crit}$ = 0.9, and $\\bar{R}_{G, \\text{worst}}$ is 0. \n\n::: {#b4098769 .cell execution_count=4}\n``` {.python .cell-code}\n# Calculated V_s\ndef weighted_regret(z, best_regret, worst_regret):\n    return z*(best_regret) + (1-z)*(worst_regret)\n\n# Odds from 0.01 to 100\n# Construct odds grid (log-spaced) and convert to z = odds/(1+odds)\nodds = np.logspace(-2, 3, 500)\nz_list = odds / (1.0 + odds)\n\n# Calculate V_A and V_G over odds\nV_A = z_list * 0 + (1 - z_list) * g_best_regret\nV_G = z_list * a_best_regret + (1 - z_list) * 0\n\n# Plot odds against weighted regret\nfig, ax = plt.subplots()\nax.plot(odds, V_A, label='V_A (weighted regret for A)', linestyle='-')\nax.plot(odds, V_G, label='V_G (weighted regret for G)', linestyle='--')\n\nax.set_ylabel(\"V_s (weighted expected regret)\", size=14)\n\nax.set_xscale('log')\n# integer ticks to show (only those within the odds range will be used)\ninteger_ticks = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n# Choose x limits: show more of odds > 10 — e.g., start at 0.1 to include small odds too\nax.set_xlim(0.1, 1000)\n# Keep only ticks that fall inside the x limits\nticks_in_range = [t for t in integer_ticks if (t >= ax.get_xlim()[0] and t <= ax.get_xlim()[1])]\nax.set_xticks(ticks_in_range)\n# Label ticks as integers (no decimals)\nax.set_xticklabels([str(int(t)) for t in ticks_in_range])\nax.set_xlabel(\"Odds of Strategy A's Priors\", size=14)\n\nax.set_ylim([0, 50])\n\nax.tick_params(labelsize=12)\n\nax.legend(fontsize='large')\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab04-rdm_files/figure-ipynb/cell-5-output-1.png){}\n:::\n:::\n\n\nBased on this figure and fig. 7 in the paper, we can make a few observations about robust decision-making with this criterion. First, because of how poorly strategy A performs under low $X^{true}_{crit}$, the town should be very confident in the scientific assessment to go with strategy A. Second, having more strategy options (fig. 7 in the paper) can help decisionmakers make more robust decisions under a wide range of beliefs.\n\n#### Criterion 2: performing well over a wide range of plausible futures\n\nWe'll now shift our attention to a second robustness criterion: satisficing over a wide range of futures. While decisionmakers may want to identify satisficing strategies over multiple objectives, Lempert and Collins use regret for this criterion. This translates to finding a strategy that has low regret for many values of $X^{true}_{crit}$. In the paper, they define arbitrary cutoffs for what constitutes \"good enough\" regret. In practice, analysts and decisionmakers often iterate on defining a satisficing threshold based on visualizations about strategy performance over a wide range of futures and refined sets of strategies. \n\nWe're going to consider both the net present value of expected utility and regret for strategies A and G. \n\n::: {#f08915e7 .cell execution_count=5}\n``` {.python .cell-code}\n# Helpful for getting the pv for multiple xcrit\ndef compute_pvs_from_U(U_array, discount_rate):\n    U = np.asarray(U_array)\n    t = np.arange(U.shape[2])\n    discount_factors = 1.0 / ((1.0 + discount_rate) ** t)\n    pv = (U * discount_factors).sum(axis=2) \n    return pv\n\npv_A_samples = compute_pvs_from_U(all_U, discount_rate)     \npv_G_samples = compute_pvs_from_U(strat_g_U, discount_rate)  \n\n# Mean present value over each xcrit\n# Same index as all_U and strat_g_U\nmean_pv_A = pv_A_samples.mean(axis=1)  \nmean_pv_G = pv_G_samples.mean(axis=1) \n\n# Calculate regret\n# Two strategies under consideration, so regret is\n# 0 when one of the strategies is better for a xcrit value\npv_best_by_x = np.maximum(mean_pv_A, mean_pv_G) \nregret_A = pv_best_by_x - mean_pv_A\nregret_G = pv_best_by_x - mean_pv_G\n\n# Plot regret over critical x values in one panel\n# Plot present value expected utility in the other\n# x values (e.g., xcrit_list)\nx = np.array(xcrit_list)\nn_x = len(x)\n\n# bar width and positions\nwidth = 0.35\nxpos = np.arange(n_x)\n\nfig, ax = plt.subplots(nrows=2, sharex=True, figsize=(10,6))\n\n# Top: expected PV bars\nax0 = ax[0]\nax0.bar(xpos - width/2, mean_pv_A, width=width, color='C0', label='Strategy A')\nax0.bar(xpos + width/2, mean_pv_G, width=width, color='C1', label='Strategy G')\nax0.set_ylabel('Present value utility', size=14)\nax0.legend()\nax0.grid(axis='y', linestyle=':', alpha=0.6)\n\n# Bottom: regret bars\nax1 = ax[1]\nax1.bar(xpos - width/2, regret_A, width=width, color='C0')\nax1.bar(xpos + width/2, regret_G, width=width, color='C1')\nax1.set_ylabel('Regret', size=14)\nax1.grid(axis='y', linestyle=':', alpha=0.6)\n\n# x ticks and labels\nax1.set_xticks(xpos)\nax1.set_xticklabels([f\"{val:.2f}\" for val in x])\nax1.set_xlabel('Xcrit', size=14)\n\nax[0].set_yscale('symlog')\nax[1].set_yscale('symlog')\n\nax0.tick_params(labelsize=12)\nax1.tick_params(labelsize=12)\n```\n\n::: {.cell-output .cell-output-display}\n![](lab04-rdm_files/figure-ipynb/cell-6-output-1.png){}\n:::\n:::\n\n\nThis kind of visualization helps put into context the relative performance of the strategies for different scenarios of $X_{crit}$. For example, Lempert and Collins suggest that regret <= 12 is acceptable. According to our calculations, unacceptable regret only becomes an issue if the true $X_{crit}$ is less than .6. From a satisficing perspective, if we wanted to ensure acceptable regret, we would prefer Strategy G because it robustly achieves that goal. We could also consider an alternative threshold, such as positive utility. Strategy G would be more robust than strategy A because it has positive utility over more scenarios of $X_{crit}$. Note that the satisficing approach does not account for beliefs about the probability of different values for $X_{crit}$. This may not be an appropriate assumption for all situations, but it can still be helpful to represent robustness in multiple ways. \n\n### Wrapping up lab\n\nThere is no separate lab report this week. The project progress report will ask you to think hard about this lab, but I would prefer you spend your time and effort on your project than this toy example. \n\nYou are free to experiment with the code from this lab, or adapt any of the code to expand the analysis or test your understanding. I am happy to take a look and provide feedback or talk through challenging concepts.  \n\n---\njupyter:\n  kernelspec:\n    display_name: engs199\n    language: python\n    name: engs199\n    path: /Users/f006dwr/Library/Jupyter/kernels/engs199\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.13.3\n---\n",
    "supporting": [
      "lab04-rdm_files/figure-ipynb"
    ],
    "filters": []
  }
}